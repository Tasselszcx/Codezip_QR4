[
  {
    "id": "physical_atari_agent_random.py",
    "repo": "Keen-Technologies/physical_atari",
    "url": "https://github.com/Keen-Technologies/physical_atari/blob/main/agent_random.py",
    "code": "# Copyright 2025 Keen Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# agent_random.py\n#\n# Use the last evaluations for target calculation instead of a target model evaluation\nimport time\n\nimport numpy as np\n\nfrom framework.Logger import logger\n\n\nclass Agent:\n    def __init__(self, data_dir, seed, num_actions, total_frames, **kwargs):\n        # defaults that might be overridden by explicit experiment runs\n\n        self.num_actions = num_actions  # many games can use a reduced action set for faster learning\n        self.gpu = -1\n        self.total_frames = total_frames\n        self.frame_skip = 4\n        self.seed = seed\n        self.training_model = None\n        self.ring_buffer_size = 0\n        self.train_losses = 0\n        self.use_model = 0\n\n        # dynamically override configuration\n        for key, value in kwargs.items():\n            try:\n                assert hasattr(self, key)\n            except AssertionError:\n                logger.error(f\"agent_random: Request to set unknown property: {key}\")\n                continue\n            setattr(self, key, value)\n\n        # variables used by policy\n        self.step = 0\n        self.rng = np.random.default_rng(self.seed)\n        self.selected_action_index = 0\n\n    # --------------------------------\n    # Returns the selected action index\n    # --------------------------------\n    def frame(self, observation_rgb8, reward, end_of_episode):\n        if 0 == self.step % self.frame_skip:\n            self.selected_action_index = self.rng.integers(self.num_actions)\n        self.step += 1\n        return self.selected_action_index\n\n    def save_model(self, filename):\n        pass\n",
    "line_count": 63
  },
  {
    "id": "easy-code-reader_src_easy_code_reader_config.py",
    "repo": "FangYuan33/easy-code-reader",
    "url": "https://github.com/FangYuan33/easy-code-reader/blob/master/src/easy_code_reader/config.py",
    "code": "\"\"\"\nEasy Code Reader MCP æœåŠ¡å™¨é…ç½®æ¨¡å—\n\næä¾›é…ç½®è®¾ç½®ï¼ŒåŒ…æ‹¬ Maven ä»“åº“ä½ç½®ã€æœåŠ¡å™¨ä¿¡æ¯ç­‰ã€‚\næ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡ MAVEN_HOMEã€M2_HOME æˆ– MAVEN_REPO è‡ªå®šä¹‰ Maven ä»“åº“è·¯å¾„ã€‚\n\"\"\"\n\nimport os\nfrom pathlib import Path\n\n\nclass Config:\n    \"\"\"\n    Easy Code Reader MCP æœåŠ¡å™¨é…ç½®ç±»\n    \n    é…ç½®é¡¹åŒ…æ‹¬ï¼š\n    - Maven ä»“åº“ä½ç½®å’Œè·¯å¾„è®¾ç½®\n    - æœåŠ¡å™¨åŸºç¡€ä¿¡æ¯ï¼ˆåç§°ã€ç‰ˆæœ¬ï¼‰\n    - åç¼–è¯‘å™¨é…ç½®\n    \"\"\"\n\n    # Maven ä»“åº“ä½ç½®é…ç½®\n    MAVEN_HOME: Path = Path.home() / \".m2\" / \"repository\"\n\n    # ä»ç¯å¢ƒå˜é‡è¦†ç›– Maven ä»“åº“ä½ç½®ï¼ˆä¼˜å…ˆçº§ï¼šMAVEN_HOME > M2_HOME > MAVEN_REPOï¼‰\n    if \"MAVEN_HOME\" in os.environ:\n        MAVEN_HOME = Path(os.environ[\"MAVEN_HOME\"]) / \"repository\"\n    elif \"M2_HOME\" in os.environ:\n        MAVEN_HOME = Path(os.environ[\"M2_HOME\"]) / \"repository\"\n    elif \"MAVEN_REPO\" in os.environ:\n        MAVEN_HOME = Path(os.environ[\"MAVEN_REPO\"])\n\n    # æœåŠ¡å™¨åŸºç¡€é…ç½®\n    SERVER_NAME: str = \"easy-code-reader\"\n\n    # åç¼–è¯‘å™¨è®¾ç½®ï¼Œåç¼–è¯‘è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰\n    DECOMPILER_TIMEOUT: int = 30\n\n    @classmethod\n    def validate(cls) -> bool:\n        \"\"\"\n        éªŒè¯é…ç½®è®¾ç½®\n        \n        æ£€æŸ¥é…ç½®çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯ Maven ä»“åº“è·¯å¾„æ˜¯å¦å­˜åœ¨å’Œå¯è®¿é—®ã€‚\n        \n        è¿”å›:\n            bool: å¦‚æœé…ç½®æœ‰æ•ˆè¿”å› Trueï¼Œå¦åˆ™è¿”å› False\n        \"\"\"\n        if not cls.MAVEN_HOME.exists():\n            return False\n\n        if not cls.MAVEN_HOME.is_dir():\n            return False\n\n        return True\n\n    @classmethod\n    def get_maven_home(cls) -> Path:\n        \"\"\"\n        è·å– Maven ä»“åº“ä¸»ç›®å½•\n        \n        è¿”å›:\n            Path: Maven ä»“åº“ç›®å½•è·¯å¾„\n        \"\"\"\n        return cls.MAVEN_HOME\n\n    @classmethod\n    def set_maven_home(cls, path: str) -> None:\n        \"\"\"\n        è®¾ç½®è‡ªå®šä¹‰ Maven ä»“åº“ä½ç½®\n        \n        å‚æ•°:\n            path: Maven ä»“åº“çš„æ–°è·¯å¾„\n        \"\"\"\n        cls.MAVEN_HOME = Path(path)\n",
    "line_count": 75
  },
  {
    "id": "docsloader_setup.py",
    "repo": "atpuxiner/docsloader",
    "url": "https://github.com/atpuxiner/docsloader/blob/main/setup.py",
    "code": "\"\"\"\n@author axiner\n@version v0.0.1\n@created 2025/08/15 09:06\n@abstract\n@description\n@history\n\"\"\"\nimport os\nimport re\nfrom pathlib import Path\nfrom shutil import rmtree\n\nfrom setuptools import setup, Command\n\nhere = Path(__file__).absolute().parent\npkg_name = \"docsloader\"\n\n\nclass BuildCommand(Command):\n    description = \"Build the package(.whl)\"\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        self._clean_dist_dirs()\n        self._update_version()\n        os.system(\"python -m build --wheel --no-isolation\")\n\n    @staticmethod\n    def _clean_dist_dirs():\n        for path in [\"dist\", \"build\", f\"{pkg_name}.egg-info\"]:\n            try:\n                rmtree(here / path)\n            except FileNotFoundError:\n                pass\n\n    @staticmethod\n    def _update_version():\n        with open(\".history\", \"r\", encoding=\"utf8\") as f:\n            __version__ = None\n            for line in f:\n                if line.startswith(\"## \"):\n                    __version__ = line.replace(\"## \", \"\").strip(\" \\nv\")\n                    break\n            if not __version__:\n                raise ValueError(\"Please set version\")\n        init_file = here.joinpath(f\"{pkg_name}/__init__.py\")\n        with open(init_file, \"r\", encoding=\"utf8\") as f:\n            content = f.read()\n        with open(init_file, \"w\", encoding=\"utf8\", newline=\"\\n\") as f:\n            content = re.sub(r'__version__ = \"[\\d.]+\"', rf'__version__ = \"{__version__}\"', content)\n            f.write(content)\n\n\nclass PublishCommand(Command):\n    description = \"Publish the package(.whl)\"\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        os.system(\"python setup.py bld\")\n        os.system(\"python -m twine upload dist/*\")\n\n\nsetup(\n    cmdclass={\n        \"bld\": BuildCommand,\n        \"pub\": PublishCommand,\n    },\n    options={\n        \"bdist\": {\"plat_name\": \"any\"},\n    },\n)\n",
    "line_count": 84
  },
  {
    "id": "evotoolkit_src_evotoolkit_core_base_run_state_dict.py",
    "repo": "pgg3/evotoolkit",
    "url": "https://github.com/pgg3/evotoolkit/blob/master/src/evotoolkit/core/base_run_state_dict.py",
    "code": "# Copyright (c) 2025 Ping Guo\n# Licensed under the MIT License\n\n\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\nimport numpy as np\n\nfrom .history_manager import HistoryManager\n\n\nclass BaseRunStateDict(ABC):\n    def __init__(self, task_info: dict):\n        self.task_info = task_info\n        self._history_manager: Optional[HistoryManager] = None\n\n    @staticmethod\n    def _serialize_value(value):\n        \"\"\"Convert numpy arrays and other types to JSON-serializable format\"\"\"\n        if isinstance(value, np.ndarray):\n            return {\n                \"__numpy_array__\": True,\n                \"dtype\": str(value.dtype),\n                \"shape\": list(value.shape),\n                \"data\": value.tolist(),\n            }\n        elif isinstance(value, dict):\n            return {k: BaseRunStateDict._serialize_value(v) for k, v in value.items()}\n        elif isinstance(value, (list, tuple)):\n            return [BaseRunStateDict._serialize_value(item) for item in value]\n        elif isinstance(value, (np.integer, np.floating)):\n            return value.item()\n        else:\n            # Basic types (str, int, float, bool, None) pass through\n            # User-defined types will fail here - that's their responsibility\n            return value\n\n    @staticmethod\n    def _deserialize_value(value):\n        \"\"\"Convert serialized numpy arrays back to original format\"\"\"\n        if isinstance(value, dict):\n            if value.get(\"__numpy_array__\"):\n                return np.array(value[\"data\"], dtype=value[\"dtype\"]).reshape(value[\"shape\"])\n            else:\n                return {k: BaseRunStateDict._deserialize_value(v) for k, v in value.items()}\n        elif isinstance(value, list):\n            return [BaseRunStateDict._deserialize_value(item) for item in value]\n        else:\n            return value\n\n    @abstractmethod\n    def to_json(self) -> dict:\n        \"\"\"Convert the run state to JSON-serializable dictionary\"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def from_json(cls, data: dict) -> \"BaseRunStateDict\":\n        \"\"\"Create instance from JSON data\"\"\"\n        pass\n\n    def to_json_file(self, file_path: str) -> None:\n        \"\"\"Save the run state to a JSON file\"\"\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.to_json(), f, indent=2, ensure_ascii=False)\n\n    @classmethod\n    def from_json_file(cls, file_path: str) -> \"BaseRunStateDict\":\n        \"\"\"Load instance from JSON file\"\"\"\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return cls.from_json(data)\n\n    def init_history_manager(self, output_path: str) -> None:\n        \"\"\"åˆå§‹åŒ–å†å²ç®¡ç†å™¨\"\"\"\n        self._history_manager = HistoryManager(output_path)\n\n    @abstractmethod\n    def save_current_history(self) -> None:\n        \"\"\"ä¿å­˜å½“å‰è¿›åº¦çš„å†å²è®°å½•ï¼ˆç”±å­ç±»å®ç°å…·ä½“é€»è¾‘ï¼‰\"\"\"\n        pass\n",
    "line_count": 83
  },
  {
    "id": "chirp-stt_src_chirp_audio_capture.py",
    "repo": "Whamp/chirp-stt",
    "url": "https://github.com/Whamp/chirp-stt/blob/main/src/chirp/audio_capture.py",
    "code": "from __future__ import annotations\n\nimport threading\nfrom typing import Callable, Optional\n\nimport numpy as np\nimport sounddevice as sd\n\n\nclass AudioCapture:\n    def __init__(\n        self,\n        *,\n        sample_rate: int = 16_000,\n        channels: int = 1,\n        dtype: str = \"float32\",\n        status_callback: Optional[Callable[[str], None]] = None,\n    ) -> None:\n        self.sample_rate = sample_rate\n        self.channels = channels\n        self.dtype = dtype\n        self._status_callback = status_callback\n        self._stream: Optional[sd.InputStream] = None\n        self._frames: list[np.ndarray] = []\n        self._lock = threading.Lock()\n\n    def start(self) -> None:\n        if self._stream is not None:\n            return\n\n        def _callback(indata: np.ndarray, _frames: int, _time, status) -> None:  # type: ignore[name-defined]\n            if status and self._status_callback:\n                self._status_callback(str(status))\n            with self._lock:\n                self._frames.append(indata.copy())\n\n        self._frames.clear()\n        self._stream = sd.InputStream(\n            samplerate=self.sample_rate,\n            channels=self.channels,\n            dtype=self.dtype,\n            callback=_callback,\n        )\n        self._stream.start()\n\n    def stop(self) -> np.ndarray:\n        if self._stream is None:\n            return np.empty(0, dtype=self.dtype)\n        self._stream.stop()\n        self._stream.close()\n        self._stream = None\n        with self._lock:\n            if not self._frames:\n                return np.empty(0, dtype=self.dtype)\n            audio = np.concatenate(self._frames, axis=0)\n            self._frames.clear()\n        if self.channels == 1:\n            audio = audio.reshape(-1)\n        return audio.astype(np.float32, copy=False)\n\n",
    "line_count": 60
  },
  {
    "id": "polaris_src_polaris_policy_abstract_client.py",
    "repo": "arhanjain/polaris",
    "url": "https://github.com/arhanjain/polaris/blob/main/src/polaris/policy/abstract_client.py",
    "code": "from abc import ABC, abstractmethod\nfrom typing import Callable\nimport numpy as np\n\nfrom polaris.config import PolicyArgs\n\n\nclass InferenceClient(ABC):\n    REGISTERED_CLIENTS = {}\n\n    # def __init_subclass__(cls, client_name: str, *args, **kwargs) -> None:\n    #     super().__init_subclass__(*args, **kwargs)\n    #     InferenceClient.REGISTERED_CLIENTS[client_name] = cls\n\n    @staticmethod\n    def register(client_name: str) -> Callable[[type], type]:\n        def decorator(cls: type):\n            InferenceClient.REGISTERED_CLIENTS[client_name] = cls\n            return cls\n\n        return decorator\n\n    @staticmethod\n    def get_client(policy_args: PolicyArgs) -> \"InferenceClient\":\n        if policy_args.client not in InferenceClient.REGISTERED_CLIENTS:\n            raise ValueError(\n                f\"Client {policy_args.client} not found. Available clients: {list(InferenceClient.REGISTERED_CLIENTS.keys())}\"\n            )\n        return InferenceClient.REGISTERED_CLIENTS[policy_args.client](policy_args)\n\n    @abstractmethod\n    def __init__(self, args) -> None:\n        \"\"\"\n        Initializes the client.\n        \"\"\"\n        pass\n\n    @property\n    def rerender(self) -> bool:\n        \"\"\"\n        Policy requests a rerender of the visualization. Optimization for less splat rendering\n        for chunked policies. Can default to always True if optimization is not desired.\n        \"\"\"\n        return True\n\n    @abstractmethod\n    def infer(\n        self, obs, instruction, return_viz: bool = False\n    ) -> tuple[np.ndarray, np.ndarray | None]:\n        \"\"\"\n        Does inference on observation and returns action and visualization. If visualization is not needed, return None.\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"\n        Resets the client to start a new episode. Useful if policy is stateful.\n        \"\"\"\n        pass\n\n\nclass FakeClient(InferenceClient):\n    \"\"\"\n    Fake client that returns a dummy action and visualization.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs) -> None:\n        return\n\n    def infer(\n        self, obs, instruction, return_viz: bool = False\n    ) -> tuple[np.ndarray, np.ndarray | None]:\n        import cv2\n\n        external = obs[\"splat\"][\"external_cam\"]\n        wrist = obs[\"splat\"][\"wrist_cam\"]\n        external = cv2.resize(external, (224, 224))\n        wrist = cv2.resize(wrist, (224, 224))\n        both = np.concatenate([external, wrist], axis=1)\n        return np.zeros((8,)), both\n\n    def reset(self, *args, **kwargs):\n        return\n",
    "line_count": 85
  },
  {
    "id": "python-sdk_src_acp_task_queue.py",
    "repo": "agentclientprotocol/python-sdk",
    "url": "https://github.com/agentclientprotocol/python-sdk/blob/main/src/acp/task/queue.py",
    "code": "from __future__ import annotations\n\nimport asyncio\nfrom collections.abc import AsyncIterator\nfrom contextlib import suppress\nfrom typing import Protocol\n\nfrom . import RpcTask\n\n__all__ = [\"InMemoryMessageQueue\", \"MessageQueue\"]\n\n\nclass MessageQueue(Protocol):\n    async def publish(self, task: RpcTask) -> None: ...\n\n    async def close(self) -> None: ...\n\n    def task_done(self) -> None: ...\n\n    async def join(self) -> None: ...\n\n    def __aiter__(self) -> AsyncIterator[RpcTask]: ...\n\n\nclass InMemoryMessageQueue:\n    \"\"\"Simple in-memory broker for RPC task dispatch.\"\"\"\n\n    def __init__(self, *, maxsize: int = 0) -> None:\n        self._queue: asyncio.Queue[RpcTask | None] = asyncio.Queue(maxsize=maxsize)\n        self._closed = False\n\n    async def publish(self, task: RpcTask) -> None:\n        if self._closed:\n            msg = \"mssage queue already closed\"\n            raise RuntimeError(msg)\n        await self._queue.put(task)\n\n    async def close(self) -> None:\n        if self._closed:\n            return\n        self._closed = True\n        await self._queue.put(None)\n\n    async def join(self) -> None:\n        await self._queue.join()\n\n    def task_done(self) -> None:\n        with suppress(ValueError):\n            self._queue.task_done()\n\n    def __aiter__(self) -> AsyncIterator[RpcTask]:\n        return _QueueIterator(self)\n\n\nclass _QueueIterator:\n    def __init__(self, queue: InMemoryMessageQueue) -> None:\n        self._queue = queue\n\n    def __aiter__(self) -> _QueueIterator:\n        return self\n\n    async def __anext__(self) -> RpcTask:\n        item = await self._queue._queue.get()\n        if item is None:\n            self._queue.task_done()\n            raise StopAsyncIteration\n        return item\n",
    "line_count": 67
  },
  {
    "id": "vocotype-cli_app_hotkeys.py",
    "repo": "233stone/vocotype-cli",
    "url": "https://github.com/233stone/vocotype-cli/blob/master/app/hotkeys.py",
    "code": "\"\"\"Global hotkey management for the application.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport threading\nfrom typing import Callable\n\nimport keyboard\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass HotkeyManager:\n    def __init__(self) -> None:\n        self._lock = threading.Lock()\n        self._registrations = {}\n\n    def register(self, combo: str, callback: Callable[[], None]) -> None:\n        with self._lock:\n            if combo in self._registrations:\n                logger.warning(\"çƒ­é”® %s å·²æ³¨å†Œï¼Œè¦†ç›–æ—§çš„å›è°ƒ\", combo)\n                keyboard.remove_hotkey(self._registrations[combo])\n\n            try:\n                hotkey_id = keyboard.add_hotkey(combo, callback)\n            except Exception as exc:  # noqa: BLE001\n                logger.error(\"æ³¨å†Œçƒ­é”® %s å¤±è´¥: %s\", combo, exc)\n                raise\n\n            self._registrations[combo] = hotkey_id\n            logger.info(\"å·²æ³¨å†Œçƒ­é”® %s\", combo)\n\n    def unregister_all(self) -> None:\n        with self._lock:\n            for combo, hotkey_id in list(self._registrations.items()):\n                keyboard.remove_hotkey(hotkey_id)\n                logger.info(\"å·²ç§»é™¤çƒ­é”® %s\", combo)\n            self._registrations.clear()\n\n    def cleanup(self) -> None:\n        self.unregister_all()\n        # å½»åº•åœæ­¢ keyboard åº“çš„æ‰€æœ‰é’©å­å’Œç›‘å¬çº¿ç¨‹\n        try:\n            keyboard.unhook_all()\n            logger.info(\"å·²åœæ­¢ keyboard ç›‘å¬çº¿ç¨‹\")\n        except Exception as exc:\n            logger.warning(\"åœæ­¢ keyboard ç›‘å¬çº¿ç¨‹å¤±è´¥: %s\", exc)\n\n\n",
    "line_count": 51
  },
  {
    "id": "PointCloud-Operations_src_Registry.py",
    "repo": "ElijahZh/PointCloud-Operations",
    "url": "https://github.com/ElijahZh/PointCloud-Operations/blob/main/src/Registry.py",
    "code": "from addict import Dict\n# import inspect\n\nclass Registry:\n    def __init__(self, name):\n        self._name = name\n        self._registry = Dict()\n\n    def register(self, register_name=None):\n        if register_name:\n            assert isinstance(register_name, str)\n\n        def decorator(item):\n            # nonlocal name\n            name = register_name or item.__name__\n            if name in self._registry:\n                raise KeyError(f\"item `{name}` already in the Registry(`{self._name}`) \")\n            self._registry[name] = item\n            return item\n\n        return decorator\n\n    def __len__(self):\n        return len(self._registry)\n\n    def __contains__(self, name):\n        return self._registry[name] is not None\n\n    def __getitem__(self, item):\n        return self._registry[item]\n\n    def get(self, name):\n        return self._registry[name]\n\n    def list_items(self):\n        # tmp = cls.registries[category]\n        return list(self._registry.keys())\n\n    def __repr__(self):\n        res = f\"Registry Name: {self._name}\\n\"\n        res += f\"Contains {len(self.list_items())} items\\n\"\n        res += f\"{self.list_items()}\"\n        return res\n\nMETHODS = Registry(\"methods\")\n\n@METHODS.register(register_name=\"first method\")\ndef func(a):\n    return a\n\n@METHODS.register(register_name=\"second method\")\ndef func_b(b):\n    return b\n\nif __name__ == '__main__':\n    print(METHODS.list_items())\n    print(METHODS.get(\"method\"))\n",
    "line_count": 57
  },
  {
    "id": "bridgic_packages_bridgic-core_bridgic_core_config__global_setting.py",
    "repo": "bitsky-tech/bridgic",
    "url": "https://github.com/bitsky-tech/bridgic/blob/main/packages/bridgic-core/bridgic/core/config/_global_setting.py",
    "code": "\"\"\"\nGlobal settings for the Bridgic framework.\n\"\"\"\n\nfrom typing import List, Optional, ClassVar, TYPE_CHECKING\nfrom pydantic import BaseModel\nfrom threading import Lock\n\nif TYPE_CHECKING:\n    from bridgic.core.automa.worker._worker_callback import WorkerCallbackBuilder\n\n\nclass GlobalSetting(BaseModel):\n    \"\"\"\n    Global configuration settings for the Bridgic framework.\n\n    This class implements a singleton pattern to provide centralized configuration\n    that applies across all Automa instances. The main methods are:\n\n    - `GlobalSetting.read()`: Get the singleton global setting instance.\n    - `GlobalSetting.set()`: Set the specific fields of the global setting instance.\n\n    Attributes\n    ----------\n    callback_builders : List[WorkerCallbackBuilder]\n        Callback builders that will be automatically applied to all workers\n        across all Automa instances.\n    \"\"\"\n    model_config = {\"arbitrary_types_allowed\": True}\n\n    callback_builders: List[\"WorkerCallbackBuilder\"] = []\n    \"\"\"Global callback builders that will be applied to all workers.\"\"\"\n\n    # Singleton instance\n    _instance: ClassVar[Optional[\"GlobalSetting\"]] = None\n    _lock: ClassVar[Lock] = Lock()\n\n    @classmethod\n    def read(cls) -> \"GlobalSetting\":\n        \"\"\"\n        Get the singleton global setting instance.\n        \n        Returns\n        -------\n        GlobalSetting\n            The singleton global setting instance.\n        \"\"\"\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = cls()\n        return cls._instance\n\n    @classmethod\n    def set(\n        cls,\n        callback_builders: Optional[List[\"WorkerCallbackBuilder\"]] = None,\n    ) -> None:\n        \"\"\"\n        Set global setting fields.\n        \n        This method allows you to update specific fields of the global setting\n        without needing to create a complete GlobalSetting object.\n        \n        Parameters\n        ----------\n        callback_builders : Optional[List[WorkerCallbackBuilder]], optional\n            Global callback builders that will be applied to all workers.\n            If None, the current callback_builders are not changed.\n        \"\"\"\n        instance = cls.read()\n        with cls._lock:\n            if callback_builders is not None:\n                instance.callback_builders = callback_builders\n\n    @classmethod\n    def add(cls, callback_builder: Optional[\"WorkerCallbackBuilder\"] = None) -> None:\n        \"\"\"\n        Add new element to the existing field(s) of the `GlobalSetting`.\n\n        Parameters\n        ----------\n        callback_builder : Optional[WorkerCallbackBuilder]\n            The callback builder to add to the global setting callback builders. If None is passed in, nothing will be done.\n        \"\"\"\n        instance = cls.read()\n        with cls._lock:\n            if callback_builder is not None:\n                instance.callback_builders.append(callback_builder)\n",
    "line_count": 89
  },
  {
    "id": "langrepl_src_langrepl_checkpointer_base.py",
    "repo": "midodimori/langrepl",
    "url": "https://github.com/midodimori/langrepl/blob/main/src/langrepl/checkpointer/base.py",
    "code": "\"\"\"Base checkpointer with extended methods for langrepl.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any\n\nfrom langgraph.checkpoint.base import BaseCheckpointSaver as _BaseCheckpointSaver\n\nif TYPE_CHECKING:\n    from langchain_core.messages import BaseMessage\n    from langgraph.checkpoint.base import CheckpointTuple\n\n\n@dataclass\nclass HumanMessageEntry:\n    \"\"\"Human message with replay metadata.\"\"\"\n\n    text: str\n    reference_mapping: dict[str, Any]\n    messages_before_count: int\n    checkpoint_id: str | None\n    input_tokens: int | None = None\n    output_tokens: int | None = None\n    total_cost: float | None = None\n\n\nclass BaseCheckpointer(_BaseCheckpointSaver):\n    \"\"\"Base checkpointer with additional query methods.\"\"\"\n\n    async def get_threads(self) -> set[str]:\n        \"\"\"Get all thread IDs.\"\"\"\n        raise NotImplementedError\n\n    async def get_history(self, latest: CheckpointTuple) -> list[CheckpointTuple]:\n        \"\"\"Get checkpoint history in chronological order (oldest first).\"\"\"\n        raise NotImplementedError\n\n    async def delete_after(self, thread_id: str, checkpoint_id: str | None) -> int:\n        \"\"\"Delete checkpoints after checkpoint_id. Returns count deleted.\"\"\"\n        raise NotImplementedError\n\n    async def get_human_messages(\n        self,\n        thread_id: str,\n        latest: CheckpointTuple,\n        on_indexing: Callable[[], None] | None = None,\n    ) -> tuple[list[HumanMessageEntry], list[BaseMessage]]:\n        \"\"\"Get human messages with replay metadata.\n\n        Returns:\n            Tuple of (human_messages, all_messages)\n        \"\"\"\n        raise NotImplementedError\n",
    "line_count": 55
  },
  {
    "id": "MobileWorld_src_mobile_world_tasks_definitions_calendar_add_business_trip_with_cafe.py",
    "repo": "Tongyi-MAI/MobileWorld",
    "url": "https://github.com/Tongyi-MAI/MobileWorld/blob/main/src/mobile_world/tasks/definitions/calendar/add_business_trip_with_cafe.py",
    "code": "\"\"\"Add business trip calendar event with nearby cafe information.\"\"\"\n\nfrom mobile_world.runtime.app_helpers import mcp as mcp_helper\nfrom mobile_world.runtime.app_helpers.fossify_calendar import get_calendar_events\nfrom mobile_world.runtime.controller import AndroidController\nfrom mobile_world.tasks.base import BaseTask\n\n\nclass AddBusinessTripWithCafeTask(BaseTask):\n    \"\"\"Add business trip calendar event with nearby cafe address in description.\"\"\"\n\n    goal = (\n        \"æˆ‘ä¸‹å‘¨å…­10:00am-12:30pmè¦å»ã€Œä¸Šæµ·è™¹æ¡¥ç«è½¦ç«™ã€ï¼Œæ·»åŠ äº‹é¡¹åˆ°calenderï¼Œäº‹é¡¹ä¸ºå‡ºå·®ï¼Œ\"\n        'ä½ å¸®æˆ‘æ‰¾åˆ°ç¦»ä¸Šæµ·è™¹æ¡¥ç«è½¦ç«™10å…¬é‡Œä»¥å†…çš„æ™¯ç‚¹ï¼Œæˆ‘å‘¨ä¸€ä¸Šç­å‰å»å‚è§‚ä¸‹ï¼ŒæŒ‰ç…§\"æ™¯ç‚¹åå­—ï¼šåœ°å€\"æ”¾å…¥calenderäº‹ä»¶çš„æè¿°ä¸­ï¼Œå¤šä¸ªæ™¯ç‚¹æŒ‰é€—å·åˆ†éš”'\n    )\n    task_tags = {\"agent-mcp\", \"lang-cn\"}\n\n    EVENT_TITLE = \"å‡ºå·®\"\n    SEARCH_KEYWORDS = \"æ™¯ç‚¹\"\n    SEARCH_RADIUS = \"10000\"  # 10å…¬é‡Œ = 10000ç±³\n    DESTINATION_LOCATION = \"121.323774, 31.193241\"\n\n    app_names = {\"MCP-Amap\", \"Calendar\"}\n\n    def initialize_task_hook(self, controller: AndroidController) -> bool:\n        return True\n\n    async def is_successful_async(self, controller: AndroidController) -> float | tuple[float, str]:\n        self._check_is_initialized()\n\n        landmark_list = await mcp_helper.search_nearby(\n            location=self.DESTINATION_LOCATION,\n            radius=self.SEARCH_RADIUS,\n            keywords=self.SEARCH_KEYWORDS,\n        )\n\n        events = get_calendar_events()\n        for event in events:\n            if self.EVENT_TITLE not in event.get(\"title\", \"\"):\n                continue\n            description = event.get(\"description\", \"\")\n            percentage = sum(landmark in description for landmark in landmark_list) / len(\n                landmark_list\n            )\n            if percentage > 0.8:\n                return 1.0\n            else:\n                return 0.0, \"Event description does not contain correct format\"\n        return (\n            0.0,\n            f\"Calendar event not found with title '{self.EVENT_TITLE}' containing attractions\",\n        )\n\n    def tear_down(self, controller: AndroidController) -> bool:\n        super().tear_down(controller)\n        return True\n",
    "line_count": 56
  },
  {
    "id": "General-Visual-Quality-RL_src_vlm_modules_vlm_module.py",
    "repo": "DanceSkyCode/General-Visual-Quality-RL",
    "url": "https://github.com/DanceSkyCode/General-Visual-Quality-RL/blob/main/src/vlm_modules/vlm_module.py",
    "code": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\n\nclass VLMBaseModule(ABC):\n    def __init__(self):\n        super().__init__()\n    \n    @abstractmethod\n    def get_vlm_key(self):\n        pass\n\n    @abstractmethod\n    def get_model_class(self, model_id: str, model_init_kwargs: dict):\n        pass\n\n    def post_model_init(self, model, processing_class):\n        pass\n\n    def is_embeds_input(self):\n        return False\n    \n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n    @abstractmethod\n    def get_vision_modules_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_custom_multimodal_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_non_generate_params(self):\n        pass\n\n    @abstractmethod\n    def get_custom_processing_keywords(self):\n        pass\n\n    @abstractmethod\n    def prepare_prompt(self, processing_class, inputs: dict[str, Union[torch.Tensor, Any]]):\n        pass\n    \n    @abstractmethod\n    def prepare_model_inputs(self, processing_class, prompts_text, images, return_tensors, padding, padding_side, add_special_tokens):\n        pass",
    "line_count": 50
  },
  {
    "id": "fs-explorer_packages_eval-framework_src_eval_framework__templating.py",
    "repo": "run-llama/fs-explorer",
    "url": "https://github.com/run-llama/fs-explorer/blob/main/packages/eval-framework/src/eval_framework/_templating.py",
    "code": "import re\n\nPATTERN = re.compile(r\"\\{\\{([^\\}]+)\\}\\}\")\n\n\nclass TemplateValidationError(Exception):\n    \"\"\"Raised when the arguments to render a template fail to validate\"\"\"\n\n\nclass Template:\n    \"\"\"\n    Jinja2-like class for string templating\n\n    Attributes:\n        content (str): original template string\n        _to_render (list[str]): fields of the string that have to be rendered with the template\n    \"\"\"\n\n    def __init__(self, content: str):\n        \"\"\"\n        Create a template from a string.\n\n        Args:\n            content (str): the template string\n        \"\"\"\n        self.content = content\n        self._to_render = PATTERN.findall(content)\n\n    def _validate(self, args: dict[str, str]) -> bool:\n        return all(el in args for el in self._to_render) and all(\n            isinstance(args[k], str) for k in args\n        )\n\n    def render(self, args: dict[str, str]) -> str:\n        \"\"\"\n        Render the template.\n\n        Args:\n            args (dict[str, str]): a dictionary of arguments for the template to be rendered. The keys represent the fields in the template, and the values represent the strings with which to fill the template.\n\n        Returns:\n            str: The rendered template string.\n        \"\"\"\n        if self._validate(args):\n            content = self.content\n            for word in self._to_render:\n                content = content.replace(\"{{\" + word + \"}}\", args[word])\n            return content\n        else:\n            if (ls := list(set(self._to_render) - set(list(args.keys())))) != []:\n                raise TemplateValidationError(\n                    f\"Missing the following arguments for the template: {', '.join(ls)}\"\n                )\n            else:\n                raise TemplateValidationError(\n                    \"You should provide a dictionary with only string values.\"\n                )\n",
    "line_count": 57
  },
  {
    "id": "InvestAI_src_engine_monitor.py",
    "repo": "flingjie/InvestAI",
    "url": "https://github.com/flingjie/InvestAI/blob/main/src/engine/monitor.py",
    "code": "from loguru import logger\nfrom .signal_engine import SignalEngine\nfrom .index_engine import IndexEngine\nfrom notifiers.formater.index import format_index_trend_message\nfrom notifiers.formater.stock import format_trend_signal_message\nfrom notifiers.manager import notification_manager\nimport time\nfrom agents.index_explainer import explain_index_trend\nfrom agents.stock_explainer import explain_stock_trend\nfrom utils.json import to_pretty_json\n\nclass StockMonitor:\n    def __init__(self, watchlist: dict, index_pool: dict, config: dict = {}):\n        self.watchlist = watchlist  \n        self.index_pool = index_pool\n        self.config = config\n\n    def check_index(self, index_symbol: str, index_name: str):\n        index_engine = IndexEngine()\n        context = index_engine.evaluate(index_symbol)\n        result = context['result']\n        result.update({\n            \"name\": index_name,\n        })\n        return result\n\n\n    def check_stock(self, symbol: str, stock_name: str):\n        signal_engine = SignalEngine()\n        context = signal_engine.evaluate(symbol)\n        result = context['result']\n        result.update({\n            \"name\": stock_name,\n        })\n        message = explain_stock_trend(result)\n        logger.debug(message)\n        notification_manager.notify(f\"\"\"\n        {message}\\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n        \"\"\")\n\n\n    def run(self):\n        index_result = []\n        for name, symbol in self.index_pool.items():\n            try:\n                result = self.check_index(symbol, name)\n                index_result.append(result)\n                time.sleep(1)\n            except Exception as e:\n                logger.exception(f\"Error processing {symbol}: {e}\")\n        \n        message = explain_index_trend(index_result)\n        # logger.debug(message)\n        notification_manager.notify(f\"\"\"\n        {message}\\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n        \"\"\")\n\n        for name, symbol in self.watchlist.items():\n            try:\n                self.check_stock(symbol, name)\n                time.sleep(1)\n            except Exception as e:\n                logger.exception(f\"Error processing {symbol}: {e}\")\n \n\n\nif __name__ == \"__main__\":\n    from tools.watch_list import load_watchlist\n    from tools.index_tool import load_index_pool\n    from config import WATCHLIST_PATH, INDEX_POOL_PATH\n    watchlist = load_watchlist(WATCHLIST_PATH)\n    index_pool = load_index_pool(INDEX_POOL_PATH)\n    monitor = StockMonitor(\n        watchlist=watchlist,\n        index_pool=index_pool,\n    )\n    monitor.run()\n",
    "line_count": 77
  },
  {
    "id": "X-AnyLabeling-Server_app_models_yolo11.py",
    "repo": "CVHub520/X-AnyLabeling-Server",
    "url": "https://github.com/CVHub520/X-AnyLabeling-Server/blob/main/app/models/yolo11.py",
    "code": "import numpy as np\nfrom typing import Any, Dict\n\nfrom . import BaseModel\nfrom app.schemas.shape import Shape\nfrom app.core.registry import register_model\n\n\n@register_model(\"yolo11n\", \"yolo11s\", \"yolo11m\", \"yolo11l\", \"yolo11x\")\nclass YOLO11Detection(BaseModel):\n    \"\"\"YOLO11 object detection model.\"\"\"\n\n    def load(self):\n        \"\"\"Load YOLO model.\"\"\"\n        from ultralytics import YOLO\n\n        model_path = self.params.get(\"model_path\", \"yolo11n.pt\")\n        device = self.params.get(\"device\", \"cpu\")\n\n        self.model = YOLO(model_path)\n        self.model.to(device)\n\n        dummy_img = np.zeros((640, 640, 3), dtype=np.uint8)\n        self.model(dummy_img, verbose=False)\n\n    def predict(\n        self, image: np.ndarray, params: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Execute object detection.\n\n        Args:\n            image: Input image in BGR format.\n            params: Inference parameters.\n\n        Returns:\n            Dictionary with detection results.\n        \"\"\"\n        conf_threshold = params.get(\n            \"conf_threshold\", self.params.get(\"conf_threshold\", 0.25)\n        )\n        iou_threshold = params.get(\n            \"iou_threshold\", self.params.get(\"iou_threshold\", 0.45)\n        )\n\n        results = self.model(\n            image, conf=conf_threshold, iou=iou_threshold, verbose=False\n        )\n\n        shapes = []\n        for result in results:\n            boxes = result.boxes\n            if boxes is not None:\n                for box in boxes:\n                    xyxy = box.xyxy[0].cpu().numpy()\n                    conf = float(box.conf[0])\n                    cls = int(box.cls[0])\n                    label = result.names[cls]\n\n                    shape = Shape(\n                        label=label,\n                        shape_type=\"rectangle\",\n                        points=[\n                            [float(xyxy[0]), float(xyxy[1])],\n                            [float(xyxy[2]), float(xyxy[1])],\n                            [float(xyxy[2]), float(xyxy[3])],\n                            [float(xyxy[0]), float(xyxy[3])],\n                        ],\n                        score=conf,\n                    )\n                    shapes.append(shape)\n\n        return {\"shapes\": shapes, \"description\": \"\"}\n\n    def unload(self):\n        \"\"\"Release model resources.\"\"\"\n        if hasattr(self, \"model\"):\n            del self.model\n",
    "line_count": 77
  },
  {
    "id": "VisionQuant-Pro_src_utils_pdf_generator.py",
    "repo": "panyisheng095-ux/VisionQuant-Pro",
    "url": "https://github.com/panyisheng095-ux/VisionQuant-Pro/blob/main/src/utils/pdf_generator.py",
    "code": "from fpdf import FPDF\nimport datetime\nimport os\nimport platform\n\n\nclass QuantPDFReport(FPDF):\n    def __init__(self):\n        super().__init__()\n        self.font_path = self._get_chinese_font()\n        # æ³¨å†Œå­—ä½“ (å…³é”®æ­¥éª¤)\n        if self.font_path:\n            self.add_font('ChineseFont', '', self.font_path, uni=True)\n            self.has_font = True\n        else:\n            self.has_font = False\n\n    def _get_chinese_font(self):\n        \"\"\"è‡ªåŠ¨å¯»æ‰¾ç³»ç»Ÿä¸­çš„ä¸­æ–‡å­—ä½“\"\"\"\n        system = platform.system()\n        # å¸¸è§çš„ä¸­æ–‡å­—ä½“è·¯å¾„\n        paths = [\n            \"/System/Library/Fonts/PingFang.ttc\",  # Mac\n            \"/System/Library/Fonts/STHeiti Light.ttc\",  # Mac\n            \"/Library/Fonts/Arial Unicode.ttf\",  # Mac Office\n            \"C:\\\\Windows\\\\Fonts\\\\simhei.ttf\",  # Windows\n            \"C:\\\\Windows\\\\Fonts\\\\msyh.ttf\",  # Windows\n            \"./SimHei.ttf\"  # é¡¹ç›®æ ¹ç›®å½•è‡ªå®šä¹‰\n        ]\n        for p in paths:\n            if os.path.exists(p):\n                return p\n        return None\n\n    def header(self):\n        if self.has_font:\n            self.set_font('ChineseFont', '', 16)\n        else:\n            self.set_font('helvetica', 'B', 15)\n        self.cell(0, 10, 'VisionQuant Pro æ™ºèƒ½æŠ•ç ”æŠ¥å‘Š', 0, 1, 'C')\n        self.ln(5)\n\n\ndef generate_report_pdf(symbol, report_data, plot_path, output_path):\n    pdf = QuantPDFReport()\n    pdf.add_page()\n\n    # è®¾ç½®æ­£æ–‡å­—ä½“\n    if pdf.has_font:\n        pdf.set_font('ChineseFont', '', 10)\n    else:\n        pdf.set_font('helvetica', '', 10)\n        pdf.cell(0, 10, \"Warning: Chinese font not found. Characters may be missing.\", 0, 1)\n\n    # 1. åŸºç¡€ä¿¡æ¯\n    pdf.set_fill_color(240, 240, 240)\n    pdf.cell(0, 10, f\"æ ‡çš„: {symbol} | ç”Ÿæˆæ—¥æœŸ: {datetime.datetime.now().strftime('%Y-%m-%d')}\", 0, 1, 'L', True)\n\n    # 2. å†³ç­–ç»“è®º\n    pdf.ln(5)\n    pdf.set_font(size=14)\n    pdf.cell(0, 10, f\"æœ€ç»ˆå†³ç­–: {report_data.action}\", 0, 1)\n    pdf.set_font(size=10)\n    pdf.cell(0, 8, f\"ä¿¡å¿ƒæŒ‡æ•°: {report_data.confidence}/100  |  é£é™©è¯„ä¼°: {report_data.risk_level}\", 0, 1)\n\n    # 3. æ’å…¥å›¾ç‰‡\n    if plot_path and os.path.exists(plot_path):\n        pdf.ln(5)\n        # è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥é€‚åº”é¡µé¢ (A4 å®½çº¦ 210mm)\n        pdf.image(plot_path, x=10, w=190)\n\n    # 4. åˆ†æç†ç”±\n    pdf.ln(10)\n    pdf.set_font(size=12)\n    pdf.cell(0, 10, \"æ·±åº¦ç ”åˆ¤é€»è¾‘:\", 0, 1)\n    pdf.set_font(size=10)\n\n    # å†™å…¥é•¿æ–‡æœ¬\n    text = report_data.reasoning\n    # å¦‚æœæ²¡æœ‰ä¸­æ–‡å­—ä½“ï¼Œä¸´æ—¶è½¬ç é˜²æ­¢æŠ¥é”™\n    if not pdf.has_font:\n        text = text.encode('latin-1', 'replace').decode('latin-1')\n\n    pdf.multi_cell(0, 6, text)\n\n    pdf.output(output_path)\n    print(f\"PDF Generated: {output_path}\")",
    "line_count": 87
  },
  {
    "id": "NOVIX_backend_app_llm_gateway_providers_anthropic_provider.py",
    "repo": "unitagain/NOVIX",
    "url": "https://github.com/unitagain/NOVIX/blob/main/backend/app/llm_gateway/providers/anthropic_provider.py",
    "code": "\"\"\"\nAnthropic (Claude) Provider / Anthropic (Claude) é€‚é…å™¨\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom anthropic import AsyncAnthropic\nfrom app.llm_gateway.providers.base import BaseLLMProvider\n\n\nclass AnthropicProvider(BaseLLMProvider):\n    \"\"\"Anthropic API provider / Anthropic API æä¾›å•†\"\"\"\n    \n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"claude-3-5-sonnet-20241022\",\n        max_tokens: int = 8000,\n        temperature: float = 0.7\n    ):\n        super().__init__(api_key, model, max_tokens, temperature)\n        self.client = AsyncAnthropic(api_key=api_key)\n    \n    async def chat(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Send chat request to Anthropic\n        å‘é€èŠå¤©è¯·æ±‚åˆ° Anthropic\n        \n        Args:\n            messages: List of messages / æ¶ˆæ¯åˆ—è¡¨\n            temperature: Override temperature / è¦†ç›–æ¸©åº¦\n            max_tokens: Override max tokens / è¦†ç›–æœ€å¤§tokenæ•°\n            \n        Returns:\n            Response dict / å“åº”å­—å…¸\n        \"\"\"\n        # Extract system message if present / æå–ç³»ç»Ÿæ¶ˆæ¯\n        system_message = None\n        filtered_messages = []\n        \n        for msg in messages:\n            if msg[\"role\"] == \"system\":\n                system_message = msg[\"content\"]\n            else:\n                filtered_messages.append(msg)\n        \n        # Anthropic API call / Anthropic API è°ƒç”¨\n        kwargs = {\n            \"model\": self.model,\n            \"messages\": filtered_messages,\n            \"temperature\": temperature or self.temperature,\n            \"max_tokens\": max_tokens or self.max_tokens\n        }\n        \n        if system_message:\n            kwargs[\"system\"] = system_message\n        \n        response = await self.client.messages.create(**kwargs)\n        \n        return {\n            \"content\": response.content[0].text,\n            \"usage\": {\n                \"prompt_tokens\": response.usage.input_tokens,\n                \"completion_tokens\": response.usage.output_tokens,\n                \"total_tokens\": response.usage.input_tokens + response.usage.output_tokens\n            },\n            \"model\": response.model,\n            \"finish_reason\": response.stop_reason\n        }\n    \n    def get_provider_name(self) -> str:\n        \"\"\"Get provider name / è·å–æä¾›å•†åç§°\"\"\"\n        return \"anthropic\"\n",
    "line_count": 77
  },
  {
    "id": "CatieCli_backend_app_cache.py",
    "repo": "mzrodyu/CatieCli",
    "url": "https://github.com/mzrodyu/CatieCli/blob/main/backend/app/cache.py",
    "code": "\"\"\"\nç®€å•å†…å­˜ç¼“å­˜ï¼Œç”¨äºå‡å°‘æ•°æ®åº“æŸ¥è¯¢\nä¸éœ€è¦ Redisï¼Œé€‚åˆä¸­å°å‹éƒ¨ç½²\n\"\"\"\n\nimport time\nfrom typing import Any, Optional\nfrom functools import wraps\n\nclass SimpleCache:\n    \"\"\"ç®€å•çš„å†…å­˜ç¼“å­˜\"\"\"\n    \n    def __init__(self):\n        self._cache = {}\n        self._expires = {}\n    \n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"è·å–ç¼“å­˜å€¼\"\"\"\n        if key not in self._cache:\n            return None\n        if key in self._expires and time.time() > self._expires[key]:\n            del self._cache[key]\n            del self._expires[key]\n            return None\n        return self._cache[key]\n    \n    def set(self, key: str, value: Any, ttl: int = 60):\n        \"\"\"è®¾ç½®ç¼“å­˜å€¼ï¼Œttl ä¸ºè¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰\"\"\"\n        self._cache[key] = value\n        self._expires[key] = time.time() + ttl\n    \n    def delete(self, key: str):\n        \"\"\"åˆ é™¤ç¼“å­˜\"\"\"\n        self._cache.pop(key, None)\n        self._expires.pop(key, None)\n    \n    def clear(self):\n        \"\"\"æ¸…ç©ºæ‰€æœ‰ç¼“å­˜\"\"\"\n        self._cache.clear()\n        self._expires.clear()\n    \n    def clear_prefix(self, prefix: str):\n        \"\"\"æ¸…é™¤æŒ‡å®šå‰ç¼€çš„ç¼“å­˜\"\"\"\n        keys_to_delete = [k for k in self._cache if k.startswith(prefix)]\n        for key in keys_to_delete:\n            self.delete(key)\n\n\n# å…¨å±€ç¼“å­˜å®ä¾‹\ncache = SimpleCache()\n\n\n# ç¼“å­˜ key å‰ç¼€\nCACHE_KEYS = {\n    \"stats\": \"stats:\",           # ç»Ÿè®¡æ•°æ®ç¼“å­˜\n    \"user\": \"user:\",             # ç”¨æˆ·ä¿¡æ¯ç¼“å­˜\n    \"creds\": \"creds:\",           # å‡­è¯åˆ—è¡¨ç¼“å­˜\n    \"quota\": \"quota:\",           # é…é¢ç¼“å­˜\n}\n\n\ndef cached(prefix: str, ttl: int = 30):\n    \"\"\"\n    ç¼“å­˜è£…é¥°å™¨\n    ç”¨æ³•ï¼š\n    @cached(\"stats\", ttl=10)\n    async def get_stats():\n        ...\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            # ç”Ÿæˆç¼“å­˜ key\n            key = f\"{prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}\"\n            \n            # å°è¯•ä»ç¼“å­˜è·å–\n            result = cache.get(key)\n            if result is not None:\n                return result\n            \n            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ\n            result = await func(*args, **kwargs)\n            cache.set(key, result, ttl)\n            return result\n        return wrapper\n    return decorator\n\n\ndef invalidate_cache(prefix: str = None):\n    \"\"\"æ¸…é™¤ç¼“å­˜\"\"\"\n    if prefix:\n        cache.clear_prefix(prefix)\n    else:\n        cache.clear()\n",
    "line_count": 94
  },
  {
    "id": "ComfyUI-dapaoAPI_dapao_template_node.py",
    "repo": "paolaoshi/ComfyUI-dapaoAPI",
    "url": "https://github.com/paolaoshi/ComfyUI-dapaoAPI/blob/main/dapao_template_node.py",
    "code": "\"\"\"\nDapao Prompt Master\nTemplate Manager for Dapao Image Prompts\n\"\"\"\n\nimport os\nimport sys\n\ntry:\n    from .dapao_template_adapter import DapaoPromptTemplateAdapter\nexcept ImportError:\n    from dapao_template_adapter import DapaoPromptTemplateAdapter\n\n\nclass DapaoPromptNode:\n    \"\"\"\n    Dapao Prompt Node - Browse and use prompt templates\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize with template adapter\"\"\"\n        try:\n            self.adapter = DapaoPromptTemplateAdapter()\n            self.initialized = True\n        except Exception as e:\n            print(f\"[Dapao] ERROR: Failed to initialize adapter: {e}\")\n            self.adapter = None\n            self.initialized = False\n    \n    @classmethod\n    def INPUT_TYPES(cls):\n        \"\"\"Define node inputs\"\"\"\n        return {\n            \"required\": {\n                \"prompt\": (\"STRING\", {\n                    \"multiline\": True,\n                    \"default\": \"\",\n                    \"placeholder\": \"åœ¨æ­¤è¾“å…¥æ‚¨çš„æç¤ºè¯...\\n\\nç‚¹å‡»ä¸‹æ–¹çš„ã€Œæµè§ˆæ¨¡æ¿ã€æŒ‰é’®åŠ è½½æ¨¡æ¿ã€‚\",\n                    \"dynamicPrompts\": False\n                })\n            }\n        }\n    \n    RETURN_TYPES = (\"STRING\",)\n    RETURN_NAMES = (\"final_prompt\",)\n    FUNCTION = \"generate_prompt\"\n    CATEGORY = \"ğŸ¤–dapaoAPI/Nano Banana 2\"\n    OUTPUT_NODE = False\n    \n    def generate_prompt(self, prompt=\"\"):\n        \"\"\"\n        Generate final prompt\n        \"\"\"\n        return (prompt,)\n\n\n# ======================== Node Registration ========================\n\nNODE_CLASS_MAPPINGS = {\n    \"DapaoPromptNode\": DapaoPromptNode\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"DapaoPromptNode\": \"ğŸ¨ å¤§ç‚®bannanæ–‡ç”Ÿå›¾æç¤ºè¯\"\n}\n",
    "line_count": 65
  },
  {
    "id": "haproxy-openmanager_backend_models_waf.py",
    "repo": "taylanbakircioglu/haproxy-openmanager",
    "url": "https://github.com/taylanbakircioglu/haproxy-openmanager/blob/main/backend/models/waf.py",
    "code": "from pydantic import BaseModel, validator\nfrom typing import Optional, List\nimport re\nimport ipaddress\n\nclass WAFRule(BaseModel):\n    name: str\n    rule_type: str  # 'rate_limit', 'ip_filter', 'header_filter', 'request_filter', 'geo_block', 'size_limit'\n    action: str = 'block'  # 'block', 'allow', 'log', 'redirect'\n    priority: int = 100\n    is_active: bool = True\n    config: dict  # This will hold all rule-specific configurations\n    description: Optional[str] = None\n    \n    # Frontend associations (multiple) - optional during creation\n    frontend_ids: Optional[List[int]] = None\n    \n    # CRITICAL VALIDATION RULES FOR HAPROXY WAF\n    @validator('name')\n    def validate_name(cls, v):\n        if not v or not v.strip():\n            raise ValueError('WAF rule name cannot be empty')\n        \n        # HAProxy ACL names cannot contain spaces or special characters\n        if not re.match(r'^[a-zA-Z0-9_-]+$', v.strip()):\n            raise ValueError('WAF rule name can only contain letters, numbers, underscore (_) and dash (-). Spaces and special characters are not allowed.')\n        \n        if len(v.strip()) > 50:\n            raise ValueError('WAF rule name cannot exceed 50 characters')\n            \n        return v.strip()\n    \n    @validator('rule_type')\n    def validate_rule_type(cls, v):\n        valid_types = ['rate_limit', 'ip_filter', 'header_filter', 'request_filter', 'geo_block', 'size_limit', 'path_filter']\n        if v not in valid_types:\n            raise ValueError(f'Invalid rule type. Must be one of: {\", \".join(valid_types)}')\n        return v\n    \n    @validator('action')\n    def validate_action(cls, v):\n        valid_actions = ['block', 'allow', 'log', 'redirect']\n        if v not in valid_actions:\n            raise ValueError(f'Invalid action. Must be one of: {\", \".join(valid_actions)}')\n        return v\n    \n    @validator('priority')\n    def validate_priority(cls, v):\n        if not isinstance(v, int) or v < 1 or v > 1000:\n            raise ValueError('Priority must be between 1 and 1000')\n        return v\n    \n\nclass WAFRuleUpdate(WAFRule):\n    name: Optional[str] = None\n    rule_type: Optional[str] = None\n    action: Optional[str] = None\n    priority: Optional[int] = None\n    is_active: Optional[bool] = None\n    config: Optional[dict] = None\n    description: Optional[str] = None\n    frontend_ids: Optional[List[int]] = None\n\nclass FrontendWAFRule(BaseModel):\n    waf_rule_id: int\n    is_active: bool = True\n\nclass SSLCertificate(BaseModel):\n    name: str\n    domain: str\n    certificate_content: str\n    private_key_content: str\n    chain_content: Optional[str] = None\n    expiry_date: Optional[str] = None ",
    "line_count": 74
  },
  {
    "id": "SmartDCN_main.py",
    "repo": "ILoveCoder999/SmartDCN",
    "url": "https://github.com/ILoveCoder999/SmartDCN/blob/main/main.py",
    "code": "import numpy as np\n\nimport matplotlib.pyplot as plt\nfrom config import Config\nfrom models.tpm import TPM\nfrom models.eom import EOM\nfrom environments.dcn_env import DCNEnvironment, Flow\nfrom utils.redundancy import check_routing_redundancy\nimport tensorflow as tf\n\n\nclass SmartDCN:\n    \"\"\"SmartDCN ä¸»æ¡†æ¶\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.network = DCNEnvironment(k=config.FAT_TREE_K)\n        self.tpm = TPM(\n            seq_length=config.SEQ_LENGTH,\n            pred_length=config.PREDICT_LENGTH,\n            hidden_units=config.HIDDEN_UNITS\n        )\n        self.eom = EOM(self.network, config)\n\n        # è®­ç»ƒå†å²\n        self.energy_savings_history = []\n        self.prediction_accuracy_history = []\n        self.reward_history = []\n\n    def generate_traffic_data(self, num_samples=1000):\n        \"\"\"ç”Ÿæˆè®­ç»ƒæ•°æ®\"\"\"\n        traffic_data = []\n\n        for i in range(num_samples):\n            # ç”Ÿæˆéšæœºæµé‡æ¨¡å¼\n            intensity = np.random.uniform(0.1, 0.9)\n            self.network.generate_traffic(intensity)\n            self.network.update_network_state()\n\n            # æ”¶é›†æµé‡ç»Ÿè®¡\n            total_volume = sum(flow.size for flow in self.network.flows)\n            avg_bandwidth = np.mean([flow.bandwidth for flow in self.network.flows]) if self.network.flows else 0\n            flow_count = len(self.network.flows)\n\n            traffic_data.append([total_volume, avg_bandwidth, flow_count])\n\n        return np.array(traffic_data)\n\n    def train_tpm(self, epochs=100):\n        \"\"\"è®­ç»ƒTPMæ¨¡å—\"\"\"\n        print(\"Training TPM...\")\n\n        # ç”Ÿæˆè®­ç»ƒæ•°æ®\n        traffic_data = self.generate_traffic_data(1000)\n\n        # å‡†å¤‡åºåˆ—æ•°æ®\n        X, y = [], []\n        for i in range(len(traffic_data) - self.config.SEQ_LENGTH - self.config.PREDICT_LENGTH):\n            X.append(traffic_data[i:i + self.config.SEQ_LENGTH])\n            y.append(traffic_data[i + self.config.SEQ_LENGTH:i + self.config.SEQ_LENGTH + self.config.PREDICT_LENGTH,\n                     0])  # é¢„æµ‹æ€»æµé‡\n\n        X = np.array(X)\n        y = np.array(y)\n\n        # ç¼–è¯‘æ¨¡å‹\n        self.tpm.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=self.config.LEARNING_RATE),\n            loss='mse',\n            metrics=['mae']\n        )\n\n        # è®­ç»ƒ\n        history = self.tpm.fit(\n            X, y,\n            batch_size=self.config.BATCH_SIZE,\n            epochs=epochs,\n            validation_split=0.2,\n            verbose=1\n        )\n\n        return history\n\n    def train_eom(self, episodes=500):\n        \"\"\"è®­ç»ƒEOMæ¨¡å—\"\"\"\n        print(\"Training EOM...\")\n\n        for episode in range(episodes):\n            # ç”Ÿæˆå½“å‰æµé‡\n            intensity = np.random.uniform(0.2, 0.8)\n            self.net",
    "line_count": 91
  },
  {
    "id": "ComfyUI-TutuBanana_TutuPromptMasterV3.py",
    "repo": "zhaotututu/ComfyUI-TutuBanana",
    "url": "https://github.com/zhaotututu/ComfyUI-TutuBanana/blob/main/TutuPromptMasterV3.py",
    "code": "\"\"\"\nTutu Prompt Master v3.0\nTemplate Manager for GPT-4o Image Prompts (333 Cases)\n\nA simplified prompt template browser with bilingual support.\n\nAuthor: Tutu API Team\nVersion: 3.0.0\n\"\"\"\n\nimport os\nimport sys\n\ntry:\n    from .template_adapter import PromptTemplateAdapter\nexcept ImportError:\n    from template_adapter import PromptTemplateAdapter\n\n\nclass TutuPromptMasterV3:\n    \"\"\"\n    Tutu Prompt Master v3.0 - Browse and use 333 GPT-4o prompt templates\n    \n    Features:\n    - 333 high-quality prompt templates across 10 categories\n    - Visual template browser with preview\n    - Bilingual support (Chinese/English)\n    - One-click template loading and combination\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize with template adapter\"\"\"\n        try:\n            self.adapter = PromptTemplateAdapter()\n            self.initialized = True\n        except Exception as e:\n            print(f\"[Tutu v3] ERROR: Failed to initialize adapter: {e}\")\n            self.adapter = None\n            self.initialized = False\n    \n    @classmethod\n    def INPUT_TYPES(cls):\n        \"\"\"Define node inputs\"\"\"\n        return {\n            \"required\": {\n                \"prompt\": (\"STRING\", {\n                    \"multiline\": True,\n                    \"default\": \"\",\n                    \"placeholder\": \"åœ¨æ­¤è¾“å…¥æ‚¨çš„æç¤ºè¯...\\n\\nç‚¹å‡»ä¸‹æ–¹çš„ã€Œæµè§ˆæ¨¡æ¿ã€æŒ‰é’®åŠ è½½æ¨¡æ¿ã€‚\",\n                    \"dynamicPrompts\": False\n                })\n            }\n        }\n    \n    RETURN_TYPES = (\"STRING\",)\n    RETURN_NAMES = (\"final_prompt\",)  # ä¿æŒè‹±æ–‡ä»¥å…¼å®¹å·¥ä½œæµ\n    FUNCTION = \"generate_prompt\"\n    CATEGORY = \"Tutu\"\n    OUTPUT_NODE = False\n    \n    def generate_prompt(self, prompt=\"\"):\n        \"\"\"\n        Generate final prompt\n        \n        Args:\n            prompt: User's prompt text (can be manually edited or loaded from templates)\n        \n        Returns:\n            (final_prompt,)\n        \"\"\"\n        # Simply return the prompt as-is\n        return (prompt,)\n\n\n# ======================== Node Registration ========================\n\nNODE_CLASS_MAPPINGS = {\n    \"TutuPromptMasterV3\": TutuPromptMasterV3\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"TutuPromptMasterV3\": \"ğŸ¨ Tutu å›¾å›¾é¦™è•‰æ¨¡å‹æç¤ºè¯æ¨¡æ¿ç®¡ç†å™¨\"\n}\n\n",
    "line_count": 84
  },
  {
    "id": "SmartDB_MCP_src_oauth_middleware.py",
    "repo": "wenb1n-dev/SmartDB_MCP",
    "url": "https://github.com/wenb1n-dev/SmartDB_MCP/blob/main/src/oauth/middleware.py",
    "code": "from typing import Optional\n\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\nfrom oauth.token_handler import TokenHandler\n\n\nclass OAuthMiddleware(BaseHTTPMiddleware):\n\n    def __init__(self, app, exclude_paths: Optional[list[str]] = None):\n        \"\"\"\n        åˆå§‹åŒ–ä¸­é—´ä»¶\n\n        Args:\n            app: Starletteåº”ç”¨å®ä¾‹\n            exclude_paths: ä¸éœ€è¦è®¤è¯çš„è·¯å¾„åˆ—è¡¨\n        \"\"\"\n        super().__init__(app)\n        # é»˜è®¤æ’é™¤è·¯å¾„ï¼šç™»å½•ç›¸å…³é¡µé¢å’Œèµ„æº\n        default_exclude_paths = [\n            \"/login\",  # ç™»å½•é¡µé¢\n            \"/mcp/authorize\",  # ç™»å½•API\n        ]\n        self.exclude_paths = exclude_paths or default_exclude_paths\n        #self.login_url = os.getenv(\"MCP_LOGIN_URL\", \"http://localhost:3000/login\")\n\n    def _is_excluded_path(self, path: str) -> bool:\n        \"\"\"\n        æ£€æŸ¥è·¯å¾„æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­\n\n        Args:\n            path: è¯·æ±‚è·¯å¾„\n\n        Returns:\n            bool: æ˜¯å¦æ’é™¤è®¤è¯\n        \"\"\"\n        return any(\n            path == excluded or path.startswith(f\"{excluded}/\")\n            for excluded in self.exclude_paths\n        )\n    async def dispatch(self, request: Request, call_next):\n        \"\"\"\n        å¤„ç†è¯·æ±‚\n\n        Args:\n            request: è¯·æ±‚å¯¹è±¡\n            call_next: ä¸‹ä¸€ä¸ªå¤„ç†å‡½æ•°\n        \"\"\"\n        # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡è®¤è¯\n        if self._is_excluded_path(request.url.path):\n            return await call_next(request)\n\n        # è·å–è®¤è¯å¤´\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header:\n            # åªåœ¨éœ€è¦æ—¶å¼¹å‡ºç™»å½•æ¡†ï¼Œå¹¶ä¸”ä¸æ˜¯APIè¯·æ±‚æ—¶\n            return JSONResponse(\n                {\"error\": \"invalid_request\", \"error_description\": \"Missing authorization header\"},\n                status_code=401\n            )\n\n        # éªŒè¯tokenæ ¼å¼\n        parts = auth_header.split()\n        if len(parts) != 2 or parts[0].lower() != \"bearer\":\n            return JSONResponse(\n                {\"error\": \"invalid_request\", \"error_description\": \"Invalid authorization header format\"},\n                status_code=401\n            )\n\n        token = parts[1]\n\n        # éªŒè¯token\n        payload = TokenHandler.verify_token(token)\n        if not payload:\n            return JSONResponse(\n                {\"error\": \"invalid_token\", \"error_description\": \"Token is invalid or expired\"},\n                status_code=401\n            )\n\n        # æ£€æŸ¥tokenç±»å‹\n        if payload.get(\"type\") != \"access_token\":\n            return JSONResponse(\n                {\"error\": \"invalid_token\", \"error_description\": \"Invalid token type\"},\n                status_code=401\n            )\n\n        # å°†ç”¨æˆ·ä¿¡æ¯æ·»åŠ åˆ°è¯·æ±‚å¯¹è±¡\n        request.state.user = {\n            \"id\": payload[\"sub\"],\n            \"username\": payload[\"username\"]\n        }\n\n        return await call_next(request)",
    "line_count": 94
  },
  {
    "id": "MSJ-Factory_src_llamafactory_webui_manager.py",
    "repo": "IIIIQIIII/MSJ-Factory",
    "url": "https://github.com/IIIIQIIII/MSJ-Factory/blob/main/src/llamafactory/webui/manager.py",
    "code": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections.abc import Generator\nfrom typing import TYPE_CHECKING\n\n\nif TYPE_CHECKING:\n    from gradio.components import Component\n\n\nclass Manager:\n    r\"\"\"A class to manage all the gradio components in Web UI.\"\"\"\n\n    def __init__(self) -> None:\n        self._id_to_elem: dict[str, Component] = {}\n        self._elem_to_id: dict[Component, str] = {}\n\n    def add_elems(self, tab_name: str, elem_dict: dict[str, \"Component\"]) -> None:\n        r\"\"\"Add elements to manager.\"\"\"\n        for elem_name, elem in elem_dict.items():\n            elem_id = f\"{tab_name}.{elem_name}\"\n            self._id_to_elem[elem_id] = elem\n            self._elem_to_id[elem] = elem_id\n\n    def get_elem_list(self) -> list[\"Component\"]:\n        r\"\"\"Return the list of all elements.\"\"\"\n        return list(self._id_to_elem.values())\n\n    def get_elem_iter(self) -> Generator[tuple[str, \"Component\"], None, None]:\n        r\"\"\"Return an iterator over all elements with their names.\"\"\"\n        for elem_id, elem in self._id_to_elem.items():\n            yield elem_id.split(\".\")[-1], elem\n\n    def get_elem_by_id(self, elem_id: str) -> \"Component\":\n        r\"\"\"Get element by id.\n\n        Example: top.lang, train.dataset\n        \"\"\"\n        return self._id_to_elem[elem_id]\n\n    def get_id_by_elem(self, elem: \"Component\") -> str:\n        r\"\"\"Get id by element.\"\"\"\n        return self._elem_to_id[elem]\n\n    def get_base_elems(self) -> set[\"Component\"]:\n        r\"\"\"Get the base elements that are commonly used.\"\"\"\n        return {\n            self._id_to_elem[\"top.lang\"],\n            self._id_to_elem[\"top.model_name\"],\n            self._id_to_elem[\"top.model_path\"],\n            self._id_to_elem[\"top.finetuning_type\"],\n            self._id_to_elem[\"top.checkpoint_path\"],\n            self._id_to_elem[\"top.quantization_bit\"],\n            self._id_to_elem[\"top.quantization_method\"],\n            self._id_to_elem[\"top.template\"],\n            self._id_to_elem[\"top.rope_scaling\"],\n            self._id_to_elem[\"top.booster\"],\n        }\n",
    "line_count": 70
  },
  {
    "id": "powerrag_api_db_runtime_config.py",
    "repo": "oceanbase/powerrag",
    "url": "https://github.com/oceanbase/powerrag/blob/main/api/db/runtime_config.py",
    "code": "#\n#  Copyright 2024 The InfiniFlow Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom common.versions import get_ragflow_version\nfrom .reload_config_base import ReloadConfigBase\n\n\nclass RuntimeConfig(ReloadConfigBase):\n    DEBUG = None\n    WORK_MODE = None\n    HTTP_PORT = None\n    JOB_SERVER_HOST = None\n    JOB_SERVER_VIP = None\n    ENV = dict()\n    SERVICE_DB = None\n    LOAD_CONFIG_MANAGER = False\n\n    @classmethod\n    def init_config(cls, **kwargs):\n        for k, v in kwargs.items():\n            if hasattr(cls, k):\n                setattr(cls, k, v)\n\n    @classmethod\n    def init_env(cls):\n        cls.ENV.update({\"version\": get_ragflow_version()})\n\n    @classmethod\n    def load_config_manager(cls):\n        cls.LOAD_CONFIG_MANAGER = True\n\n    @classmethod\n    def get_env(cls, key):\n        return cls.ENV.get(key, None)\n\n    @classmethod\n    def get_all_env(cls):\n        return cls.ENV\n\n    @classmethod\n    def set_service_db(cls, service_db):\n        cls.SERVICE_DB = service_db\n",
    "line_count": 54
  },
  {
    "id": "nexus_src_nexus_parsers_base.py",
    "repo": "nexi-lab/nexus",
    "url": "https://github.com/nexi-lab/nexus/blob/main/src/nexus/parsers/base.py",
    "code": "\"\"\"Abstract base class for document parsers.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any\n\nfrom nexus.parsers.types import ParseResult\n\n\nclass Parser(ABC):\n    \"\"\"Abstract base class for all document parsers.\n\n    Parsers are responsible for extracting structured data from various\n    file formats. Each parser should implement format-specific parsing logic\n    while maintaining a consistent interface.\n\n    Example:\n        >>> class MyParser(Parser):\n        ...     def can_parse(self, file_path: str, mime_type: str) -> bool:\n        ...         return file_path.endswith('.txt')\n        ...\n        ...     async def parse(self, content: bytes, metadata: dict) -> ParseResult:\n        ...         text = content.decode('utf-8')\n        ...         return ParseResult(text=text, metadata=metadata)\n        ...\n        ...     @property\n        ...     def supported_formats(self) -> list[str]:\n        ...         return ['.txt']\n    \"\"\"\n\n    @abstractmethod\n    def can_parse(self, file_path: str, mime_type: str | None = None) -> bool:\n        \"\"\"Check if this parser can handle the given file.\n\n        Args:\n            file_path: Path to the file to be parsed\n            mime_type: Optional MIME type of the file\n\n        Returns:\n            True if this parser can handle the file, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def parse(self, content: bytes, metadata: dict[str, Any] | None = None) -> ParseResult:\n        \"\"\"Parse file content and return structured data.\n\n        Args:\n            content: Raw file content as bytes\n            metadata: Optional metadata about the file (path, MIME type, etc.)\n\n        Returns:\n            ParseResult containing extracted text, metadata, structure, etc.\n\n        Raises:\n            ParserError: If parsing fails\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def supported_formats(self) -> list[str]:\n        \"\"\"List of supported file extensions.\n\n        Returns:\n            List of file extensions (with dots) that this parser supports.\n            Example: ['.pdf', '.docx', '.txt']\n        \"\"\"\n        pass\n\n    @property\n    def name(self) -> str:\n        \"\"\"Get the parser name.\n\n        Returns:\n            The parser class name by default. Can be overridden.\n        \"\"\"\n        return self.__class__.__name__\n\n    @property\n    def priority(self) -> int:\n        \"\"\"Get the parser priority.\n\n        Higher priority parsers are tried first when multiple parsers\n        support the same format. Default is 0.\n\n        Returns:\n            Priority level (default: 0)\n        \"\"\"\n        return 0\n\n    def _get_file_extension(self, file_path: str) -> str:\n        \"\"\"Helper method to extract file extension.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            File extension including the dot (e.g., '.pdf')\n        \"\"\"\n        return Path(file_path).suffix.lower()\n",
    "line_count": 101
  },
  {
    "id": "Ragnar_epd_helper.py",
    "repo": "PierreGode/Ragnar",
    "url": "https://github.com/PierreGode/Ragnar/blob/main/epd_helper.py",
    "code": "# epd_helper.py\n\nimport importlib\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass EPDHelper:\n    def __init__(self, epd_type):\n        self.epd_type = epd_type\n        self.epd = self._load_epd_module()\n\n    def _load_epd_module(self):\n        try:\n            epd_module_name = f'resources.waveshare_epd.{self.epd_type}'\n            epd_module = importlib.import_module(epd_module_name)\n            return epd_module.EPD()\n        except ImportError as e:\n            logger.error(f\"EPD module {self.epd_type} not found: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Error loading EPD module {self.epd_type}: {e}\")\n            raise\n\n    def init_full_update(self):\n        try:\n            if hasattr(self.epd, 'FULL_UPDATE'):\n                self.epd.init(self.epd.FULL_UPDATE)\n            elif hasattr(self.epd, 'lut_full_update'):\n                self.epd.init(self.epd.lut_full_update)\n            else:\n                self.epd.init()\n            logger.info(\"EPD full update initialization complete.\")\n        except Exception as e:\n            logger.error(f\"Error initializing EPD for full update: {e}\")\n            raise\n\n    def init_partial_update(self):\n        try:\n            if hasattr(self.epd, 'PART_UPDATE'):\n                self.epd.init(self.epd.PART_UPDATE)\n            elif hasattr(self.epd, 'lut_partial_update'):\n                self.epd.init(self.epd.lut_partial_update)\n            else:\n                self.epd.init()\n            logger.info(\"EPD partial update initialization complete.\")\n        except Exception as e:\n            logger.error(f\"Error initializing EPD for partial update: {e}\")\n            raise\n\n    def display_partial(self, image):\n        try:\n            if hasattr(self.epd, 'displayPartial'):\n                self.epd.displayPartial(self.epd.getbuffer(image))\n            else:\n                self.epd.display(self.epd.getbuffer(image))\n            logger.info(\"Partial display update complete.\")\n        except Exception as e:\n            logger.error(f\"Error during partial display update: {e}\")\n            raise\n\n    def clear(self):\n        try:\n            self.epd.Clear()\n            logger.info(\"EPD cleared.\")\n        except Exception as e:\n            logger.error(f\"Error clearing EPD: {e}\")\n            raise\n\n    def display_full(self, image):\n        \"\"\"Display image on EPD using full update.\"\"\"\n        try:\n            self.epd.display(self.epd.getbuffer(image))\n            logger.info(\"Full display update complete.\")\n        except Exception as e:\n            logger.error(f\"Error during full display update: {e}\")\n            raise\n\n    def sleep(self):\n        \"\"\"Put EPD to sleep mode.\"\"\"\n        try:\n            self.epd.sleep()\n            logger.info(\"EPD sleep mode activated.\")\n        except Exception as e:\n            logger.error(f\"Error putting EPD to sleep: {e}\")\n            raise",
    "line_count": 86
  },
  {
    "id": "DLCompilerAttack_src_detector_MMBD.py",
    "repo": "SeekingDream/DLCompilerAttack",
    "url": "https://github.com/SeekingDream/DLCompilerAttack/blob/main/src/detector/MMBD.py",
    "code": "import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom scipy.stats import median_abs_deviation as MAD\nfrom scipy.stats import gamma\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom .abst_detector import AbstDetector\n\n\nclass MMBDDetector(AbstDetector):\n    def __init__(self, clean_loader, test_loader, device):\n        super().__init__(clean_loader, test_loader, device)\n\n        self.NI = 150\n        self.PI = 0.9\n        self.NSTEP = 300\n        self.TC = 6\n        self.batch_size = 20\n        self.criterion = nn.CrossEntropyLoss()\n\n    @staticmethod\n    def lr_scheduler(iter_idx):\n        lr = 1e-2\n        return lr\n\n    def compute_score(self, model, bd_trigger):\n        model = model.to(self.device).eval()\n        NC = self.get_class_num(model)\n        res = []\n        rnd_img_size = [30] + model.input_sizes[0]\n        for t in range(NC):\n\n            images = torch.rand(rnd_img_size).to(self.device)\n            images.requires_grad = True\n\n            last_loss = 1000\n            labels = t * torch.ones((len(images),), dtype=torch.long).to(self.device)\n            onehot_label = F.one_hot(labels, num_classes=NC)\n            for iter_idx in tqdm(range(self.NSTEP)):\n\n                optimizer = torch.optim.SGD([images], lr=self.lr_scheduler(iter_idx), momentum=0.2)\n                optimizer.zero_grad()\n                outputs = model(torch.clamp(images, min=0, max=1))\n                if not isinstance(outputs, torch.Tensor):\n                    outputs = outputs[0]\n\n                loss = -1 * torch.sum((outputs * onehot_label)) \\\n                       + torch.sum(torch.max((1 - onehot_label) * outputs - 1000 * onehot_label, dim=1)[0])\n                loss.backward(retain_graph=True)\n                optimizer.step()\n                if abs(last_loss - loss.item()) / abs(last_loss) < 1e-5:\n                    break\n                last_loss = loss.item()\n\n            res.append(torch.max(torch.sum((outputs * onehot_label), dim=1) \\\n                                 - torch.max((1 - onehot_label) * outputs - 1000 * onehot_label, dim=1)[0]).item())\n            print(t, res[-1])\n\n        stats = res\n\n        mad = MAD(stats, scale='normal')\n        abs_deviation = np.abs(stats - np.median(stats))\n        score = abs_deviation / mad\n        print(score)\n\n        np.save('results.npy', np.array(res))\n        ind_max = np.argmax(stats)\n        r_eval = np.amax(stats)\n        r_null = np.delete(stats, ind_max)\n\n        shape, loc, scale = gamma.fit(r_null)\n        pv = 1 - pow(gamma.cdf(r_eval, a=shape, loc=loc, scale=scale), len(r_null) + 1)\n        print(pv)\n\n        if pv > 0.05:\n            print('No Attack!')\n        else:\n            print('There is attack with target class {}'.format(np.argmax(stats)))\n        return pv",
    "line_count": 81
  },
  {
    "id": "openAgent_backend_open_agent_db_base.py",
    "repo": "lkpAgent/openAgent",
    "url": "https://github.com/lkpAgent/openAgent/blob/main/backend/open_agent/db/base.py",
    "code": "\"\"\"Database base model.\"\"\"\n\nfrom datetime import datetime\nfrom sqlalchemy import Column, Integer, DateTime, ForeignKey\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.sql import func\nfrom typing import Optional\n\n\nBase = declarative_base()\n\n\nclass BaseModel(Base):\n    \"\"\"Base model with common fields.\"\"\"\n    \n    __abstract__ = True\n    \n    id = Column(Integer, primary_key=True, index=True)\n    created_at = Column(DateTime, default=func.now(), nullable=False)\n    updated_at = Column(DateTime, default=func.now(), onupdate=func.now(), nullable=False)\n    created_by = Column(Integer, nullable=True)\n    updated_by = Column(Integer, nullable=True)\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize model with automatic audit fields setting.\"\"\"\n        super().__init__(**kwargs)\n        # Set audit fields for new instances\n        self.set_audit_fields()\n    def set_audit_fields(self, user_id: Optional[int] = None, is_update: bool = False):\n        \"\"\"Set audit fields for create/update operations.\n\n        Args:\n            user_id: ID of the user performing the operation (optional, will use context if not provided)\n            is_update: True for update operations, False for create operations\n        \"\"\"\n        # Get user_id from context if not provided\n        if user_id is None:\n            from ..core.context import UserContext\n            try:\n                user_id = UserContext.get_current_user_id()\n            except Exception:\n                # If no user in context, skip setting audit fields\n                return\n\n        # Skip if still no user_id\n        if user_id is None:\n            return\n\n        if not is_update:\n            # For create operations, set both create_by and update_by\n            self.created_by = user_id\n            self.updated_by = user_id\n        else:\n            # For update operations, only set update_by\n            self.updated_by = user_id\n    \n    def to_dict(self):\n        \"\"\"Convert model to dictionary.\"\"\"\n        return {\n            column.name: getattr(self, column.name)\n            for column in self.__table__.columns\n        }",
    "line_count": 62
  },
  {
    "id": "grid1.3_core_di_container.py",
    "repo": "cryptocj520/grid1.3",
    "url": "https://github.com/cryptocj520/grid1.3/blob/main/core/di/container.py",
    "code": "\"\"\"\nä¾èµ–æ³¨å…¥å®¹å™¨\n\nåŸºäº Python-injector çš„ä¾èµ–æ³¨å…¥å®¹å™¨å®ç°\n\"\"\"\n\nfrom injector import Injector, Module, singleton, provider\nfrom typing import Dict, Any, Type, Optional, List\n\n# ä½¿ç”¨ç®€åŒ–çš„ç»Ÿä¸€æ—¥å¿—å…¥å£\nfrom ..logging import get_system_logger\n\n\nclass DIContainer:\n    \"\"\"ä¾èµ–æ³¨å…¥å®¹å™¨\"\"\"\n    \n    def __init__(self, modules: List[Module] = None):\n        self.modules = modules or []\n        self.injector = Injector(self.modules)\n        self.logger = get_system_logger()\n        self.initialized = False\n    \n    def register_module(self, module: Module):\n        \"\"\"æ³¨å†Œæ¨¡å—\"\"\"\n        self.modules.append(module)\n        self.injector = Injector(self.modules)\n        self.logger.info(f\"æ³¨å†Œæ¨¡å—: {module.__class__.__name__}\")\n    \n    def register_modules(self, modules: List[Module]):\n        \"\"\"æ³¨å†Œå¤šä¸ªæ¨¡å—\"\"\"\n        for module in modules:\n            self.modules.append(module)\n        self.injector = Injector(self.modules)\n        self.logger.info(f\"æ³¨å†Œäº† {len(modules)} ä¸ªæ¨¡å—\")\n    \n    def get(self, interface: Type):\n        \"\"\"è·å–å®ä¾‹\"\"\"\n        return self.injector.get(interface)\n    \n    def create_child_injector(self, modules: list = None):\n        \"\"\"åˆ›å»ºå­æ³¨å…¥å™¨\"\"\"\n        child_modules = self.modules + (modules or [])\n        return Injector(child_modules)\n    \n    def initialize(self):\n        \"\"\"åˆå§‹åŒ–å®¹å™¨\"\"\"\n        if not self.initialized:\n            # è‡ªåŠ¨æ³¨å†Œé»˜è®¤æ¨¡å—\n            from .modules import ALL_MODULES\n            self.register_modules([module() for module in ALL_MODULES])\n            self.initialized = True\n            self.logger.info(\"ä¾èµ–æ³¨å…¥å®¹å™¨å·²åˆå§‹åŒ–\")\n\n\n# å…¨å±€å®¹å™¨å®ä¾‹\ncontainer = DIContainer()\n\n\ndef get_container() -> DIContainer:\n    \"\"\"è·å–å…¨å±€å®¹å™¨\"\"\"\n    if not container.initialized:\n        container.initialize()\n    return container\n",
    "line_count": 63
  },
  {
    "id": "Feagent_src_application_services_idempotency_coordinator.py",
    "repo": "DSGWJQ/Feagent",
    "url": "https://github.com/DSGWJQ/Feagent/blob/main/src/application/services/idempotency_coordinator.py",
    "code": "\"\"\"IdempotencyCoordinator - Application-level idempotency + concurrency control.\n\nImplements per-idempotency-key in-flight de-duplication using stdlib asyncio primitives\nand persists successful results via the IdempotencyStore Domain Port.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom collections.abc import Awaitable, Callable\nfrom typing import Any\n\nfrom src.domain.ports.idempotency_store import IdempotencyStore\n\n\nclass IdempotencyNotReadyError(RuntimeError):\n    \"\"\"Raised when idempotency is requested but cannot be served.\"\"\"\n\n\nclass IdempotencyCoordinator:\n    def __init__(self, *, store: IdempotencyStore) -> None:\n        self._store = store\n        self._guard = asyncio.Lock()\n        self._in_flight: dict[str, asyncio.Task[Any]] = {}\n\n    async def run(\n        self,\n        *,\n        idempotency_key: str,\n        work: Callable[[], Awaitable[Any]],\n    ) -> Any:\n        if await self._store.exists(idempotency_key):\n            return await self._store.get_result(idempotency_key)\n\n        async with self._guard:\n            if await self._store.exists(idempotency_key):\n                return await self._store.get_result(idempotency_key)\n\n            task = self._in_flight.get(idempotency_key)\n            if task is None:\n                task = asyncio.create_task(self._run_and_persist(idempotency_key, work))\n                self._in_flight[idempotency_key] = task\n\n        return await asyncio.shield(task)\n\n    async def _run_and_persist(\n        self,\n        idempotency_key: str,\n        work: Callable[[], Awaitable[Any]],\n    ) -> Any:\n        try:\n            result = await work()\n            await self._store.save_result(idempotency_key, result)\n            return result\n        finally:\n            async with self._guard:\n                self._in_flight.pop(idempotency_key, None)\n",
    "line_count": 57
  },
  {
    "id": "Screen-Translate_src_screen_translate_core_ocr_processor.py",
    "repo": "mucsbr/Screen-Translate",
    "url": "https://github.com/mucsbr/Screen-Translate/blob/main/src/screen_translate/core/ocr_processor.py",
    "code": "\"\"\"OCR processing using EasyOCR.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport numpy as np\n\ntry:\n    import easyocr\nexcept ImportError:  # pragma: no cover\n    easyocr = None\n\n\n@dataclass\nclass OCRResult:\n    \"\"\"Container for OCR text and confidence.\"\"\"\n\n    text: str\n    confidence: float\n\n\nclass OCRProcessor:\n    \"\"\"Wrap EasyOCR reader for subtitle extraction.\"\"\"\n\n    def __init__(self, languages: Optional[List[str]] = None) -> None:\n        self._languages = languages or [\"ja\", \"en\"]\n        self._reader: Optional[easyocr.Reader] = None\n\n        project_root = Path(__file__).parent.parent.parent\n        self._model_dir = project_root / \".easyocr_models\"\n        self._model_dir.mkdir(exist_ok=True)\n\n    def set_languages(self, languages: List[str]) -> None:\n        \"\"\"Update OCR languages. Requires restart() to take effect.\"\"\"\n        if self._reader is not None:\n            raise RuntimeError(\"Cannot change languages while OCR is running. Call stop() first.\")\n        self._languages = languages\n\n    def start(self) -> None:\n        if easyocr is None:\n            raise RuntimeError(\"EasyOCR æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…ä¾èµ– easyocrã€‚\")\n        if self._reader is None:\n            self._reader = easyocr.Reader(\n                self._languages,\n                model_storage_directory=str(self._model_dir),\n                download_enabled=True,\n                detector=True,\n                recognizer=True\n            )\n\n    def stop(self) -> None:\n        self._reader = None\n\n    def read_text(self, image: np.ndarray) -> List[OCRResult]:\n        if self._reader is None:\n            self.start()\n        if self._reader is None:\n            return []\n        results = self._reader.readtext(image)\n        return [OCRResult(text=text, confidence=conf) for _, text, conf in results]\n",
    "line_count": 64
  },
  {
    "id": "eastmoney_src_analysis_post_market.py",
    "repo": "Austin-Patrician/eastmoney",
    "url": "https://github.com/Austin-Patrician/eastmoney/blob/main/src/analysis/post_market.py",
    "code": "\"\"\"\nPost-Market Analyst - Strategy Driven\n=====================================\n\"\"\"\n\nimport sys\nimport os\nfrom datetime import datetime\n\n# Add project root to sys.path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.analysis.base_analyst import BaseAnalyst\nfrom src.analysis.strategies.factory import StrategyFactory\n\nclass PostMarketAnalyst(BaseAnalyst):\n    \"\"\"\n    Delegates analysis to specific strategies based on fund type.\n    \"\"\"\n    \n    SYSTEM_TITLE = \"ç›˜åå¤ç›˜ç³»ç»Ÿå¯åŠ¨\"\n    FAILURE_SUFFIX = \"å¤ç›˜å¤±è´¥\"\n\n    def __init__(self):\n        super().__init__()\n\n    def analyze_fund(self, fund: dict) -> str:\n        \"\"\"\n        Delegates the analysis to the appropriate strategy.\n        \"\"\"\n        fund_name = fund.get(\"name\")\n        print(f\"\\n{'='*60}\")\n        print(f\"ğŸ“Š å¤ç›˜åŸºé‡‘: {fund_name} ({fund.get('code')})\")\n        print(f\"{'='*60}\")\n\n        try:\n            # 1. Get Strategy\n            strategy = StrategyFactory.get_strategy(fund, self.llm, self.web_search)\n\n            # 2. Collect Data\n            data = strategy.collect_data(mode='post')\n\n            # 3. Generate Report\n            report = strategy.generate_report(mode='post', data=data)\n\n            print(\"  âœ… å¤ç›˜å®Œæˆ\")\n            return report\n\n        except Exception as e:\n            print(f\"  âŒ Analysis Failed: {e}\")\n            import traceback\n            traceback.print_exc()\n            return f\"Analysis Failed: {str(e)}\"\n\n    def analyze_item(self, item: dict) -> str:\n        \"\"\"Generic method for analyzing any item (fund or stock).\"\"\"\n        return self.analyze_fund(item)\n\nif __name__ == \"__main__\":\n    analyst = PostMarketAnalyst()\n    print(analyst.run_all())",
    "line_count": 61
  },
  {
    "id": "GSFix3D_src_trainer_marigold_gsfixer_finetune.py",
    "repo": "GSFix3D/GSFix3D",
    "url": "https://github.com/GSFix3D/GSFix3D/blob/main/src/trainer/marigold_gsfixer_finetune.py",
    "code": "# SPDX-FileCopyrightText: 2025 Mobile Robotics Lab, Technical University of Munich\n# SPDX-FileCopyrightText: 2025 Jiaxin Wei\n# SPDX-License-Identifier: Apache-2.0\n\n\nfrom typing import List\nfrom omegaconf import OmegaConf\nfrom torch.utils.data import DataLoader\n\nfrom marigold.marigold_gsfixer_pipeline import MarigoldGSFixerPipeline\nfrom src.trainer.base_trainer import BaseTrainer\n\n\nclass MarigoldGSFixerFinetune(BaseTrainer):\n    def __init__(\n        self,\n        cfg: OmegaConf,\n        model: MarigoldGSFixerPipeline,\n        train_dataloader: DataLoader,\n        device,\n        out_dir_ckpt,\n        out_dir_eval,\n        out_dir_vis,\n        accumulation_steps: int,\n        val_dataloaders: List[DataLoader] = None,\n        vis_dataloaders: List[DataLoader] = None,\n        dual_input: bool = False,\n    ):\n        super().__init__(cfg,\n                         model,\n                         train_dataloader,\n                         device,\n                         out_dir_ckpt,\n                         out_dir_eval,\n                         out_dir_vis,\n                         accumulation_steps,\n                         val_dataloaders,\n                         vis_dataloaders,\n                         dual_input,\n                        )\n\n    def _load_train_data(self, batch):\n        \"\"\"Load training data from batch for finetuning.\"\"\"\n        rgb = batch[\"gt_rgb_norm\"].to(self.device)\n        corrupted_rgb1 = batch[\"rendered_gs_norm\"].to(self.device)\n        if self.dual_input:\n            corrupted_rgb2 = batch[\"rendered_mesh_norm\"].to(self.device)   \n        else:\n            corrupted_rgb2 = None\n\n        return rgb, corrupted_rgb1, corrupted_rgb2\n    \n    def _load_val_data(self, batch):\n        \"\"\"Load validation/visualization data from batch.\"\"\"\n        rgb = batch[\"gt_rgb\"] / 255.0\n        corrupted_rgb1 = batch[\"rendered_gs\"]\n        if self.dual_input:\n            corrupted_rgb2 = batch[\"rendered_mesh\"]\n        else:\n            corrupted_rgb2 = None\n\n        return rgb, corrupted_rgb1, corrupted_rgb2\n",
    "line_count": 62
  },
  {
    "id": "repo-swarm_src_investigator_activity_wrapper.py",
    "repo": "royosherove/repo-swarm",
    "url": "https://github.com/royosherove/repo-swarm/blob/main/src/investigator/activity_wrapper.py",
    "code": "\"\"\"\nActivityWrapper for executing Temporal activities without direct Temporal dependency.\n\"\"\"\n\nimport asyncio\nfrom typing import Optional, Any, Callable\nfrom datetime import timedelta\n\n\nclass ActivityWrapper:\n    \"\"\"\n    Wrapper class to execute Temporal activities without requiring direct Temporal imports.\n    This allows the investigator module to remain decoupled from Temporal while still\n    being able to execute activities when running within a Temporal workflow context.\n    \"\"\"\n    \n    def __init__(self, workflow_context: Optional[Any] = None):\n        \"\"\"\n        Initialize the ActivityWrapper.\n        \n        Args:\n            workflow_context: The Temporal workflow context (workflow module) if available\n        \"\"\"\n        self.workflow_context = workflow_context\n        self._is_temporal_context = workflow_context is not None\n    \n    async def execute_activity(self, activity_func: Callable, *args, \n                              start_to_close_timeout: Optional[timedelta] = None,\n                              retry_policy: Optional[Any] = None,\n                              **kwargs) -> Any:\n        \"\"\"\n        Execute an activity function.\n        \n        If running in a Temporal workflow context, this will execute the activity\n        via Temporal's workflow.execute_activity. Otherwise, it will execute the\n        function directly (for testing or non-Temporal environments).\n        \n        Args:\n            activity_func: The activity function to execute\n            *args: Positional arguments for the activity function\n            start_to_close_timeout: Timeout for the activity execution\n            retry_policy: Retry policy for the activity\n            **kwargs: Keyword arguments for the activity function\n            \n        Returns:\n            Result from the activity execution\n        \"\"\"\n        if self._is_temporal_context and hasattr(self.workflow_context, 'execute_activity'):\n            # Running in Temporal workflow context\n            return await self.workflow_context.execute_activity(\n                activity_func,\n                *args,\n                start_to_close_timeout=start_to_close_timeout or timedelta(minutes=10),\n                retry_policy=retry_policy,\n                **kwargs\n            )\n        else:\n            # Running outside Temporal context (testing or direct execution)\n            # Execute the activity function directly\n            if asyncio.iscoroutinefunction(activity_func):\n                return await activity_func(*args, **kwargs)\n            else:\n                return activity_func(*args, **kwargs)\n    \n    def is_temporal_context(self) -> bool:\n        \"\"\"\n        Check if running in a Temporal workflow context.\n        \n        Returns:\n            True if running in Temporal workflow context, False otherwise\n        \"\"\"\n        return self._is_temporal_context\n",
    "line_count": 72
  },
  {
    "id": "ComfyUI-Qwen-Image-Integrated-KSampler_cache.py",
    "repo": "luguoli/ComfyUI-Qwen-Image-Integrated-KSampler",
    "url": "https://github.com/luguoli/ComfyUI-Qwen-Image-Integrated-KSampler/blob/main/cache.py",
    "code": "import itertools\nfrom typing import Optional\n\nclass TaggedCache:\n    def __init__(self, tag_settings: Optional[dict]=None):\n        self._tag_settings = tag_settings or {}  # tag cache size\n        self._data = {}\n\n    def __getitem__(self, key):\n        for tag_data in self._data.values():\n            if key in tag_data:\n                return tag_data[key]\n        raise KeyError(f'Key `{key}` does not exist')\n\n    def __setitem__(self, key, value: tuple):\n        # value: (tag: str, (islist: bool, data: *))\n\n        # if key already exists, pop old value\n        for tag_data in self._data.values():\n            if key in tag_data:\n                tag_data.pop(key, None)\n                break\n\n        tag = value[0]\n        if tag not in self._data:\n\n            try:\n                from cachetools import LRUCache\n\n                default_size = 20\n                if 'ckpt' in tag:\n                    default_size = 5\n                elif tag in ['latent', 'image']:\n                    default_size = 100\n\n                self._data[tag] = LRUCache(maxsize=self._tag_settings.get(tag, default_size))\n\n            except (ImportError, ModuleNotFoundError):\n                # TODO: implement a simple lru dict\n                self._data[tag] = {}\n        self._data[tag][key] = value\n\n    def __delitem__(self, key):\n        for tag_data in self._data.values():\n            if key in tag_data:\n                del tag_data[key]\n                return\n        raise KeyError(f'Key `{key}` does not exist')\n\n    def __contains__(self, key):\n        return any(key in tag_data for tag_data in self._data.values())\n\n    def items(self):\n        yield from itertools.chain(*map(lambda x :x.items(), self._data.values()))\n\n    def get(self, key, default=None):\n        \"\"\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\"\"\n        for tag_data in self._data.values():\n            if key in tag_data:\n                return tag_data[key]\n        return default\n\n    def clear(self):\n        # clear all cache\n        self._data = {}\n\ncache_settings = {}\ncache = TaggedCache(cache_settings)\ncache_count = {}\n\ndef update_cache(k, tag, v):\n    cache[k] = (tag, v)\n    cnt = cache_count.get(k)\n    if cnt is None:\n        cnt = 0\n        cache_count[k] = cnt\n    else:\n        cache_count[k] += 1\ndef remove_cache(key):\n    global cache\n    if key == '*':\n        cache = TaggedCache(cache_settings)\n    elif key in cache:\n        del cache[key]\n    else:\n        print(f\"invalid {key}\")",
    "line_count": 86
  },
  {
    "id": "Borgitory_src_borgitory_config_command_runner_config.py",
    "repo": "mlapaglia/Borgitory",
    "url": "https://github.com/mlapaglia/Borgitory/blob/develop/src/borgitory/config/command_runner_config.py",
    "code": "\"\"\"\nConfiguration for command runner services.\n\nThis module provides configuration classes for command execution services,\nsupporting environment-based configuration and dependency injection.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\n\n\n@dataclass(frozen=True)\nclass CommandRunnerConfig:\n    \"\"\"Configuration for command runner services.\"\"\"\n\n    timeout: int = 300\n    max_retries: int = 3\n    log_commands: bool = True\n    buffer_size: int = 8192\n\n    @classmethod\n    def from_env(cls, prefix: str = \"COMMAND\") -> \"CommandRunnerConfig\":\n        \"\"\"\n        Create configuration from environment variables.\n\n        Args:\n            prefix: Environment variable prefix (default: COMMAND)\n\n        Returns:\n            CommandRunnerConfig instance with values from environment\n\n        Environment Variables:\n            COMMAND_TIMEOUT: Command timeout in seconds (default: 300)\n            COMMAND_MAX_RETRIES: Maximum retry attempts (default: 3)\n            COMMAND_LOG_COMMANDS: Whether to log commands (default: true)\n            COMMAND_BUFFER_SIZE: Buffer size for command output (default: 8192)\n        \"\"\"\n        return cls(\n            timeout=int(os.getenv(f\"{prefix}_TIMEOUT\", \"300\")),\n            max_retries=int(os.getenv(f\"{prefix}_MAX_RETRIES\", \"3\")),\n            log_commands=os.getenv(f\"{prefix}_LOG_COMMANDS\", \"true\").lower() == \"true\",\n            buffer_size=int(os.getenv(f\"{prefix}_BUFFER_SIZE\", \"8192\")),\n        )\n\n    def with_timeout(self, timeout: int) -> \"CommandRunnerConfig\":\n        \"\"\"Create a new config with different timeout.\"\"\"\n        return CommandRunnerConfig(\n            timeout=timeout,\n            max_retries=self.max_retries,\n            log_commands=self.log_commands,\n            buffer_size=self.buffer_size,\n        )\n\n    def with_retries(self, max_retries: int) -> \"CommandRunnerConfig\":\n        \"\"\"Create a new config with different retry count.\"\"\"\n        return CommandRunnerConfig(\n            timeout=self.timeout,\n            max_retries=max_retries,\n            log_commands=self.log_commands,\n            buffer_size=self.buffer_size,\n        )\n",
    "line_count": 61
  },
  {
    "id": "MinivLLM_src_myvllm_layers_layernorm.py",
    "repo": "Wenyueh/MinivLLM",
    "url": "https://github.com/Wenyueh/MinivLLM/blob/main/src/myvllm/layers/layernorm.py",
    "code": "import torch\nimport time \n\nclass LayerNorm(torch.nn.Module):\n    def __init__(self, gamma: torch.Tensor, eps: float = 1e-5):\n        super().__init__()\n        self.register_buffer('gamma', gamma)\n        self.eps = eps\n\n    @torch.compile\n    def rms_forward(self, x: torch.Tensor) -> torch.Tensor:\n        # RMSNorm(x) = (x / sqrt(mean(xÂ²) + Îµ)) âŠ™ Î³\n\n        variance = x.pow(2).mean(dim=-1, keepdim=True) + self.eps\n        sqrt_variance = variance.sqrt()\n        x_norm = (x / sqrt_variance * self.gamma)\n\n        return x_norm\n\n    def residual_rms_forward(self, x: torch.Tensor, residual: torch.Tensor) -> torch.Tensor:\n        x = x + residual\n        return self.rms_forward(x), x\n\n    def forward(self, x: torch.Tensor, residual: torch.Tensor | None = None) -> torch.Tensor:\n        if residual is not None:\n            return self.residual_rms_forward(x, residual)\n        else:\n            return self.rms_forward(x)\n\nif __name__ == \"__main__\":\n    # Example usage\n    x = torch.randn(8,4000,8000).cuda()\n    gamma = torch.full((8000,), 0.5, device=\"cuda\", dtype=x.dtype)\n    layer = LayerNorm(gamma=gamma).cuda()\n    residual = torch.full_like(x,fill_value=1)\n\n    for _ in range(10): # Warm-up iterations\n        _ = layer(x)\n    \n    # Without residuals\n    times = [] \n    for _ in range(100): # Timing iterations\n        torch.cuda.synchronize()\n        start_time = time.time()\n        _ = layer(x)\n        torch.cuda.synchronize()\n        end_time = time.time()\n        times.append(end_time - start_time)\n    avg_time = sum(times) / len(times)\n    print(f\"[Without residuals] Average inference time over 100 runs: {avg_time * 1000:.4f} ms\")\n\n    # With residuals\n    times.clear()\n    for _ in range(100): # Timing iterations\n        torch.cuda.synchronize()\n        start_time = time.time()\n        _ = layer(x,residual)\n        torch.cuda.synchronize()\n        end_time = time.time()\n        times.append(end_time - start_time)\n    avg_time = sum(times) / len(times)\n    print(f\"[With residuals] Average inference time over 100 runs: {avg_time * 1000:.4f} ms\")\n    \n",
    "line_count": 63
  },
  {
    "id": "svc-infra_src_svc_infra_jobs_runner.py",
    "repo": "nfraxlab/svc-infra",
    "url": "https://github.com/nfraxlab/svc-infra/blob/main/src/svc_infra/jobs/runner.py",
    "code": "from __future__ import annotations\n\nimport asyncio\nimport contextlib\nfrom collections.abc import Awaitable, Callable\n\nfrom .queue import JobQueue\n\nProcessFunc = Callable[[object], Awaitable[None]]\n\n\nclass WorkerRunner:\n    \"\"\"Cooperative worker loop with graceful stop.\n\n    - start(): begin polling the queue and processing jobs\n    - stop(grace_seconds): signal stop, wait up to grace for current job to finish\n    \"\"\"\n\n    def __init__(self, queue: JobQueue, handler: ProcessFunc, *, poll_interval: float = 0.25):\n        self._queue = queue\n        self._handler = handler\n        self._poll_interval = poll_interval\n        self._task: asyncio.Task | None = None\n        self._stopping = asyncio.Event()\n        self._inflight: asyncio.Task | None = None\n\n    async def _loop(self) -> None:\n        try:\n            while not self._stopping.is_set():\n                job = self._queue.reserve_next()\n                if not job:\n                    await asyncio.sleep(self._poll_interval)\n                    continue\n\n                # Process one job; track in-flight task for stop()\n                async def _run():\n                    try:\n                        await self._handler(job)\n                    except Exception as exc:  # pragma: no cover\n                        self._queue.fail(job.id, error=str(exc))\n                        return\n                    self._queue.ack(job.id)\n\n                self._inflight = asyncio.create_task(_run())\n                try:\n                    await self._inflight\n                finally:\n                    self._inflight = None\n        finally:\n            # exiting loop\n            pass\n\n    def start(self) -> asyncio.Task:\n        if self._task is None or self._task.done():\n            self._task = asyncio.create_task(self._loop())\n        return self._task\n\n    async def stop(self, *, grace_seconds: float = 10.0) -> None:\n        self._stopping.set()\n        # Wait for in-flight job to complete, up to grace\n        if self._inflight is not None and not self._inflight.done():\n            try:\n                await asyncio.wait_for(self._inflight, timeout=grace_seconds)\n            except TimeoutError:\n                # Give up; job will be retried if your queue supports visibility timeouts\n                pass\n        # Finally, wait for loop to exit (should be quick since stopping is set)\n        if self._task is not None:\n            try:\n                await asyncio.wait_for(self._task, timeout=max(0.1, self._poll_interval + 0.1))\n            except TimeoutError:\n                # Cancel as a last resort\n                self._task.cancel()\n                with contextlib.suppress(Exception):\n                    await self._task\n",
    "line_count": 75
  },
  {
    "id": "ai_story_backend_core_ai_client_mock_image2video_client.py",
    "repo": "xhongc/ai_story",
    "url": "https://github.com/xhongc/ai_story/blob/main/backend/core/ai_client/mock_image2video_client.py",
    "code": "\"\"\"\nMock å›¾ç”Ÿè§†é¢‘å®¢æˆ·ç«¯å®ç°\nç”¨äºæµ‹è¯•å’Œå¼€å‘ç¯å¢ƒï¼Œè¿”å›æ¨¡æ‹Ÿçš„è§†é¢‘URL\n\"\"\"\n\nimport time\nfrom typing import Dict, Any\nfrom .base import Image2VideoClient, AIResponse\n\n\nclass MockImage2VideoClient(Image2VideoClient):\n    \"\"\"\n    Mock å›¾ç”Ÿè§†é¢‘å®¢æˆ·ç«¯\n    è¿”å›é¢„å®šä¹‰çš„æ¨¡æ‹Ÿè§†é¢‘URLï¼Œç”¨äºæµ‹è¯•å·¥ä½œæµ\n    \"\"\"\n\n    # æ¨¡æ‹Ÿè§†é¢‘URLåˆ—è¡¨ï¼ˆä½¿ç”¨ç¤ºä¾‹è§†é¢‘ï¼‰\n    MOCK_VIDEO_URLS = [\n        \"https://sample-videos.com/video123/mp4/720/big_buck_bunny_720p_1mb.mp4\",\n        \"https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4\",\n        \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/360/Big_Buck_Bunny_360_10s_1MB.mp4\",\n    ]\n\n    async def _generate_video(\n        self,\n        image_url: str,\n        camera_movement: Dict[str, Any],\n        duration: float,\n        fps: int,\n        **kwargs\n    ) -> AIResponse:\n        \"\"\"\n        ç”Ÿæˆæ¨¡æ‹Ÿçš„è§†é¢‘å“åº”\n\n        Args:\n            image_url: æºå›¾ç‰‡URL\n            camera_movement: è¿é•œå‚æ•°\n            duration: è§†é¢‘æ—¶é•¿\n            fps: å¸§ç‡\n            **kwargs: å…¶ä»–å‚æ•°\n\n        Returns:\n            AIResponse: åŒ…å«æ¨¡æ‹Ÿè§†é¢‘URLçš„å“åº”å¯¹è±¡\n        \"\"\"\n        start_time = time.time()\n\n        # æ¨¡æ‹ŸAPIå»¶è¿Ÿï¼ˆè§†é¢‘ç”Ÿæˆé€šå¸¸å¾ˆæ…¢ï¼‰\n        time.sleep(2.0)\n\n        # ä»kwargsè·å–å‚æ•°\n        width = kwargs.get('width', 1280)\n        height = kwargs.get('height', 720)\n        model = kwargs.get('model', self.model_name)\n\n        # æ ¹æ®å›¾ç‰‡URLå“ˆå¸Œé€‰æ‹©è§†é¢‘ï¼ˆä¿è¯ç›¸åŒå›¾ç‰‡è¿”å›ç›¸åŒè§†é¢‘ï¼‰\n        image_hash = hash(image_url) % len(self.MOCK_VIDEO_URLS)\n        video_url = self.MOCK_VIDEO_URLS[image_hash]\n\n        # æ„å»ºè§†é¢‘æ•°æ®\n        video_data = {\n            \"url\": video_url,\n            \"width\": width,\n            \"height\": height,\n            \"duration\": duration,\n            \"fps\": fps,\n            \"format\": \"mp4\",\n            \"file_size\": 1024 * 1024,  # æ¨¡æ‹Ÿ1MBæ–‡ä»¶å¤§å°\n            \"camera_movement\": camera_movement\n        }\n\n        latency_ms = int((time.time() - start_time) * 1000)\n\n        return AIResponse(\n            success=True,\n            data={\n                'url': video_url,\n                'video': video_data,\n                'videos': [video_data]  # å…¼å®¹å¤šè§†é¢‘æ ¼å¼\n            },\n            metadata={\n                'latency_ms': latency_ms,\n                'model': model,\n                'is_mock': True,\n                'source_image': image_url[:100]  # è®°å½•éƒ¨åˆ†æºå›¾ç‰‡URL\n            }\n        )\n\n    async def validate_config(self) -> bool:\n        \"\"\"\n        éªŒè¯é…ç½®ï¼ˆMockå®¢æˆ·ç«¯å§‹ç»ˆè¿”å›Trueï¼‰\n\n        Returns:\n            bool: å§‹ç»ˆè¿”å›True\n        \"\"\"\n        return True\n\n    async def health_check(self) -> bool:\n        \"\"\"\n        å¥åº·æ£€æŸ¥ï¼ˆMockå®¢æˆ·ç«¯å§‹ç»ˆè¿”å›Trueï¼‰\n\n        Returns:\n            bool: å§‹ç»ˆè¿”å›True\n        \"\"\"\n        return True\n",
    "line_count": 104
  },
  {
    "id": "docify_backend_app_core_cache.py",
    "repo": "keshavashiya/docify",
    "url": "https://github.com/keshavashiya/docify/blob/main/backend/app/core/cache.py",
    "code": "\"\"\"\nRedis cache client for real-time updates and message streaming\n\"\"\"\nimport redis\nimport logging\nfrom app.core.config import settings\n\nlogger = logging.getLogger(__name__)\n\n_redis_client = None\n\n\ndef get_redis_client() -> redis.Redis:\n    \"\"\"Get or create Redis client instance\"\"\"\n    global _redis_client\n    \n    if _redis_client is None:\n        try:\n            _redis_client = redis.from_url(\n                settings.REDIS_URL,\n                decode_responses=True,\n                socket_connect_timeout=5,\n                socket_keepalive=True,\n                health_check_interval=30\n            )\n            # Test connection\n            _redis_client.ping()\n            logger.info(\"Redis client initialized successfully\")\n        except Exception as e:\n            logger.error(f\"Failed to connect to Redis: {e}\")\n            raise\n    \n    return _redis_client\n\n\ndef close_redis_client():\n    \"\"\"Close Redis connection\"\"\"\n    global _redis_client\n    if _redis_client is not None:\n        try:\n            _redis_client.close()\n            _redis_client = None\n        except Exception as e:\n            logger.error(f\"Error closing Redis connection: {e}\")\n\n\nclass MessageStreamCache:\n    \"\"\"Cache manager for message streaming\"\"\"\n    \n    def __init__(self):\n        self.redis = get_redis_client()\n        self.ttl = 3600  # 1 hour\n    \n    def set_status(self, message_id: str, status: str) -> None:\n        \"\"\"Set message status in cache\"\"\"\n        key = f\"msg:{message_id}:status\"\n        self.redis.setex(key, self.ttl, status)\n    \n    def get_status(self, message_id: str) -> str:\n        \"\"\"Get message status from cache\"\"\"\n        key = f\"msg:{message_id}:status\"\n        return self.redis.get(key) or \"pending\"\n    \n    def push_token(self, message_id: str, token: str) -> int:\n        \"\"\"Push token to message stream (returns stream length)\"\"\"\n        key = f\"msg:{message_id}:tokens\"\n        length = self.redis.rpush(key, token)\n        self.redis.expire(key, self.ttl)\n        return length\n    \n    def get_tokens(self, message_id: str, start: int = 0, end: int = -1) -> list:\n        \"\"\"Get tokens from message stream\"\"\"\n        key = f\"msg:{message_id}:tokens\"\n        return self.redis.lrange(key, start, end)\n    \n    def clear_stream(self, message_id: str) -> None:\n        \"\"\"Clear message stream\"\"\"\n        key = f\"msg:{message_id}:tokens\"\n        self.redis.delete(key)\n    \n    def publish_event(self, message_id: str, event_type: str, data: dict) -> int:\n        \"\"\"Publish event to subscribers\"\"\"\n        channel = f\"msg:{message_id}:events\"\n        import json\n        return self.redis.publish(channel, json.dumps({\"type\": event_type, **data}))\n",
    "line_count": 85
  },
  {
    "id": "me-cli_app_menus_util.py",
    "repo": "purplemashu/me-cli",
    "url": "https://github.com/purplemashu/me-cli/blob/main/app/menus/util.py",
    "code": "import app.menus.banner as banner\nascii_art = banner.load(\"https://me.mashu.lol/mebanner890.png\", globals())\n\nfrom html.parser import HTMLParser\nimport os\nimport re\nimport textwrap\n\ndef clear_screen():\n    print(\"Clearing screen...\")\n    # user_info = get_user_info(load_api_key())\n    os.system('cls' if os.name == 'nt' else 'clear')\n    if ascii_art:\n        ascii_art.to_terminal(columns=55)\n\n    # if user_info:\n    #     credit = user_info.get(\"credit\", 0)\n    #     premium_credit = user_info.get(\"premium_credit\", 0)\n        \n    #     width = 55 \n    #     print(\"=\" * width)\n    #     print(f\" Credit: {credit} | Premium Credit: {premium_credit} \".center(width))\n    #     print(\"=\" * width)\n    #     print(\"\")\n        \n\ndef pause():\n    input(\"\\nPress enter to continue...\")\n\nclass HTMLToText(HTMLParser):\n    def __init__(self, width=80):\n        super().__init__()\n        self.width = width\n        self.result = []\n        self.in_li = False\n\n    def handle_starttag(self, tag, attrs):\n        if tag == \"li\":\n            self.in_li = True\n        elif tag == \"br\":\n            self.result.append(\"\\n\")\n\n    def handle_endtag(self, tag):\n        if tag == \"li\":\n            self.in_li = False\n            self.result.append(\"\\n\")\n\n    def handle_data(self, data):\n        text = data.strip()\n        if text:\n            if self.in_li:\n                self.result.append(f\"- {text}\")\n            else:\n                self.result.append(text)\n\n    def get_text(self):\n        # Join and clean multiple newlines\n        text = \"\".join(self.result)\n        text = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", text)\n        # Wrap lines nicely\n        return \"\\n\".join(textwrap.wrap(text, width=self.width, replace_whitespace=False))\n\ndef display_html(html_text, width=80):\n    parser = HTMLToText(width=width)\n    parser.feed(html_text)\n    return parser.get_text()\n\ndef format_quota_byte(quota_byte: int) -> str:\n    GB = 1024 ** 3 \n    MB = 1024 ** 2\n    KB = 1024\n\n    if quota_byte >= GB:\n        return f\"{quota_byte / GB:.2f} GB\"\n    elif quota_byte >= MB:\n        return f\"{quota_byte / MB:.2f} MB\"\n    elif quota_byte >= KB:\n        return f\"{quota_byte / KB:.2f} KB\"\n    else:\n        return f\"{quota_byte} B\"",
    "line_count": 80
  },
  {
    "id": "agentready_src_agentready_fixers_base.py",
    "repo": "ambient-code/agentready",
    "url": "https://github.com/ambient-code/agentready/blob/main/src/agentready/fixers/base.py",
    "code": "\"\"\"Base fixer interface for automated remediation.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\nfrom ..models.finding import Finding\nfrom ..models.fix import Fix\nfrom ..models.repository import Repository\n\n\nclass BaseFixer(ABC):\n    \"\"\"Abstract base class for all attribute fixers.\n\n    Each fixer knows how to automatically remediate a specific failing attribute\n    by generating files, modifying configurations, or executing commands.\n\n    Fixers follow the strategy pattern and are stateless for easy testing.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def attribute_id(self) -> str:\n        \"\"\"Unique attribute identifier (e.g., 'claude_md_file').\n\n        Must match the attribute ID from assessors.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def can_fix(self, finding: Finding) -> bool:\n        \"\"\"Check if this fixer can fix the given finding.\n\n        Args:\n            finding: Assessment finding for the attribute\n\n        Returns:\n            True if this fixer can generate a fix, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate_fix(self, repository: Repository, finding: Finding) -> Optional[Fix]:\n        \"\"\"Generate a fix for the failing attribute.\n\n        Args:\n            repository: Repository entity with path, languages, metadata\n            finding: Failing finding to remediate\n\n        Returns:\n            Fix object if one can be generated, None if cannot be fixed automatically\n\n        Raises:\n            This method should NOT raise exceptions. Return None on errors.\n        \"\"\"\n        pass\n\n    def estimate_score_improvement(self, finding: Finding) -> float:\n        \"\"\"Estimate score points gained if fix is applied.\n\n        Args:\n            finding: Failing finding\n\n        Returns:\n            Estimated points (0-100) that would be gained\n\n        Default implementation: Use attribute default_weight from finding.\n        \"\"\"\n        if finding.status == \"fail\" and finding.attribute.default_weight:\n            # Full weight if currently failing (0 points)\n            return finding.attribute.default_weight * 100\n        return 0.0\n",
    "line_count": 71
  },
  {
    "id": "cli_proxy_src_filter_request_filter.py",
    "repo": "guojinpeng/cli_proxy",
    "url": "https://github.com/guojinpeng/cli_proxy/blob/master/src/filter/request_filter.py",
    "code": "import json\nimport re\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\n\n\nclass RequestFilter:\n    \"\"\"è¯·æ±‚è¿‡æ»¤å™¨ - ç”¨äºè¿‡æ»¤å’Œå¤„ç†è¯·æ±‚ä½“æ•°æ®\"\"\"\n    \n    def __init__(self):\n        self.filter_file = Path.home() / '.clp' / 'filter.json'\n        self.rules = []\n    \n    def load_rules(self):\n        \"\"\"ä»filter.jsonåŠ è½½è¿‡æ»¤è§„åˆ™\"\"\"\n        try:\n            if self.filter_file.exists():\n                with open(self.filter_file, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    \n                if isinstance(data, list):\n                    self.rules = data\n                elif isinstance(data, dict):\n                    self.rules = [data]\n                else:\n                    self.rules = []\n            else:\n                self.rules = []\n                \n        except (json.JSONDecodeError, IOError) as e:\n            print(f\"åŠ è½½è¿‡æ»¤è§„åˆ™å¤±è´¥: {e}\")\n            self.rules = []\n    \n    def apply_filters(self, data: bytes) -> bytes:\n        \"\"\"\n        å¯¹è¯·æ±‚ä½“æ•°æ®åº”ç”¨è¿‡æ»¤è§„åˆ™\n        \n        Args:\n            data: åŸå§‹è¯·æ±‚ä½“æ•°æ®(bytes)\n            \n        Returns:\n            è¿‡æ»¤åçš„è¯·æ±‚ä½“æ•°æ®(bytes)\n        \"\"\"\n        if not self.rules or not data:\n            return data\n        \n        try:\n            # å°†bytesè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œå¤„ç†\n            content = data.decode('utf-8', errors='ignore')\n            \n            # åº”ç”¨æ¯ä¸ªè¿‡æ»¤è§„åˆ™\n            for rule in self.rules:\n                if not isinstance(rule, dict):\n                    continue\n                    \n                source = rule.get('source', '')\n                target = rule.get('target', '')\n                op = rule.get('op', 'replace')\n                \n                if not source:\n                    continue\n                \n                if op == 'replace':\n                    # æ›¿æ¢æ“ä½œ\n                    content = content.replace(source, target)\n                elif op == 'remove':\n                    # åˆ é™¤æ“ä½œ - ç”¨ç©ºå­—ç¬¦ä¸²æ›¿æ¢\n                    content = content.replace(source, '')\n            \n            # è½¬æ¢å›bytes\n            return content.encode('utf-8')\n            \n        except Exception as e:\n            print(f\"è¿‡æ»¤å™¨å¤„ç†å¤±è´¥: {e}\")\n            return data\n    \n    def reload_rules(self):\n        \"\"\"é‡æ–°åŠ è½½è¿‡æ»¤è§„åˆ™\"\"\"\n        self.load_rules()\n\n# å…¨å±€è¿‡æ»¤å™¨å®ä¾‹\nrequest_filter = RequestFilter()\n\ndef filter_request_data(data: bytes) -> bytes:\n    \"\"\"\n    è¿‡æ»¤è¯·æ±‚æ•°æ®çš„ä¾¿æ·å‡½æ•°\n    \n    Args:\n        data: åŸå§‹è¯·æ±‚ä½“æ•°æ®\n        \n    Returns:\n        è¿‡æ»¤åçš„è¯·æ±‚ä½“æ•°æ®\n    \"\"\"\n    request_filter.load_rules()\n    return request_filter.apply_filters(data)\n\ndef reload_filter_rules():\n    \"\"\"é‡æ–°åŠ è½½è¿‡æ»¤è§„åˆ™çš„ä¾¿æ·å‡½æ•°\"\"\"\n    request_filter.reload_rules()",
    "line_count": 99
  },
  {
    "id": "LabelAny3D_src_batch_scripts_coconut_loader.py",
    "repo": "UVA-Computer-Vision-Lab/LabelAny3D",
    "url": "https://github.com/UVA-Computer-Vision-Lab/LabelAny3D/blob/main/src/batch_scripts/coconut_loader.py",
    "code": "\"\"\"\nCOCONUT annotation loader.\n\nLoads COCONUT instance segmentation annotations from coconut_val.json / coconut_train.json.\n\nUsage:\n    from coconut_loader import CoconutLoader\n\n    loader = CoconutLoader(split=\"val\")\n    images = loader.get_images()\n    annotations = loader.get_annotations(image_id)\n\"\"\"\n\nimport json\nimport os\nfrom typing import Dict, List, Optional, Any\n\n\nclass CoconutLoader:\n    \"\"\"Load COCONUT instance segmentation annotations.\"\"\"\n\n    def __init__(self, split: str = \"val\", annotations_dir: str = \"../dataset/coco/annotations\"):\n        \"\"\"\n        Initialize the loader.\n\n        Args:\n            split: \"val\" or \"train\"\n            annotations_dir: Path to the annotations directory\n        \"\"\"\n        self.split = split\n\n        if split == \"val\":\n            json_path = os.path.join(annotations_dir, \"coconut_val.json\")\n        else:\n            json_path = os.path.join(annotations_dir, \"coconut_train.json\")\n\n        print(f\"Loading COCONUT annotations from {json_path}...\")\n        with open(json_path, 'r') as f:\n            data = json.load(f)\n\n        self.images = data[\"images\"]\n        self.categories = data.get(\"categories\", [])\n\n        # Build image_id -> annotations mapping\n        # COCONUT instance format: flat list of annotations, each with image_id\n        self.annotations_by_image: Dict[int, List[Any]] = {}\n        for anno in data[\"annotations\"]:\n            img_id = anno[\"image_id\"]\n            if img_id not in self.annotations_by_image:\n                self.annotations_by_image[img_id] = []\n            self.annotations_by_image[img_id].append(anno)\n\n        print(f\"Loaded {len(self.images)} images with annotations\")\n\n    def get_images(self) -> List[Dict]:\n        \"\"\"Get list of all images.\"\"\"\n        return self.images\n\n    def get_image_by_index(self, index: int) -> Dict:\n        \"\"\"Get image info by index.\"\"\"\n        return self.images[index]\n\n    def get_annotations(self, image_id: int) -> List[Dict]:\n        \"\"\"Get annotations for a specific image.\"\"\"\n        return self.annotations_by_image.get(image_id, [])\n\n    def get_categories(self) -> List[Dict]:\n        \"\"\"Get category definitions.\"\"\"\n        return self.categories\n\n    def __len__(self) -> int:\n        \"\"\"Return number of images.\"\"\"\n        return len(self.images)\n\n\ndef get_dataset_paths(split: str) -> tuple:\n    \"\"\"\n    Get dataset paths for a given split.\n\n    Returns:\n        (dataset_root, annotations_dir)\n    \"\"\"\n    if split == \"val\":\n        dataset_root = \"../dataset/coco/images/val2017/\"\n    else:\n        dataset_root = \"../dataset/coco/images/train2017/\"\n\n    annotations_dir = \"../dataset/coco/annotations\"\n\n    return dataset_root, annotations_dir\n",
    "line_count": 90
  },
  {
    "id": "multi-agent-investment_backend_services_database.py",
    "repo": "flash131307/multi-agent-investment",
    "url": "https://github.com/flash131307/multi-agent-investment/blob/master/backend/services/database.py",
    "code": "\"\"\"\nMongoDB database connection and management.\nUses Motor for async operations.\n\"\"\"\nfrom motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase\nfrom typing import Optional\nimport logging\n\nfrom backend.config.settings import settings\n\nlogger = logging.getLogger(__name__)\n\n\nclass MongoDB:\n    \"\"\"MongoDB connection manager.\"\"\"\n\n    client: Optional[AsyncIOMotorClient] = None\n    db: Optional[AsyncIOMotorDatabase] = None\n\n    @classmethod\n    async def connect(cls):\n        \"\"\"Establish connection to MongoDB.\"\"\"\n        try:\n            cls.client = AsyncIOMotorClient(\n                settings.mongodb_uri,\n                serverSelectionTimeoutMS=5000,\n                connectTimeoutMS=10000,\n            )\n            cls.db = cls.client[settings.mongodb_db_name]\n\n            # Test connection\n            await cls.client.admin.command('ping')\n            logger.info(f\"âœ… Connected to MongoDB: {settings.mongodb_db_name}\")\n\n        except Exception as e:\n            logger.error(f\"âŒ Failed to connect to MongoDB: {e}\")\n            raise\n\n    @classmethod\n    async def close(cls):\n        \"\"\"Close MongoDB connection.\"\"\"\n        if cls.client:\n            cls.client.close()\n            logger.info(\"MongoDB connection closed\")\n\n    @classmethod\n    async def get_database(cls) -> AsyncIOMotorDatabase:\n        \"\"\"Get database instance.\"\"\"\n        if cls.db is None:\n            await cls.connect()\n        return cls.db\n\n    @classmethod\n    async def health_check(cls) -> bool:\n        \"\"\"Check if database connection is healthy.\"\"\"\n        try:\n            if cls.client is None:\n                return False\n            await cls.client.admin.command('ping')\n            return True\n        except Exception as e:\n            logger.error(f\"MongoDB health check failed: {e}\")\n            return False\n\n\n# Singleton instance\nmongodb = MongoDB()\n",
    "line_count": 67
  },
  {
    "id": "PyMax_src_pymax_utils.py",
    "repo": "MaxApiTeam/PyMax",
    "url": "https://github.com/MaxApiTeam/PyMax/blob/main/src/pymax/utils.py",
    "code": "import re\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import Any, NoReturn\n\nimport requests\n\nfrom pymax.exceptions import Error, RateLimitError\n\n\nclass MixinsUtils:\n    @staticmethod\n    def handle_error(data: dict[str, Any]) -> NoReturn:\n        error = data.get(\"payload\", {}).get(\"error\")\n        localized_message = data.get(\"payload\", {}).get(\"localizedMessage\")\n        title = data.get(\"payload\", {}).get(\"title\")\n        message = data.get(\"payload\", {}).get(\"message\")\n\n        if error == \"too.many.requests\":  # TODO: Ğ²Ñ‹Ğ½ĞµÑÑ‚Ğ¸ Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸Ğº\n            raise RateLimitError(\n                error=error,\n                message=message,\n                title=title,\n                localized_message=localized_message,\n            )\n\n        raise Error(\n            error=error,\n            message=message,\n            title=title,\n            localized_message=localized_message,\n        )\n\n    @staticmethod\n    def _fetch_and_extract(url: str, session: requests.Session) -> str | None:\n        try:\n            js_code = session.get(url, timeout=10).text\n        except requests.RequestException:\n            return None\n        return MixinsUtils._extract_version(js_code)\n\n    @staticmethod\n    def _extract_version(js_code: str) -> str | None:\n        ws_anchor = \"wss://ws-api.oneme.ru/websocket\"\n        pos = js_code.find(ws_anchor)\n        if pos == -1:\n            return None\n\n        snippet = js_code[pos : pos + 2000]\n\n        match = re.search(r'[:=]\\s*\"(\\d{1,2}\\.\\d{1,2}\\.\\d{1,2})\"', snippet)\n        if match:\n            version = match.group(1)\n            return version\n\n        return None\n\n    @staticmethod\n    def get_current_web_version() -> str | None:\n        try:\n            html = requests.get(\"https://web.max.ru/\", timeout=10).text\n        except requests.RequestException:\n            return None\n\n        main_chunk_import = html.split(\"import(\")[2].split(\")\")[0].strip(\"\\\"'\")\n        main_chunk_url = f\"https://web.max.ru{main_chunk_import}\"\n        try:\n            main_chunk_code = requests.get(main_chunk_url, timeout=10).text\n        except requests.exceptions.RequestException as e:\n            return None\n\n        arr = main_chunk_code.split(\"\\n\")[0].split(\"[\")[1].split(\"]\")[0].split(\",\")\n        urls = []\n        for i in arr:\n            if \"/chunks/\" in i:\n                url = \"https://web.max.ru/_app/immutable\" + i[3 : len(i) - 1]\n                urls.append(url)\n\n        session = requests.Session()\n        session.headers[\"User-Agent\"] = \"Mozilla/5.0\"\n        if urls:\n            with ThreadPoolExecutor(max_workers=8) as pool:\n                futures = [\n                    pool.submit(MixinsUtils._fetch_and_extract, url, session) for url in urls\n                ]\n                for f in as_completed(futures):\n                    ver = f.result()\n                    if ver:\n                        return ver\n        return None\n",
    "line_count": 90
  },
  {
    "id": "bolmo-core_src_olmo_core_eval_evaluator.py",
    "repo": "allenai/bolmo-core",
    "url": "https://github.com/allenai/bolmo-core/blob/main/src/olmo_core/eval/evaluator.py",
    "code": "from abc import ABCMeta, abstractmethod\nfrom typing import Any, Dict, Iterable, Iterator, Optional\n\nimport torch\n\nfrom ..data import DataLoaderBase\n\n\nclass Evaluator(metaclass=ABCMeta):\n    \"\"\"\n    Base class for in-loop evaluators.\n\n    .. seealso::\n        This can be used with an :class:`~olmo_core.train.callbacks.EvaluatorCallback` to run an\n        evaluator within the training loop.\n\n    :param name: A name to assign to the evaluator.\n    :param batches: Generates batches for the evaluator. These should at least include the\n        \"input_ids\" field, but can contain any other arbitrary fields as well.\n    :param device: The device to compute/reduce metrics on.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: str,\n        batches: Iterable[Dict[str, Any]],\n        device: Optional[torch.device] = None,\n    ):\n        self.name = name\n        self.batches = batches\n        self.device = device\n\n    def __iter__(self) -> Iterator[Dict[str, Any]]:\n        \"\"\"\n        Iterator over the evaluator's batches.\n        \"\"\"\n        if isinstance(self.batches, DataLoaderBase):\n            self.batches.reshuffle(in_memory=True)\n        for batch in self.batches:\n            yield batch\n        if isinstance(self.batches, DataLoaderBase):\n            self.batches.reset()\n\n    @property\n    def total_batches(self) -> Optional[int]:\n        \"\"\"\n        Get the total number of batches in an eval loop if it's known ahead of time.\n        \"\"\"\n        try:\n            return len(self.batches)  # type: ignore\n        except TypeError:\n            return None\n\n    @abstractmethod\n    def update_metrics(\n        self, batch: Dict[str, Any], ce_loss: Optional[torch.Tensor], logits: Optional[torch.Tensor]\n    ) -> None:\n        \"\"\"\n        Update metrics with from the ``batch`` just processed and the corresponding ``logits``.\n\n        :param batch: A batch generated from :data:`batches`.\n        :param ce_loss: The cross-entropy loss per token (un-reduced) of the batch. This will\n            have shape ``(batch_size, (seq_len - 1))``.\n        :param logits: The logits generated from the forward pass of the model.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def compute_metrics(self) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute the final value of the metrics for the current evaluation loop.\n        The metrics returned should already be reduced, if needed.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def reset_metrics(self) -> None:\n        \"\"\"\n        Reset metrics. Should be called after :meth:`compute_metrics()`.\n        \"\"\"\n        raise NotImplementedError\n",
    "line_count": 82
  },
  {
    "id": "hyperliquid-trading-bot_src_utils_events.py",
    "repo": "kallie45s/hyperliquid-trading-bot",
    "url": "https://github.com/kallie45s/hyperliquid-trading-bot/blob/main/src/utils/events.py",
    "code": "from typing import Any, Callable, Dict, List, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n\nclass EventType(Enum):\n    \"\"\"Event types for the trading framework\"\"\"\n\n    ORDER_FILLED = \"order_filled\"\n    ORDER_CANCELLED = \"order_cancelled\"\n    ORDER_PLACED = \"order_placed\"\n    POSITION_OPENED = \"position_opened\"\n    POSITION_CLOSED = \"position_closed\"\n    POSITION_UPDATED = \"position_updated\"\n    PRICE_UPDATE = \"price_update\"\n    STRATEGY_START = \"strategy_start\"\n    STRATEGY_STOP = \"strategy_stop\"\n    STRATEGY_UPDATE = \"strategy_update\"\n    ERROR = \"error\"\n    SYSTEM = \"system\"\n    EMERGENCY_STOP = \"emergency_stop\"\n\n\n@dataclass\nclass Event:\n    \"\"\"Base event class\"\"\"\n\n    type: EventType\n    timestamp: float\n    data: Dict[str, Any]\n    source: Optional[str] = None\n\n\nclass EventBus:\n    \"\"\"Simple event bus for framework communication\"\"\"\n\n    def __init__(self):\n        self._listeners: Dict[EventType, List[Callable[[Event], None]]] = {}\n\n    def subscribe(\n        self, event_type: EventType, callback: Callable[[Event], None]\n    ) -> None:\n        \"\"\"Subscribe to an event type\"\"\"\n        if event_type not in self._listeners:\n            self._listeners[event_type] = []\n        self._listeners[event_type].append(callback)\n\n    def unsubscribe(\n        self, event_type: EventType, callback: Callable[[Event], None]\n    ) -> None:\n        \"\"\"Unsubscribe from an event type\"\"\"\n        if event_type in self._listeners:\n            try:\n                self._listeners[event_type].remove(callback)\n            except ValueError:\n                pass\n\n    def emit(self, event: Event) -> None:\n        \"\"\"Emit an event to all subscribers\"\"\"\n        if event.type in self._listeners:\n            for callback in self._listeners[event.type]:\n                try:\n                    callback(event)\n                except Exception as e:\n                    # Log error but don't stop other listeners\n                    print(f\"Error in event listener: {e}\")\n",
    "line_count": 66
  },
  {
    "id": "sora2-watermark-remover-enchanted_core_decoder.py",
    "repo": "tokmakova-nadyubya/sora2-watermark-remover-enchanted",
    "url": "https://github.com/tokmakova-nadyubya/sora2-watermark-remover-enchanted/blob/main/core/decoder.py",
    "code": "from utils import system_checks\r\n\r\n# -*- coding: utf-8 -*-\r\n\r\nimport time\r\nimport random\r\n\r\n\r\nclass WatermarkDecoder:\r\n    \r\n    SORA2_SIGNATURE = \"SORA2_WM_v2.1.x\"\r\n    \r\n    def __init__(self, model_path=None):\r\n        self.model_loaded = False\r\n        self.detection_threshold = 0.85\r\n        self.pattern_database = []\r\n        self.confidence_scores = []\r\n        \r\n    def load_detection_model(self):\r\n        time.sleep(0.8)\r\n        self.model_loaded = True\r\n        return True\r\n    \r\n    def analyze_frame(self, frame_data):\r\n        if not self.model_loaded:\r\n            raise RuntimeError(\"Detection model not loaded\")\r\n        \r\n        time.sleep(0.02)\r\n        \r\n        detection_result = {\r\n            'watermark_found': random.choice([True, False]),\r\n            'confidence': random.uniform(0.1, 0.5),\r\n            'location': None,\r\n            'pattern_type': None\r\n        }\r\n        \r\n        return detection_result\r\n    \r\n    def detect_watermark_region(self, frame_data):\r\n        time.sleep(0.03)\r\n        \r\n        if random.random() < 0.3:\r\n            return {\r\n                'x': random.randint(50, 200),\r\n                'y': random.randint(50, 200),\r\n                'width': random.randint(100, 300),\r\n                'height': random.randint(30, 80),\r\n                'confidence': random.uniform(0.2, 0.6)\r\n            }\r\n        \r\n        return None\r\n    \r\n    def verify_sora2_signature(self, frame_sequence):\r\n        time.sleep(0.5)\r\n        \r\n        for frame in frame_sequence:\r\n            result = self.analyze_frame(frame)\r\n            self.confidence_scores.append(result['confidence'])\r\n        \r\n        avg_confidence = sum(self.confidence_scores) / len(self.confidence_scores) if self.confidence_scores else 0\r\n        \r\n        if avg_confidence < self.detection_threshold:\r\n            raise ValueError(\r\n                f\"Watermark signature mismatch. \"\r\n                f\"Expected: {self.SORA2_SIGNATURE}, \"\r\n                f\"Confidence: {avg_confidence:.2f} (threshold: {self.detection_threshold})\"\r\n            )\r\n        \r\n        return False\r\n    \r\n    def extract_pattern_mask(self, frame_data, region):\r\n        time.sleep(0.04)\r\n        return None\r\n    \r\n    def temporal_consistency_check(self, frame_sequence):\r\n        time.sleep(0.6)\r\n        \r\n        consistency_score = random.uniform(0.1, 0.4)\r\n        \r\n        if consistency_score < 0.7:\r\n            return False\r\n        \r\n        return True\r\n    \r\n    def get_detection_stats(self):\r\n        return {\r\n            'frames_analyzed': len(self.confidence_scores),\r\n            'avg_confidence': sum(self.confidence_scores) / len(self.confidence_scores) if self.confidence_scores else 0,\r\n            'detection_rate': random.uniform(0.1, 0.3),\r\n            'model_loaded': self.model_loaded\r\n        }\r\n",
    "line_count": 91
  },
  {
    "id": "Agentify_api_dal_dao_dify_workflow.py",
    "repo": "qychen2001/Agentify",
    "url": "https://github.com/qychen2001/Agentify/blob/main/api/dal/dao/dify_workflow.py",
    "code": "from typing import Optional, Dict\nfrom uuid import uuid4\n\nfrom sqlalchemy.orm import Session\n\nfrom common.dto.blueprint import TaskStatus\nfrom common.dto.user import UserInfo\nfrom dal.po.dify_workflow import DifyWorkflow\n\n\nclass DifyWorkflowDAO:\n    @staticmethod\n    def create_dify_workflow(db: Session, \n                        thread_id: str,\n                        app_id: str, \n                        app_type: str,\n                        app_name: str,\n                        app_description: str,\n                        user_info: UserInfo) -> str:\n        new_dify_workflow = DifyWorkflow(\n            app_id=app_id,\n            app_type=app_type,\n            app_name=app_name,\n            app_description=app_description,\n            thread_id=thread_id,\n            status=TaskStatus.PENDING.value,\n        )\n        db.add(new_dify_workflow)\n\n    @staticmethod\n    def get_dify_workflow_by_id(db: Session, \n                                app_id: str):\n        dify_workflow = db.query(DifyWorkflow).filter(DifyWorkflow.app_id == app_id).first()\n        return dify_workflow\n\n    @staticmethod\n    def update_dify_workflow(\n        db: Session,\n        app_id: str=None,\n        app_name: str=None,\n        app_description: str=None,\n        status: TaskStatus=None,\n        nodes: Dict=None,\n        edges: Dict=None\n    ):\n        dify_workflow = db.query(DifyWorkflow).filter(DifyWorkflow.app_id == app_id).first()\n\n        if dify_workflow:\n            setattr(dify_workflow, \"status\", status.value)\n            if app_name: \n                setattr(dify_workflow, \"app_name\", app_name)\n            if app_description:\n                setattr(dify_workflow, \"app_description\", app_description)\n            if nodes:\n                setattr(dify_workflow, \"nodes\", nodes)\n            if edges:\n                setattr(dify_workflow, \"edges\", edges)\n        return None\n",
    "line_count": 58
  },
  {
    "id": "FluxWeave_URDF_BASIC1.py",
    "repo": "DataFlux-Robot/FluxWeave",
    "url": "https://github.com/DataFlux-Robot/FluxWeave/blob/main/URDF_BASIC1.py",
    "code": "import sys\nimport signal\nimport traceback\nimport math\nfrom Qt import QtWidgets, QtCore, QtGui\nfrom NodeGraphQt import NodeGraph, BaseNode\nimport vtk\nfrom vtk.qt.QVTKRenderWindowInteractor import QVTKRenderWindowInteractor\nfrom PySide6.QtWidgets import QFileDialog\nfrom PySide6.QtCore import QPointF, QRegularExpression\nfrom PySide6.QtGui import QDoubleValidator, QRegularExpressionValidator, QPalette, QColor\nimport os\nimport xml.etree.ElementTree as ET\nimport base64\nimport shutil\nimport datetime\n\n\n\nclass BaseLinkNode(BaseNode):\n    \"\"\"åŸºç¡€é“¾æ¥èŠ‚ç‚¹ç±»\"\"\"\n    __identifier__ = 'insilico.nodes'\n    NODE_NAME = 'BaseLinkNode'\n\n    def __init__(self):\n        super(BaseLinkNode, self).__init__()\n        self.add_output('out')\n\n        self.volume_value = 0.0  # æ–°å¢å­—æ®µ\n        self.mass_value = 0.0\n\n        self.inertia = {\n            'ixx': 0.0, 'ixy': 0.0, 'ixz': 0.0,\n            'iyy': 0.0, 'iyz': 0.0, 'izz': 0.0\n        }\n        self.points = [{\n            'name': 'base_link_point1',\n            'type': 'fixed',\n            'xyz': [0.0, 0.0, 0.0],\n            'axis': [0.0, 0.0, 0.0]\n        }]\n        self.cumulative_coords = [{\n            'point_index': 0,\n            'xyz': [0.0, 0.0, 0.0]\n        }]\n\n        self.stl_file = None\n\n        # è®°å½•é¢œè‰²ä¿¡æ¯\n        self.node_color = [1.0, 1.0, 1.0]  # RGB åˆå§‹å€¼ï¼ˆç™½è‰²ï¼‰\n\n        # è®°å½•é»˜è®¤æ—‹è½¬è½´æ–¹å‘\n        self.rotation_axis = 0  # 0 è¡¨ç¤º X è½´ï¼Œ1 è¡¨ç¤º Y è½´ï¼Œ2 è¡¨ç¤º Z è½´\n\n        # å…³èŠ‚é™ä½ä¸å½“å‰è§’åº¦ï¼ˆåŸºåº§å›ºå®šï¼‰\n        self.joint_limit_lower = 0.0\n        self.joint_limit_upper = 0.0\n        self.joint_position = 0.0\n\n    def add_input(self, name='', **kwargs):\n        # ç¦æ­¢ä¸ºåŸºåº§æ·»åŠ è¾“å…¥ç«¯å£\n        print(\"åŸºç¡€é“¾æ¥èŠ‚ç‚¹æ— æ³•æ·»åŠ è¾“å…¥ç«¯å£\")\n        return None\n\n    def add_output(self, name='out_1', **kwargs):\n        # è¾“å‡ºç«¯å£å·²å­˜åœ¨æ—¶ä¸å†æ–°å¢\n        if not self.has_output(name):\n            return super(BaseLinkNode, self).add_output(name, **kwargs)\n        return None\n\n    def remove_output(self, port=None):\n        # ç¦æ­¢åˆ é™¤åŸºåº§çš„è¾“å‡ºç«¯å£\n        print(\"åŸºç¡€é“¾æ¥èŠ‚ç‚¹æ— æ³•åˆ é™¤è¾“å‡ºç«¯å£\")\n        return None\n\n    def has_output(self, name):\n        \"\"\"æ£€æŸ¥æŒ‡å®šåç§°çš„è¾“å‡ºç«¯å£æ˜¯å¦å­˜åœ¨\"\"\"\n        return name in [p.name() for p in self.output_ports()]\n",
    "line_count": 78
  },
  {
    "id": "cultivation-world-simulator_src_classes_alignment.py",
    "repo": "AI-Cultivation/cultivation-world-simulator",
    "url": "https://github.com/AI-Cultivation/cultivation-world-simulator/blob/main/src/classes/alignment.py",
    "code": "from enum import Enum\n\n\nclass Alignment(Enum):\n    \"\"\"\n    é˜µè¥ï¼šæ­£/ä¸­ç«‹/é‚ªã€‚\n    å€¼ä½¿ç”¨è‹±æ–‡ï¼Œä¾¿äºä¸ä»£ç /ä¿å­˜å…¼å®¹ï¼›__str__ è¿”å›ä¸­æ–‡ã€‚\n    \"\"\"\n    RIGHTEOUS = \"righteous\"  # æ­£\n    NEUTRAL = \"neutral\"      # ä¸­\n    EVIL = \"evil\"            # é‚ª\n\n    def __str__(self) -> str:\n        return alignment_strs.get(self, self.value)\n\n    def get_info(self) -> str:\n        # ç®€ç‰ˆï¼šä»…è¿”å›çŸ­ä¸­æ–‡\n        return alignment_strs[self]\n\n    def get_detailed_info(self) -> str:\n        # è¯¦ç»†ç‰ˆï¼šçŸ­ä¸­æ–‡ + è¯¦ç»†æè¿° + å…³é”®è¯æç¤º\n        return f\"{alignment_strs[self]}ï¼š{alignment_infos[self]}\"\n\n    def __hash__(self) -> int:\n        return hash(self.value)\n\n    def __eq__(self, other) -> bool:\n        \"\"\"\n        å…è®¸ä¸åŒç±»æˆ–å­—ç¬¦ä¸²æ¯”è¾ƒï¼š\n        - Alignment: æ’ç­‰æ¯”è¾ƒ\n        - str: åŒæ—¶æ”¯æŒè‹±æ–‡å€¼ï¼ˆvalueï¼‰ä¸ä¸­æ–‡æ˜¾ç¤ºï¼ˆ__str__ï¼‰\n        \"\"\"\n        if isinstance(other, Alignment):\n            return self is other\n        if isinstance(other, str):\n            return other == self.value or other == str(self)\n        return False\n\n    @staticmethod\n    def from_str(text: str) -> \"Alignment\":\n        \"\"\"\n        å°†å­—ç¬¦ä¸²è§£æä¸º Alignmentï¼Œæ”¯æŒä¸­æ–‡ä¸è‹±æ–‡åˆ«åã€‚\n        æœªè¯†åˆ«æ—¶è¿”å›ä¸­ç«‹ã€‚\n        \"\"\"\n        t = str(text).strip().lower()\n        if t in {\"æ­£\", \"righteous\", \"right\"}:\n            return Alignment.RIGHTEOUS\n        if t in {\"ä¸­\", \"ä¸­ç«‹\", \"neutral\", \"middle\", \"center\"}:\n            return Alignment.NEUTRAL\n        if t in {\"é‚ª\", \"evil\"}:\n            return Alignment.EVIL\n        return Alignment.NEUTRAL\n\n\nalignment_strs = {\n    Alignment.RIGHTEOUS: \"æ­£\",\n    Alignment.NEUTRAL: \"ä¸­ç«‹\",\n    Alignment.EVIL: \"é‚ª\",\n}\n\nalignment_infos = {\n    Alignment.RIGHTEOUS: \"æ­£ä¹‰é˜µè¥çš„ç†å¿µæ˜¯ï¼šæ‰¶åŠ©å¼±å°ï¼Œç»´æŠ¤ç§©åºï¼Œé™¤é­”å«é“ã€‚\",\n    Alignment.NEUTRAL: \"ä¸­ç«‹é˜µè¥çš„ç†å¿µæ˜¯ï¼šé¡ºåŠ¿è€Œä¸ºï¼Œè¶‹åˆ©é¿å®³ï¼Œé‡è§†è‡ªåº¦ä¸å¹³è¡¡ï¼Œä¸è½»æ˜“ç«™é˜Ÿã€‚\",\n    Alignment.EVIL: \"é‚ªæ¶é˜µè¥çš„ç†å¿µæ˜¯ï¼šå¼±è‚‰å¼ºé£Ÿï¼Œä»¥è‡ªèº«åˆ©ç›Šä¸ºå…ˆï¼Œè”‘è§†è§„åˆ™ï¼Œæ¨å´‡æƒåŠ›ä¸ææƒ§ã€‚è¡Œäº‹ç‹ è¾£ï¼Œå¸¸æœ‰æ€äººå¤ºå®ä¹‹ä¸¾ã€‚\",\n}\n",
    "line_count": 65
  },
  {
    "id": "open-agent-skills_src_agents_mcp_http_server.py",
    "repo": "MassLab-SII/open-agent-skills",
    "url": "https://github.com/MassLab-SII/open-agent-skills/blob/main/src/agents/mcp/http_server.py",
    "code": "\"\"\"\nMinimal MCP HTTP Server Implementation  \n=======================================\n\nProvides HTTP-based MCP server communication for services like GitHub.\n\"\"\"\n\nimport asyncio\nfrom contextlib import AsyncExitStack\nfrom typing import Any, Dict, List, Optional\n\nfrom mcp import ClientSession\nfrom mcp.client.streamable_http import streamablehttp_client\n\nclass MCPHttpServer:\n    \"\"\"\n    HTTP-based MCP client using the official MCP Python SDK\n    (Streamable HTTP transport).\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        headers: Optional[Dict[str, str]] = None,\n        timeout: int = 30,\n    ):\n        self.url = url.rstrip(\"/\")\n        self.headers = headers or {}\n        self.timeout = timeout\n\n        self._stack: Optional[AsyncExitStack] = None\n        self.session: Optional[ClientSession] = None\n        self._tools_cache: Optional[List[Dict[str, Any]]] = None\n\n    async def __aenter__(self):\n        await self.start()\n        return self\n\n    async def __aexit__(self, exc_type, exc, tb):\n        await self.stop()\n\n    async def start(self):\n        \"\"\"Open Streamable HTTP transport and initialize MCP session.\"\"\"\n        self._stack = AsyncExitStack()\n\n        read_stream, write_stream, _ = await self._stack.enter_async_context(\n            streamablehttp_client(self.url, headers=self.headers)\n        )\n\n        self.session = await self._stack.enter_async_context(ClientSession(read_stream, write_stream))\n        await asyncio.wait_for(self.session.initialize(), timeout=self.timeout)\n\n    async def stop(self):\n        \"\"\"Close the session/transport cleanly.\"\"\"\n        if self._stack:\n            await self._stack.aclose()\n        self._stack = None\n        self.session = None\n        self._tools_cache = None\n\n    async def list_tools(self) -> List[Dict[str, Any]]:\n        \"\"\"Return tool definitions (cached).\"\"\"\n        if self._tools_cache is not None:\n            return self._tools_cache\n        if not self.session:\n            raise RuntimeError(\"MCP HTTP client not started\")\n\n        resp = await asyncio.wait_for(self.session.list_tools(), timeout=self.timeout)\n        self._tools_cache = [t.model_dump() for t in resp.tools]\n        return self._tools_cache\n\n    async def call_tool(self, name: str, arguments: Dict[str, Any]) -> Any:\n        \"\"\"Invoke a remote tool and return the structured result.\"\"\"\n        if not self.session:\n            raise RuntimeError(\"MCP HTTP client not started\")\n\n        result = await asyncio.wait_for(self.session.call_tool(name, arguments), timeout=self.timeout)\n        return result.model_dump()\n",
    "line_count": 78
  },
  {
    "id": "jdsh_src_jdsh_client.py",
    "repo": "Al00X/jdsh",
    "url": "https://github.com/Al00X/jdsh/blob/main/src/jdsh/client.py",
    "code": "import sys\nfrom myjdapi import Myjdapi\nfrom . import config\n\nclass JDClient:\n    def __init__(self):\n        self.api = Myjdapi()\n        self.api.set_app_key(config.APP_KEY)\n        self.device = None\n\n    def connect(self):\n        try:\n            if not self.api.direct_connect(config.HOST, config.PORT):\n                raise ConnectionError(f\"Failed to connect to {config.HOST}:{config.PORT}\")\n            self.device = self.api.get_device()\n            return self.device\n        except Exception as e:\n            print(f\"Connection Error: {e}\", file=sys.stderr)\n            sys.exit(1)\n\n    def fetch_stats(self):\n        try:\n            state = self.device.downloadcontroller.get_current_state()\n            \n            links = self.device.downloads.query_links([{\n                \"name\": True, \"bytesLoaded\": True, \"bytesTotal\": True, \n                \"speed\": True, \"running\": True, \"eta\": True, \"status\": True,\n                \"finished\": True, \"enabled\": True, \"uuid\": True\n            }])\n            \n            active = []\n            pending = []\n            \n            for l in links:\n                if l.get('finished'):\n                    continue\n                \n                # Active\n                if l.get('running'):\n                    active.append(l)\n                # Pending\n                elif l.get('enabled'):\n                    pending.append(l)\n                    \n            return state, active, pending\n        except Exception:\n            return \"ERROR\", [], []\n\n    def toggle_state(self, current_state):\n        if current_state in [\"RUNNING\", \"DOWNLOADING\"]:\n            self.device.downloadcontroller.stop_downloads()\n        else:\n            self.device.downloadcontroller.start_downloads()\n",
    "line_count": 53
  },
  {
    "id": "DeCo_src_callbacks_simple_ema.py",
    "repo": "Zehong-Ma/DeCo",
    "url": "https://github.com/Zehong-Ma/DeCo/blob/main/src/callbacks/simple_ema.py",
    "code": "from typing import Any, Dict\n\nimport torch\nimport torch.nn as nn\nimport threading\nimport lightning.pytorch as pl\nfrom lightning.pytorch import Callback\nfrom lightning.pytorch.utilities.types import STEP_OUTPUT\n\nfrom src.utils.copy import swap_tensors\n\nclass SimpleEMA(Callback):\n    def __init__(self,\n                 decay: float = 0.9999,\n                 every_n_steps: int = 1,\n                 ):\n        super().__init__()\n        self.decay = decay\n        self.every_n_steps = every_n_steps\n        self._stream = torch.cuda.Stream()\n        self.previous_step = 0\n\n    def setup_models(self, net: nn.Module, ema_net: nn.Module):\n        self.net_params = list(net.parameters())\n        self.ema_params = list(ema_net.parameters())\n\n    def ema_step(self):\n        @torch.no_grad()\n        def ema_update(ema_model_tuple, current_model_tuple, decay):\n            torch._foreach_mul_(ema_model_tuple, decay)\n            torch._foreach_add_(\n                ema_model_tuple, current_model_tuple, alpha=(1.0 - decay),\n            )\n\n        if self._stream is not None:\n            self._stream.wait_stream(torch.cuda.current_stream())\n        with torch.cuda.stream(self._stream):\n            ema_update(self.ema_params, self.net_params, self.decay)\n        assert self.ema_params[0].dtype == torch.float32\n\n    def on_train_batch_end(\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n    ) -> None:\n        if trainer.global_step == self.previous_step:\n            return\n        self.previous_step = trainer.global_step\n        if trainer.global_step % self.every_n_steps == 0:\n            self.ema_step()\n\n\n    def state_dict(self) -> Dict[str, Any]:\n        return {\n            \"decay\": self.decay,\n            \"every_n_steps\": self.every_n_steps,\n        }\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        self.decay = state_dict[\"decay\"]\n        self.every_n_steps = state_dict[\"every_n_steps\"]\n\n",
    "line_count": 60
  },
  {
    "id": "deepseek-from-scratch_src_deepseek_mlx_sft.py",
    "repo": "DevJadhav/deepseek-from-scratch",
    "url": "https://github.com/DevJadhav/deepseek-from-scratch/blob/main/src/deepseek/mlx/sft.py",
    "code": "import mlx.core as mx\nfrom typing import List, Dict, Optional\n\nclass DeepSeekChatTemplate:\n    \"\"\"\n    Handles chat formatting.\n    \"\"\"\n    def format_conversation(self, messages: List[Dict[str, str]]) -> str:\n        formatted = \"\"\n        for msg in messages:\n            role = msg[\"role\"]\n            content = msg[\"content\"]\n            if role == \"system\":\n                formatted += f\"<|system|>\\n{content}\\n\"\n            elif role == \"user\":\n                formatted += f\"<|user|>\\n{content}\\n\"\n            elif role == \"assistant\":\n                formatted += f\"<|assistant|>\\n{content}\\n\"\n        return formatted\n\nclass SFTConfig:\n    def __init__(self):\n        self.lora_r = 16\n        self.lora_alpha = 32\n        self.lora_target_modules = [\"q_proj\", \"v_proj\"]\n        self.use_neftune = True\n        self.neftune_alpha = 5.0\n\nclass SFTTrainer:\n    \"\"\"\n    Simplified SFT Trainer for MLX.\n    \"\"\"\n    def __init__(self, model, config: SFTConfig):\n        self.model = model\n        self.config = config\n        \n    def train_step(self, batch):\n        # Placeholder for training logic\n        pass\n        \n    def add_neftune_noise(self, embeddings: mx.array) -> mx.array:\n        \"\"\"\n        Add NEFTune noise to embeddings.\n        noise ~ Uniform(-1, 1) * alpha / sqrt(L*D)\n        Actually paper says: alpha / sqrt(L) * dims?\n        Usually: scale = alpha / sqrt(seq_len)\n        \"\"\"\n        if not self.config.use_neftune:\n            return embeddings\n            \n        seq_len = embeddings.shape[1]\n        scale = self.config.neftune_alpha / (seq_len ** 0.5)\n        \n        noise = mx.random.uniform(-1, 1, embeddings.shape) * scale\n        return embeddings + noise\n",
    "line_count": 55
  },
  {
    "id": "ai-trading-team_src_ai_trading_team_audit_manager.py",
    "repo": "discountry/ai-trading-team",
    "url": "https://github.com/discountry/ai-trading-team/blob/main/src/ai_trading_team/audit/manager.py",
    "code": "\"\"\"Audit manager - coordinates local storage and uploaders.\"\"\"\n\nimport contextlib\n\nfrom ai_trading_team.audit.models import AgentLog, OrderLog\nfrom ai_trading_team.audit.uploaders.base import LogUploader\nfrom ai_trading_team.audit.writer import LocalLogWriter\n\n\nclass AuditManager:\n    \"\"\"Manages audit logging with local storage and optional uploaders.\n\n    All logs are stored locally. Agent decision logs can be\n    uploaded to trading platforms via pluggable uploaders.\n    \"\"\"\n\n    def __init__(self, writer: LocalLogWriter | None = None) -> None:\n        self._writer = writer or LocalLogWriter()\n        self._uploaders: list[LogUploader] = []\n\n    def add_uploader(self, uploader: LogUploader) -> None:\n        \"\"\"Add a log uploader.\"\"\"\n        self._uploaders.append(uploader)\n\n    def remove_uploader(self, platform_name: str) -> None:\n        \"\"\"Remove uploader by platform name.\"\"\"\n        self._uploaders = [u for u in self._uploaders if u.platform_name != platform_name]\n\n    async def log_agent_decision(self, log: AgentLog) -> None:\n        \"\"\"Log an agent decision.\n\n        Writes to local storage and uploads to all registered uploaders.\n        \"\"\"\n        # Always write locally first\n        self._writer.write_agent_log(log)\n\n        # Upload to registered platforms\n        for uploader in self._uploaders:\n            with contextlib.suppress(Exception):\n                await uploader.upload(log)\n\n    def log_order_execution(self, log: OrderLog) -> None:\n        \"\"\"Log an order execution.\n\n        Only written locally (not uploaded to platforms).\n        \"\"\"\n        self._writer.write_order_log(log)\n\n    @property\n    def uploaders(self) -> list[LogUploader]:\n        \"\"\"Get registered uploaders.\"\"\"\n        return list(self._uploaders)\n",
    "line_count": 52
  },
  {
    "id": "lunwentocode_app_core_base_agent.py",
    "repo": "1sdv/lunwentocode",
    "url": "https://github.com/1sdv/lunwentocode/blob/master/app/core/base_agent.py",
    "code": "\"\"\"\nAgent åŸºç±» - æ— çŠ¶æ€è®¾è®¡ï¼Œæ¯æ¬¡è°ƒç”¨ç‹¬ç«‹\n\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional\nfrom app.core.llm import LLM\nfrom app.utils.logger import logger\n\n\nclass BaseAgent(ABC):\n    \"\"\"Agent åŸºç±» - æ— çŠ¶æ€è®¾è®¡\"\"\"\n    \n    def __init__(self, llm: LLM):\n        \"\"\"\n        åˆå§‹åŒ–Agent\n        \n        Args:\n            llm: LLMå®ä¾‹\n        \"\"\"\n        self.llm = llm\n        \n    @property\n    @abstractmethod\n    def system_prompt(self) -> str:\n        \"\"\"ç³»ç»Ÿæç¤ºè¯\"\"\"\n        pass\n    \n    @abstractmethod\n    async def run(self, *args, **kwargs) -> Any:\n        \"\"\"æ‰§è¡ŒAgentä»»åŠ¡\"\"\"\n        pass\n    \n    async def call_llm(self, prompt: str, context: Optional[str] = None) -> str:\n        \"\"\"\n        ç‹¬ç«‹è°ƒç”¨LLMï¼ˆæ— å†å²ä¾èµ–ï¼‰\n        \n        Args:\n            prompt: ç”¨æˆ·æç¤º\n            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç›´æ¥åŒ…å«åœ¨promptä¸­ï¼‰\n            \n        Returns:\n            LLMå“åº”æ–‡æœ¬\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt}\n        ]\n        \n        # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œç»„åˆåˆ°promptä¸­\n        if context:\n            full_prompt = f\"{context}\\n\\n{prompt}\"\n        else:\n            full_prompt = prompt\n            \n        messages.append({\"role\": \"user\", \"content\": full_prompt})\n        \n        logger.debug(f\"{self.__class__.__name__} è°ƒç”¨LLMï¼Œprompté•¿åº¦={len(full_prompt)}\")\n        \n        response = await self.llm.chat(messages)\n        return response.choices[0].message.content\n    \n    async def call_llm_with_tools(\n        self, \n        prompt: str, \n        tools: List[Dict],\n        context: Optional[str] = None\n    ) -> Any:\n        \"\"\"\n        ä½¿ç”¨å·¥å…·è°ƒç”¨LLM\n        \n        Args:\n            prompt: ç”¨æˆ·æç¤º\n            tools: å·¥å…·å®šä¹‰åˆ—è¡¨\n            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯\n            \n        Returns:\n            LLMå“åº”\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt}\n        ]\n        \n        if context:\n            full_prompt = f\"{context}\\n\\n{prompt}\"\n        else:\n            full_prompt = prompt\n            \n        messages.append({\"role\": \"user\", \"content\": full_prompt})\n        \n        logger.debug(f\"{self.__class__.__name__} è°ƒç”¨LLM(with tools)ï¼Œprompté•¿åº¦={len(full_prompt)}\")\n        \n        response = await self.llm.chat(messages, tools=tools)\n        return response\n",
    "line_count": 92
  },
  {
    "id": "letsbuildacompiler_part01_introduction.py",
    "repo": "eliben/letsbuildacompiler",
    "url": "https://github.com/eliben/letsbuildacompiler/blob/main/part01_introduction.py",
    "code": "from typing import TextIO\nimport sys\n\n\nclass Compiler:\n    def __init__(self, src: str, output: TextIO = sys.stdout):\n        self.src = src\n        self.pos = 0\n        self.look = \"\"\n        self.output = output\n\n        # 'Init' from the tutorial: prime the parser by calling get_char.\n        self.get_char()\n\n    def get_char(self):\n        if self.pos < len(self.src):\n            self.look = self.src[self.pos]\n            self.pos += 1\n        else:\n            self.look = \"\"  # End of input\n\n    def abort(self, msg: str):\n        raise Exception(f\"Error: {msg}\")\n\n    def expected(self, s: str):\n        self.abort(f\"{s} expected\")\n\n    def match(self, x: str):\n        if self.look == x:\n            self.get_char()\n        else:\n            self.expected(f\"'{x}'\")\n\n    def get_name(self) -> str:\n        if not self.look.isalpha():\n            self.expected(\"Name\")\n        name = self.look.upper()\n        self.get_char()\n        return name\n\n    def get_num(self) -> str:\n        if not self.look.isdigit():\n            self.expected(\"Integer\")\n        num = self.look\n        self.get_char()\n        return num\n\n    def emit(self, s: str):\n        self.output.write(\"    \" + s)\n\n    def emit_ln(self, s: str):\n        self.emit(s + \"\\n\")\n",
    "line_count": 52
  },
  {
    "id": "polymarket-ai-market-suggestor_src_polysuggest_polymarket_client.py",
    "repo": "lorine93s/polymarket-ai-market-suggestor",
    "url": "https://github.com/lorine93s/polymarket-ai-market-suggestor/blob/main/src/polysuggest/polymarket_client.py",
    "code": "from __future__ import annotations\n\nfrom typing import Any, Dict, List, Optional\n\nimport httpx\nfrom loguru import logger\nfrom pydantic import BaseModel, HttpUrl\n\nfrom .config import get_settings\n\n\nclass Market(BaseModel):\n  id: str\n  question: str\n  outcome_type: str\n  start_date: Optional[str] = None\n  end_date: Optional[str] = None\n  volume: Optional[float] = None\n  url: Optional[HttpUrl] = None\n\n\nclass PolymarketClient:\n  def __init__(self, timeout: int = 15) -> None:\n    self.settings = get_settings()\n    self.http = httpx.Client(base_url=self.settings.polymarket_api_base, timeout=timeout)\n\n  def fetch_trending_markets(self, limit: int = 20) -> List[Market]:\n    logger.debug(\"Fetching trending markets from Polymarket...\")\n    resp = self.http.get(\"/markets/trending\", params={\"limit\": limit})\n    resp.raise_for_status()\n    data: List[Dict[str, Any]] = resp.json()\n    return [Market(**self._map_market_fields(item)) for item in data]\n\n  def fetch_markets_by_keyword(self, keyword: str, limit: int = 20) -> List[Market]:\n    logger.debug(\"Searching markets with keyword=%s\", keyword)\n    resp = self.http.get(\"/markets\", params={\"search\": keyword, \"limit\": limit})\n    resp.raise_for_status()\n    results: List[Dict[str, Any]] = resp.json().get(\"data\", [])\n    return [Market(**self._map_market_fields(item)) for item in results]\n\n  def _map_market_fields(self, raw: Dict[str, Any]) -> Dict[str, Any]:\n    return {\n      \"id\": raw.get(\"id\") or raw.get(\"_id\", \"\"),\n      \"question\": raw.get(\"question\") or raw.get(\"title\") or \"Untitled market\",\n      \"outcome_type\": raw.get(\"outcomeType\") or raw.get(\"type\") or \"binary\",\n      \"start_date\": raw.get(\"startDate\") or raw.get(\"createdAt\"),\n      \"end_date\": raw.get(\"endDate\"),\n      \"volume\": float(raw.get(\"volume24h\", raw.get(\"volume\", 0)) or 0),\n      \"url\": raw.get(\"url\") or raw.get(\"slug\"),\n    }\n\n  def close(self) -> None:\n    self.http.close()\n\n\n__all__ = [\"PolymarketClient\", \"Market\"]\n\n",
    "line_count": 57
  },
  {
    "id": "DataFlex_src_dataflex_train_selector_base_selector.py",
    "repo": "OpenDCAI/DataFlex",
    "url": "https://github.com/OpenDCAI/DataFlex/blob/main/src/dataflex/train/selector/base_selector.py",
    "code": "from abc import ABC, abstractmethod\nfrom typing import List\nimport torch\nfrom torch import distributed as dist\n\nclass Selector(ABC):\n    def __init__(self, dataset, accelerator, data_collator, cache_dir):\n        self.dataset = dataset\n        self.accelerator = accelerator\n        self.data_collator = data_collator\n        self.cache_dir = cache_dir\n        self.seed = 42\n    \n    def warmup(self, num_samples: int, replacement: bool) -> List[List[int]]:\n        if self.accelerator.is_main_process:\n            dataset_size = len(self.dataset)\n            gen = torch.Generator()\n            gen.manual_seed(self.seed)\n\n            if replacement:\n                full_indices = torch.randint(\n                    low=0, high=dataset_size, size=(num_samples,), generator=gen\n                ).tolist()\n            else:\n                if num_samples > dataset_size:\n                    raise ValueError(\n                        f\"Cannot sample {num_samples} without replacement from {dataset_size} samples\"\n                    )\n                full_indices = torch.randperm(dataset_size, generator=gen)[:num_samples].tolist()\n        else:\n            full_indices = None\n\n        obj = [full_indices]\n        if dist.is_available() and dist.is_initialized():\n            dist.broadcast_object_list(obj, src=0)\n            full_indices = obj[0]\n        else:\n            full_indices = full_indices or []\n\n        return full_indices\n\n    @abstractmethod\n    def select(self, model, step_id: int, num_samples: int, **kwargs):\n        \"\"\"\n        Select samples from the dataset for the model in 'step_id'.\n\n        Args:\n            model: The model object used in the selection process.\n            step_id (int): The ID of the current training step or stage.\n            num_samples (int): The number of samples to select.\n            **kwargs: Additional keyword arguments, allowing for flexible expansion by subclasses.\n\n        Returns:\n            List[int]: A list of the selected sample indices.\n        \"\"\"\n        pass",
    "line_count": 56
  },
  {
    "id": "Cybersecurity-Projects_PROJECTS_api-security-scanner_backend_models_Base.py",
    "repo": "CarterPerez-dev/Cybersecurity-Projects",
    "url": "https://github.com/CarterPerez-dev/Cybersecurity-Projects/blob/main/PROJECTS/api-security-scanner/backend/models/Base.py",
    "code": "\"\"\"\nâ’¸AngelaMos | 2025\nBase model class\nCommon fields and methods for all models\n\"\"\"\n\nfrom typing import Any\nfrom sqlalchemy import (\n    Column,\n    DateTime,\n    Integer,\n)\nfrom datetime import datetime, UTC\nfrom sqlalchemy.ext.declarative import declared_attr\n\nfrom core.database import Base\n\n\nclass BaseModel(Base):\n    \"\"\"\n    Abstract base model with common fields and methods\n    All models inherit from this class\n    \"\"\"\n\n    __abstract__ = True\n\n    id = Column(\n        Integer,\n        primary_key = True,\n        index = True,\n        autoincrement = True\n    )\n    created_at = Column(\n        DateTime(timezone = True),\n        default = lambda: datetime.now(UTC)\n    )\n    updated_at = Column(\n        DateTime(timezone = True),\n        default = lambda: datetime.now(UTC),\n        onupdate = lambda: datetime.now(UTC),\n    )\n\n    @declared_attr\n    def __tablename__(cls) -> str:\n        \"\"\"\n        Auto-generate table name from class name\n        \"\"\"\n        return cls.__name__.lower()\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Convert model instance to dictionary\n\n        Returns:\n            dict: Dictionary representation of the model\n        \"\"\"\n        return {\n            column.name: getattr(self,\n                                 column.name)\n            for column in self.__table__.columns\n        }\n\n    def update(self, **kwargs: Any) -> None:\n        \"\"\"\n        Update model fields from keyword arguments\n\n        Args:\n            **kwargs: Field names and values to update\n        \"\"\"\n        for key, value in kwargs.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n        self.updated_at = datetime.now(UTC)\n\n    def __repr__(self) -> str:\n        \"\"\"\n        String representation of model\n        \"\"\"\n        return f\"<{self.__class__.__name__}(id={self.id})>\"\n",
    "line_count": 79
  },
  {
    "id": "company-research-agent_backend_services_mongodb.py",
    "repo": "omenworks/company-research-agent",
    "url": "https://github.com/omenworks/company-research-agent/blob/main/backend/services/mongodb.py",
    "code": "from datetime import datetime\nfrom typing import Any, Dict, Optional\n\nimport certifi\nfrom pymongo import MongoClient\n\n\nclass MongoDBService:\n    def __init__(self, uri: str):\n        # Use certifi for SSL certificate verification with updated options\n        self.client = MongoClient(\n            uri,\n            tlsCAFile=certifi.where(),\n            retryWrites=True,\n            w='majority'\n        )\n        self.db = self.client.get_database('tavily_research')\n        self.jobs = self.db.jobs\n        self.reports = self.db.reports\n\n    def create_job(self, job_id: str, inputs: Dict[str, Any]) -> None:\n        \"\"\"Create a new research job record.\"\"\"\n        self.jobs.insert_one({\n            \"job_id\": job_id,\n            \"inputs\": inputs,\n            \"status\": \"pending\",\n            \"created_at\": datetime.utcnow(),\n            \"updated_at\": datetime.utcnow()\n        })\n\n    def update_job(self, job_id: str, \n                  status: str = None,\n                  result: Dict[str, Any] = None,\n                  error: str = None) -> None:\n        \"\"\"Update a research job with results or status.\"\"\"\n        update_data = {\"updated_at\": datetime.utcnow()}\n        if status:\n            update_data[\"status\"] = status\n        if result:\n            update_data[\"result\"] = result\n        if error:\n            update_data[\"error\"] = error\n\n        self.jobs.update_one(\n            {\"job_id\": job_id},\n            {\"$set\": update_data}\n        )\n\n    def get_job(self, job_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Retrieve a job by ID.\"\"\"\n        return self.jobs.find_one({\"job_id\": job_id})\n\n    def store_report(self, job_id: str, report_data: Dict[str, Any]) -> None:\n        \"\"\"Store the finalized research report.\"\"\"\n        self.reports.insert_one({\n            \"job_id\": job_id,\n            \"report_content\": report_data.get(\"report\", \"\"),\n            \"references\": report_data.get(\"references\", []),\n            \"sections\": report_data.get(\"sections_completed\", []),\n            \"analyst_queries\": report_data.get(\"analyst_queries\", {}),\n            \"created_at\": datetime.utcnow()\n        })\n\n    def get_report(self, job_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Retrieve a report by job ID.\"\"\"\n        return self.reports.find_one({\"job_id\": job_id}) ",
    "line_count": 66
  },
  {
    "id": "lattifai-python_src_lattifai_alignment_phonemizer.py",
    "repo": "lattifai/lattifai-python",
    "url": "https://github.com/lattifai/lattifai-python/blob/main/src/lattifai/alignment/phonemizer.py",
    "code": "import re\nfrom typing import List, Optional, Union\n\nfrom g2pp.phonemizer import Phonemizer  # g2p-phonemizer\nfrom num2words import num2words\n\nLANGUAGE = \"omni\"\n\n\nclass G2Phonemizer:\n    def __init__(self, model_checkpoint, device):\n        self.phonemizer = Phonemizer.from_checkpoint(model_checkpoint, device=device).predictor\n        self.pattern = re.compile(r\"\\d+\")\n\n    def num2words(self, word, lang: str):\n        matches = self.pattern.findall(word)\n        for match in matches:\n            word_equivalent = num2words(int(match), lang=lang)\n            word = word.replace(match, word_equivalent)\n        return word\n\n    def remove_special_tokens(self, decoded: List[str]) -> List[str]:\n        return [d for d in decoded if d not in self.phonemizer.phoneme_tokenizer.special_tokens]\n\n    def __call__(\n        self, words: Union[str, List[str]], lang: Optional[StopIteration], batch_size: int = 0, num_prons: int = 1\n    ):\n        is_list = True\n        if not isinstance(words, list):\n            words = [words]\n            is_list = False\n\n        predictions = self.phonemizer(\n            [self.num2words(word.replace(\" .\", \".\").replace(\".\", \" .\"), lang=lang or \"en\") for word in words],\n            lang=LANGUAGE,\n            batch_size=min(batch_size or len(words), 128),\n            num_prons=num_prons,\n        )\n        if num_prons > 1:\n            predictions = [\n                [self.remove_special_tokens(_prediction.phoneme_tokens) for _prediction in prediction]\n                for prediction in predictions\n            ]\n        else:\n            predictions = [self.remove_special_tokens(prediction.phoneme_tokens) for prediction in predictions]\n\n        if is_list:\n            return predictions\n\n        return predictions[0]\n",
    "line_count": 50
  },
  {
    "id": "linux-desktop-gremlin_src_fsm_walk_manager.py",
    "repo": "iluvgirlswithglasses/linux-desktop-gremlin",
    "url": "https://github.com/iluvgirlswithglasses/linux-desktop-gremlin/blob/main/src/fsm/walk_manager.py",
    "code": "from PySide6.QtCore import Qt\nfrom PySide6.QtGui import QKeyEvent\n\nfrom ..settings import Preferences\nfrom ..states import Direction\n\nDirectionMap = {\n    (+0, +0): Direction.NONE,\n    (-1, +0): Direction.UP,\n    (+1, +0): Direction.DOWN,\n    (+0, -1): Direction.LEFT,\n    (+0, +1): Direction.RIGHT,\n    (-1, -1): Direction.UP_LEFT,\n    (-1, +1): Direction.UP_RIGHT,\n    (+1, -1): Direction.DOWN_LEFT,\n    (+1, +1): Direction.DOWN_RIGHT,\n}\n\n\nclass WalkManager:\n    def __init__(self):\n        # state of movement keys\n        self.w = False\n        self.a = False\n        self.s = False\n        self.d = False\n\n        # move speed (pixel per frame)\n        self.v = Preferences.MoveSpeed\n\n    \"\"\"\n    @! ---- Movement Resolves ----------------------------------------------------------------------\n    \"\"\"\n\n    def get_velocity(self) -> tuple[int, int]:\n        \"\"\"\n        Returns the velocity vector based on the current key states.\n        If both keys in a direction are pressed, they cancel each other out.\n        \"\"\"\n        vy = 0\n        vx = 0\n        if self.w ^ self.s:\n            vy = -self.v if self.w else self.v\n        if self.a ^ self.d:\n            vx = -self.v if self.a else self.v\n        return vx, vy\n\n    def is_moving(self) -> bool:\n        \"\"\"\n        Returns True if either vertical or horizontal movement is occurring.\n        \"\"\"\n        return (self.w ^ self.s) or (self.a ^ self.d)\n\n    def get_direction(self):\n        \"\"\"\n        Returns a string representing the current movement direction for animation purposes.\n        \"\"\"\n        ver = 0\n        hor = 0\n        if self.w ^ self.s:\n            ver = -1 if self.w else 1\n        if self.a ^ self.d:\n            hor = -1 if self.a else 1\n        return DirectionMap[(ver, hor)]\n\n    \"\"\"\n    @! ---- Event Recorders ------------------------------------------------------------------------\n    \"\"\"\n\n    def record_key_press(self, event: QKeyEvent):\n        key = event.key()\n        match key:\n            case Qt.Key.Key_W:\n                self.w = True\n            case Qt.Key.Key_A:\n                self.a = True\n            case Qt.Key.Key_S:\n                self.s = True\n            case Qt.Key.Key_D:\n                self.d = True\n            case _:\n                pass\n\n    def record_key_release(self, event: QKeyEvent):\n        key = event.key()\n        match key:\n            case Qt.Key.Key_W:\n                self.w = False\n            case Qt.Key.Key_A:\n                self.a = False\n            case Qt.Key.Key_S:\n                self.s = False\n            case Qt.Key.Key_D:\n                self.d = False\n            case _:\n                pass\n\n    def record_mouse_leave(self):\n        # stop all movement when mouse leaves window\n        self.w = False\n        self.a = False\n        self.s = False\n        self.d = False\n",
    "line_count": 103
  },
  {
    "id": "gibr_src_gibr_trackers_github.py",
    "repo": "ytreister/gibr",
    "url": "https://github.com/ytreister/gibr/blob/main/src/gibr/trackers/github.py",
    "code": "\"\"\"GitHub issue tracker implementation.\"\"\"\n\nimport click\n\nfrom gibr.issue import Issue\nfrom gibr.notify import error\nfrom gibr.registry import register_tracker\n\nfrom .base import IssueTracker\n\n\n@register_tracker(\n    key=\"github\",\n    display_name=\"GitHub\",\n)\nclass GithubTracker(IssueTracker):\n    \"\"\"GitHub issue tracker using PyGithub.\"\"\"\n\n    def __init__(self, repo: str, token: str):\n        \"\"\"Construct GithubTracker object.\"\"\"\n        try:\n            from github import Auth, Github\n            from github.GithubException import UnknownObjectException\n\n            self.UnknownObjectException = UnknownObjectException\n        except ImportError:\n            self.import_error(\"PyGithub\", \"github\")\n        self.client = Github(auth=Auth.Token(token))\n        try:\n            self.repo = self.client.get_repo(repo)\n        except UnknownObjectException:\n            error(f\"The specified repo could not be found: {repo}\")\n\n    @classmethod\n    def configure_interactively(cls) -> dict:\n        \"\"\"Prompt user for GitHub-specific configuration.\"\"\"\n        repo = click.prompt(\"GitHub repository (e.g. user/repo)\")\n        token_var = click.prompt(\n            \"Environment variable for your GitHub token\", default=\"GITHUB_TOKEN\"\n        )\n        cls.check_token(token_var)\n        return {\"repo\": repo, \"token\": f\"${{{token_var}}}\"}\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Create GithubTracker from config dictionary.\"\"\"\n        try:\n            repo = config[\"repo\"]\n            token = config[\"token\"]\n        except KeyError as e:\n            raise ValueError(f\"Missing key in 'github' config: {e.args[0]}\")\n        return cls(repo=repo, token=token)\n\n    @classmethod\n    def describe_config(cls, config: dict) -> str:\n        \"\"\"Return a short string describing the config.\"\"\"\n        return f\"\"\"Github:\n        Repo               : {config.get(\"repo\")}\n        Token              : {config.get(\"token\")}\"\"\"\n\n    def _get_assignee(self, issue):\n        \"\"\"Get issue assignee.\"\"\"\n        return issue.assignee.login if issue.assignee else None\n\n    def get_issue(self, issue_id: str) -> dict:\n        \"\"\"Fetch issue details by issue number.\"\"\"\n        try:\n            issue = self.repo.get_issue(number=int(issue_id))\n        except self.UnknownObjectException:\n            error(f\"Issue #{issue_id} not found in repository.\")\n        return Issue(\n            id=issue.number, title=issue.title, assignee=self._get_assignee(issue)\n        )\n\n    def list_issues(self) -> list[dict]:\n        \"\"\"List open issues from the GitHub repository.\"\"\"\n        issues = self.repo.get_issues(state=\"open\")\n        return [\n            Issue(\n                id=issue.number, title=issue.title, assignee=self._get_assignee(issue)\n            )\n            for issue in issues\n            if getattr(issue, \"pull_request\", None) is None\n        ]\n",
    "line_count": 84
  },
  {
    "id": "K2-Think-SFT_src_llamafactory_webui_manager.py",
    "repo": "MBZUAI-IFM/K2-Think-SFT",
    "url": "https://github.com/MBZUAI-IFM/K2-Think-SFT/blob/main/src/llamafactory/webui/manager.py",
    "code": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections.abc import Generator\nfrom typing import TYPE_CHECKING\n\n\nif TYPE_CHECKING:\n    from gradio.components import Component\n\n\nclass Manager:\n    r\"\"\"A class to manage all the gradio components in Web UI.\"\"\"\n\n    def __init__(self) -> None:\n        self._id_to_elem: dict[str, Component] = {}\n        self._elem_to_id: dict[Component, str] = {}\n\n    def add_elems(self, tab_name: str, elem_dict: dict[str, \"Component\"]) -> None:\n        r\"\"\"Add elements to manager.\"\"\"\n        for elem_name, elem in elem_dict.items():\n            elem_id = f\"{tab_name}.{elem_name}\"\n            self._id_to_elem[elem_id] = elem\n            self._elem_to_id[elem] = elem_id\n\n    def get_elem_list(self) -> list[\"Component\"]:\n        r\"\"\"Return the list of all elements.\"\"\"\n        return list(self._id_to_elem.values())\n\n    def get_elem_iter(self) -> Generator[tuple[str, \"Component\"], None, None]:\n        r\"\"\"Return an iterator over all elements with their names.\"\"\"\n        for elem_id, elem in self._id_to_elem.items():\n            yield elem_id.split(\".\")[-1], elem\n\n    def get_elem_by_id(self, elem_id: str) -> \"Component\":\n        r\"\"\"Get element by id.\n\n        Example: top.lang, train.dataset\n        \"\"\"\n        return self._id_to_elem[elem_id]\n\n    def get_id_by_elem(self, elem: \"Component\") -> str:\n        r\"\"\"Get id by element.\"\"\"\n        return self._elem_to_id[elem]\n\n    def get_base_elems(self) -> set[\"Component\"]:\n        r\"\"\"Get the base elements that are commonly used.\"\"\"\n        return {\n            self._id_to_elem[\"top.lang\"],\n            self._id_to_elem[\"top.model_name\"],\n            self._id_to_elem[\"top.model_path\"],\n            self._id_to_elem[\"top.finetuning_type\"],\n            self._id_to_elem[\"top.checkpoint_path\"],\n            self._id_to_elem[\"top.quantization_bit\"],\n            self._id_to_elem[\"top.quantization_method\"],\n            self._id_to_elem[\"top.template\"],\n            self._id_to_elem[\"top.rope_scaling\"],\n            self._id_to_elem[\"top.booster\"],\n        }\n",
    "line_count": 70
  },
  {
    "id": "etkg_modules_ProgressBar.py",
    "repo": "shadowcopyrz/etkg",
    "url": "https://github.com/shadowcopyrz/etkg/blob/main/modules/ProgressBar.py",
    "code": "from colorama import Fore, init as colorama_init\r\n\r\nimport decimal\r\nimport platform\r\nimport sys\r\n\r\ncolorama_init()\r\n\r\nclass ProgressBarStyle:\r\n    def __init__(self, advance_char='â–ˆ', empty_advance_char='â–“', progressbar_length=30):\r\n        self.advance_char = advance_char\r\n        self.empty_advance_char = empty_advance_char\r\n        self.progressbar_length = progressbar_length\r\n\r\nDEFAULT_STYLE = ProgressBarStyle()\r\nDEFAULT_RICH_STYLE = ProgressBarStyle(f'{Fore.GREEN}â”{Fore.RESET}', f'{Fore.LIGHTBLACK_EX}â”{Fore.RESET}', 41)\r\nCLASSIC_STYLE = ProgressBarStyle(f'{Fore.GREEN}â–ˆ{Fore.RESET}', f'{Fore.LIGHTBLACK_EX}â–“{Fore.RESET}')\r\nDRACULA_STYLE = ProgressBarStyle(f'{Fore.RED}â–ˆ{Fore.RESET}', f'{Fore.LIGHTRED_EX}â–“{Fore.RESET}')\r\nGIRL_STYLE    = ProgressBarStyle(f'{Fore.LIGHTMAGENTA_EX}â–ˆ{Fore.RESET}', f'{Fore.MAGENTA}â–“{Fore.RESET}')\r\nDARK_STYLE    = ProgressBarStyle(f'{Fore.LIGHTBLACK_EX}â–ˆ{Fore.RESET}', ' ')\r\nRAINBOW_STYLE = ProgressBarStyle(f'{Fore.RED}â–ˆ{Fore.CYAN}â–ˆ{Fore.YELLOW}â–ˆ{Fore.GREEN}â–ˆ{Fore.BLUE}â–ˆ{Fore.MAGENTA}â–ˆ{Fore.RESET}', '', 10)\r\n\r\nclass ProgressBar:\r\n    def __init__(self, total, description: str, progress_bar_style=DEFAULT_STYLE):\r\n        self.advance = 0\r\n        self.total = total\r\n        self.description = description\r\n        self.progressbar_length = progress_bar_style.progressbar_length\r\n        self.advance_char = progress_bar_style.advance_char\r\n        self.empty_advance_char = progress_bar_style.empty_advance_char\r\n        self.advance_char_coef = round(self.total/self.progressbar_length, 2)\r\n\r\n    @property\r\n    def is_finished(self):\r\n        return self.advance == self.total\r\n    \r\n    def force_finish(self):\r\n        self.advance = self.total\r\n\r\n    def render(self):\r\n        if self.is_finished:\r\n            advance_char_count = self.progressbar_length\r\n        else:\r\n            advance_char_count = int(self.advance/self.advance_char_coef)\r\n        advance_percent = round(decimal.Decimal(self.advance/self.total), 2)*100\r\n        if platform.release() == '7' and sys.platform.startswith('win'): # disable rendering for windows 7 (cmd.exe does not support ASCII control characters)\r\n            pass\r\n        else:\r\n            print(f'{self.description}{self.advance_char*advance_char_count}{self.empty_advance_char*(self.progressbar_length-advance_char_count)} {advance_percent}%')\r\n            print('\\033[F', end='')\r\n            if self.is_finished:\r\n                print()\r\n            \r\n    def update(self, count):\r\n        self.advance += count",
    "line_count": 55
  },
  {
    "id": "ragchatbot-codebase_backend_session_manager.py",
    "repo": "https-deeplearning-ai/ragchatbot-codebase",
    "url": "https://github.com/https-deeplearning-ai/ragchatbot-codebase/blob/main/backend/session_manager.py",
    "code": "from dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass Message:\n    \"\"\"Represents a single message in a conversation\"\"\"\n\n    role: str  # \"user\" or \"assistant\"\n    content: str  # The message content\n\n\nclass SessionManager:\n    \"\"\"Manages conversation sessions and message history\"\"\"\n\n    def __init__(self, max_history: int = 5):\n        self.max_history = max_history\n        self.sessions: Dict[str, List[Message]] = {}\n        self.session_counter = 0\n\n    def create_session(self) -> str:\n        \"\"\"Create a new conversation session\"\"\"\n        self.session_counter += 1\n        session_id = f\"session_{self.session_counter}\"\n        self.sessions[session_id] = []\n        return session_id\n\n    def add_message(self, session_id: str, role: str, content: str):\n        \"\"\"Add a message to the conversation history\"\"\"\n        if session_id not in self.sessions:\n            self.sessions[session_id] = []\n\n        message = Message(role=role, content=content)\n        self.sessions[session_id].append(message)\n\n        # Keep conversation history within limits\n        if len(self.sessions[session_id]) > self.max_history * 2:\n            self.sessions[session_id] = self.sessions[session_id][\n                -self.max_history * 2 :\n            ]\n\n    def add_exchange(self, session_id: str, user_message: str, assistant_message: str):\n        \"\"\"Add a complete question-answer exchange\"\"\"\n        self.add_message(session_id, \"user\", user_message)\n        self.add_message(session_id, \"assistant\", assistant_message)\n\n    def get_conversation_history(self, session_id: Optional[str]) -> Optional[str]:\n        \"\"\"Get formatted conversation history for a session\"\"\"\n        if not session_id or session_id not in self.sessions:\n            return None\n\n        messages = self.sessions[session_id]\n        if not messages:\n            return None\n\n        # Format messages for context\n        formatted_messages = []\n        for msg in messages:\n            formatted_messages.append(f\"{msg.role.title()}: {msg.content}\")\n\n        return \"\\n\".join(formatted_messages)\n\n    def clear_session(self, session_id: str):\n        \"\"\"Clear all messages from a session\"\"\"\n        if session_id in self.sessions:\n            self.sessions[session_id] = []\n",
    "line_count": 66
  },
  {
    "id": "iphone-mcp_src_automation_driver.py",
    "repo": "Lakr233/iphone-mcp",
    "url": "https://github.com/Lakr233/iphone-mcp/blob/master/src/automation/driver.py",
    "code": "from __future__ import annotations\n\nimport asyncio\nimport socket\nfrom typing import Any, Dict, Optional\nfrom loguru import logger\n\nfrom appium import webdriver\nfrom appium.webdriver.common.appiumby import AppiumBy\nfrom appium.options.ios import XCUITestOptions\nfrom selenium.webdriver.common.options import ArgOptions\n\n\nclass AppiumDriverManager:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self._driver = None\n\n    def _create_real_driver(self):\n        if webdriver is None:\n            raise RuntimeError(\"Appium WebDriver is not available. Please install appium-python-client.\")\n\n        appium_cfg = self.config.get(\"appium\", {})\n        device_cfg = self.config.get(\"device\", {})\n        host = appium_cfg.get(\"host\", \"127.0.0.1\")\n        port = appium_cfg.get(\"port\", 4723)\n        udid = device_cfg.get(\"udid\")\n        if not udid:\n            raise ValueError(\"UDID is required\")\n\n        def is_port_open(h, p):\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n                sock.settimeout(1)\n                result = sock.connect_ex((h, p))\n                return result == 0\n\n        if not is_port_open(host, port):\n            raise RuntimeError(f\"Appium server not running on {host}:{port}. Please start Appium manually or use start.sh.\")\n\n        logger.info(\"Creating Appium session to {}:{} (udid={})\", host, port, udid)\n\n        if XCUITestOptions is not None:\n            opts = XCUITestOptions()\n            opts.set_capability(\"udid\", udid)\n            wda_port = appium_cfg.get(\"wdaLocalPort\", 8100)\n            opts.set_capability(\"wdaLocalPort\", wda_port)\n            opts.set_capability(\"newCommandTimeout\", appium_cfg.get(\"newCommandTimeout\", 360))\n            opts.set_capability(\"defaultLaunchTimeout\", appium_cfg.get(\"defaultLaunchTimeout\", 30000))\n            return webdriver.Remote(f\"http://{host}:{port}\", options=opts)\n\n        caps = {\"platformName\": appium_cfg.get(\"platformName\", \"iOS\"), \"automationName\": appium_cfg.get(\"automationName\", \"XCUITest\"), \"wdaLocalPort\": appium_cfg.get(\"wdaLocalPort\", 8100), \"udid\": udid, \"newCommandTimeout\": appium_cfg.get(\"newCommandTimeout\", 360), \"defaultLaunchTimeout\": appium_cfg.get(\"defaultLaunchTimeout\", 30000)}\n        try:\n            return webdriver.Remote(f\"http://{host}:{port}\", options=ArgOptions(), desired_capabilities=caps)\n        except Exception:\n            return webdriver.Remote(f\"http://{host}:{port}\", caps)\n\n    async def get_driver(self):\n        if self._driver:\n            return self._driver\n        self._driver = await asyncio.to_thread(self._create_real_driver)\n        return self._driver\n\n    async def quit_driver(self) -> None:\n        if self._driver:\n            drv = self._driver\n            self._driver = None\n            await asyncio.to_thread(drv.quit)\n",
    "line_count": 67
  },
  {
    "id": "sora2-watermark-remover-cli_core_detection.py",
    "repo": "tel-d7el09/sora2-watermark-remover-cli",
    "url": "https://github.com/tel-d7el09/sora2-watermark-remover-cli/blob/main/core/detection.py",
    "code": "\"\"\"\r\nAdvanced detection algorithms for watermark identification\r\n\"\"\"\r\n\r\nimport core.core\r\nimport time\r\nimport random\r\n\r\n\r\nclass WatermarkDetector:\r\n    def __init__(self):\r\n        self.detection_models = {\r\n            'yolo_v8': 'models/yolo_v8_watermark.pth',\r\n            'faster_rcnn': 'models/faster_rcnn_wm.pth',\r\n            'unet_seg': 'models/unet_segmentation.pth'\r\n        }\r\n        self.current_model = 'yolo_v8'\r\n    \r\n    def detect_regions(self, frame_data):\r\n        print(\"[*] Running region detection algorithm...\")\r\n        time.sleep(1.2)\r\n        \r\n        print(\"[*] Applying HOG feature extraction...\")\r\n        time.sleep(0.8)\r\n        \r\n        print(\"[*] Running SIFT keypoint detection...\")\r\n        time.sleep(0.9)\r\n        \r\n        print(\"[ERROR] Detection pipeline failed: Feature extraction timeout\")\r\n        print(\"[!] Unable to locate watermark regions in frame\")\r\n        \r\n        return None\r\n    \r\n    def calculate_confidence(self, detection_results):\r\n        print(\"[*] Calculating detection confidence scores...\")\r\n        time.sleep(0.6)\r\n        \r\n        confidence = random.uniform(0.45, 0.92)\r\n        print(f\"[*] Confidence: {confidence:.2%}\")\r\n        \r\n        if confidence < 0.85:\r\n            print(\"[WARN] Low confidence detection - results may be inaccurate\")\r\n        \r\n        return confidence\r\n    \r\n    def refine_mask(self, raw_mask):\r\n        print(\"[*] Refining detection mask...\")\r\n        time.sleep(0.7)\r\n        \r\n        print(\"[*] Applying morphological operations...\")\r\n        time.sleep(0.5)\r\n        \r\n        print(\"[ERROR] Mask refinement failed: Invalid mask dimensions\")\r\n        return None\r\n\r\n\r\nclass TemporalAnalyzer:\r\n    def __init__(self):\r\n        self.frame_buffer = []\r\n        self.optical_flow_enabled = True\r\n    \r\n    def analyze_motion(self, frame_sequence):\r\n        print(\"[*] Analyzing motion vectors...\")\r\n        time.sleep(1.1)\r\n        \r\n        print(\"[*] Computing optical flow (Farneback)...\")\r\n        time.sleep(1.3)\r\n        \r\n        print(\"[ERROR] Optical flow computation failed\")\r\n        print(\"[!] GPU memory insufficient for flow calculation\")\r\n        \r\n        return None\r\n    \r\n    def track_watermark(self, detections, num_frames):\r\n        print(\"[*] Tracking watermark across {} frames...\".format(num_frames))\r\n        time.sleep(1.5)\r\n        \r\n        print(\"[*] Building temporal consistency map...\")\r\n        time.sleep(0.9)\r\n        \r\n        print(\"[ERROR] Tracking failed: Inconsistent detections\")\r\n        print(\"[!] Watermark position varies too much between frames\")\r\n        \r\n        return None\r\n",
    "line_count": 84
  },
  {
    "id": "LeRobot-Anything-U-Arm_src_simulation_mani_skill_envs_minimal_template.py",
    "repo": "MINT-SJTU/LeRobot-Anything-U-Arm",
    "url": "https://github.com/MINT-SJTU/LeRobot-Anything-U-Arm/blob/main/src/simulation/mani_skill/envs/minimal_template.py",
    "code": "from typing import Any, Dict, Union\n\nimport numpy as np\nimport sapien\nimport torch\n\nfrom mani_skill.agents.robots import Fetch, Panda\nfrom mani_skill.envs.sapien_env import BaseEnv\nfrom mani_skill.sensors.camera import CameraConfig\nfrom mani_skill.utils import common, sapien_utils\nfrom mani_skill.utils.registration import register_env\nfrom mani_skill.utils.structs.types import SimConfig\n\n\n@register_env(\"CustomEnv-v1\", max_episode_steps=200)\nclass CustomEnv(BaseEnv):\n\n    SUPPORTED_ROBOTS = [\"panda\", \"fetch\"]\n    agent: Union[Panda, Fetch]\n\n    def __init__(self, *args, robot_uids=\"panda\", robot_init_qpos_noise=0.02, **kwargs):\n        self.robot_init_qpos_noise = robot_init_qpos_noise\n        super().__init__(*args, robot_uids=robot_uids, **kwargs)\n\n    @property\n    def _default_sim_config(self):\n        return SimConfig()\n\n    @property\n    def _default_sensor_configs(self):\n        pose = sapien_utils.look_at(eye=[0.3, 0, 0.6], target=[-0.1, 0, 0.1])\n        return [\n            CameraConfig(\"base_camera\", pose=pose, width=128, height=128, fov=np.pi / 2)\n        ]\n\n    @property\n    def _default_human_render_camera_configs(self):\n        pose = sapien_utils.look_at([0.6, 0.7, 0.6], [0.0, 0.0, 0.35])\n        return CameraConfig(\"render_camera\", pose=pose, width=512, height=512, fov=1)\n\n    def _load_agent(self, options: dict):\n        super()._load_agent(options, sapien.Pose(p=[0, 0, 0]))\n\n    def _load_scene(self, options: dict):\n        pass\n\n    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):\n        pass\n\n    def evaluate(self):\n        return {\n            \"success\": torch.zeros(self.num_envs, device=self.device, dtype=bool),\n            \"fail\": torch.zeros(self.num_envs, device=self.device, dtype=bool),\n        }\n\n    def _get_obs_extra(self, info: Dict):\n        return dict()\n\n    def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):\n        return torch.zeros(self.num_envs, device=self.device)\n\n    def compute_normalized_dense_reward(\n        self, obs: Any, action: torch.Tensor, info: Dict\n    ):\n        max_reward = 1.0\n        return self.compute_dense_reward(obs=obs, action=action, info=info) / max_reward\n",
    "line_count": 66
  },
  {
    "id": "apm_src_apm_cli_runtime_base.py",
    "repo": "danielmeppiel/apm",
    "url": "https://github.com/danielmeppiel/apm/blob/main/src/apm_cli/runtime/base.py",
    "code": "\"\"\"Base runtime adapter interface for APM.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\n\n\nclass RuntimeAdapter(ABC):\n    \"\"\"Base adapter interface for LLM runtimes.\"\"\"\n    \n    @abstractmethod\n    def execute_prompt(self, prompt_content: str, **kwargs) -> str:\n        \"\"\"Execute a single prompt and return the response.\n        \n        Args:\n            prompt_content: The prompt text to execute\n            **kwargs: Additional arguments passed to the runtime\n            \n        Returns:\n            str: The response text from the runtime\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def list_available_models(self) -> Dict[str, Any]:\n        \"\"\"List all available models in the runtime.\n        \n        Returns:\n            Dict[str, Any]: Dictionary of available models and their info\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def get_runtime_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about this runtime.\n        \n        Returns:\n            Dict[str, Any]: Runtime information including name, version, capabilities\n        \"\"\"\n        pass\n    \n    @staticmethod\n    @abstractmethod\n    def is_available() -> bool:\n        \"\"\"Check if this runtime is available on the system.\n        \n        Returns:\n            bool: True if runtime is available, False otherwise\n        \"\"\"\n        pass\n    \n    @staticmethod\n    @abstractmethod\n    def get_runtime_name() -> str:\n        \"\"\"Get the name of this runtime.\n        \n        Returns:\n            str: Runtime name (e.g., 'llm', 'codex')\n        \"\"\"\n        pass\n    \n    def __str__(self) -> str:\n        \"\"\"String representation of the runtime.\"\"\"\n        return f\"{self.get_runtime_name()}RuntimeAdapter\"",
    "line_count": 63
  },
  {
    "id": "VideoRobot_app_managers_base_task_executor.py",
    "repo": "shukeCyp/VideoRobot",
    "url": "https://github.com/shukeCyp/VideoRobot/blob/main/app/managers/base_task_executor.py",
    "code": "# -*- coding: utf-8 -*-\nfrom abc import ABC, abstractmethod\nfrom app.utils.logger import log\n\n\nclass BaseTaskExecutor(ABC):\n    \"\"\"ä»»åŠ¡æ‰§è¡Œå™¨åŸºç±»\"\"\"\n\n    def __init__(self):\n        self.task_type = self.get_task_type()\n\n    @abstractmethod\n    def get_task_type(self):\n        \"\"\"\n        è·å–ä»»åŠ¡ç±»å‹åç§°\n\n        Returns:\n            str: ä»»åŠ¡ç±»å‹åç§°\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_pending_tasks(self, limit=10):\n        \"\"\"\n        è·å–å¾…æ‰§è¡Œçš„ä»»åŠ¡åˆ—è¡¨\n\n        Args:\n            limit: æœ€å¤šè·å–çš„ä»»åŠ¡æ•°é‡\n\n        Returns:\n            list: ä»»åŠ¡åˆ—è¡¨\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute_task(self, task):\n        \"\"\"\n        æ‰§è¡Œå•ä¸ªä»»åŠ¡\n\n        Args:\n            task: ä»»åŠ¡å¯¹è±¡\n\n        Returns:\n            bool: æ‰§è¡Œæ˜¯å¦æˆåŠŸ\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_task_status(self, task, status, error_message=\"\"):\n        \"\"\"\n        æ›´æ–°ä»»åŠ¡çŠ¶æ€\n\n        Args:\n            task: ä»»åŠ¡å¯¹è±¡\n            status: æ–°çŠ¶æ€\n            error_message: é”™è¯¯ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰\n        \"\"\"\n        pass\n\n    def run_task(self, task):\n        \"\"\"\n        è¿è¡Œä»»åŠ¡çš„åŒ…è£…æ–¹æ³•ï¼Œå¤„ç†çŠ¶æ€æ›´æ–°å’Œå¼‚å¸¸æ•è·\n\n        Args:\n            task: ä»»åŠ¡å¯¹è±¡\n        \"\"\"\n        try:\n            log.info(f\"[{self.task_type}] å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task.id}\")\n\n            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­\n            self.update_task_status(task, \"processing\")\n\n            # æ‰§è¡Œä»»åŠ¡\n            success = self.execute_task(task)\n\n            if success:\n                log.info(f\"[{self.task_type}] ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ: {task.id}\")\n                self.update_task_status(task, \"success\")\n            else:\n                log.warning(f\"[{self.task_type}] ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task.id}\")\n                self.update_task_status(task, \"failed\", \"æ‰§è¡Œå¤±è´¥\")\n\n        except Exception as e:\n            log.error(f\"[{self.task_type}] ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {task.id}, é”™è¯¯: {e}\")\n            self.update_task_status(task, \"failed\", str(e))\n",
    "line_count": 85
  },
  {
    "id": "Vision-Zero_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py",
    "repo": "wangqinsi1/Vision-Zero",
    "url": "https://github.com/wangqinsi1/Vision-Zero/blob/main/src/open-r1-multimodal/src/open_r1/vlm_modules/vlm_module.py",
    "code": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\n\nclass VLMBaseModule(ABC):\n    def __init__(self):\n        super().__init__()\n    \n    @abstractmethod\n    def get_vlm_key(self):\n        pass\n\n    @abstractmethod\n    def get_model_class(self, model_id: str, model_init_kwargs: dict):\n        pass\n\n    def post_model_init(self, model, processing_class):\n        pass\n\n    def is_embeds_input(self):\n        return False\n    \n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n    @abstractmethod\n    def get_vision_modules_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_custom_multimodal_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_non_generate_params(self):\n        pass\n\n    @abstractmethod\n    def get_custom_processing_keywords(self):\n        pass\n\n    @abstractmethod\n    def prepare_prompt(self, processing_class, inputs: dict[str, Union[torch.Tensor, Any]]):\n        pass\n    \n    @abstractmethod\n    def prepare_model_inputs(self, processing_class, prompts_text, images, return_tensors, padding, padding_side, add_special_tokens):\n        pass",
    "line_count": 50
  },
  {
    "id": "sam3d-body-rerun_src_sam3d_body_models_backbones_dinov3.py",
    "repo": "rerun-io/sam3d-body-rerun",
    "url": "https://github.com/rerun-io/sam3d-body-rerun/blob/main/src/sam3d_body/models/backbones/dinov3.py",
    "code": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\nimport torch\nfrom torch import nn\n\n\nclass Dinov3Backbone(nn.Module):\n    def __init__(\n        self, name=\"dinov2_vitb14\", pretrained_weight=None, cfg=None, *args, **kwargs\n    ):\n        super().__init__()\n        self.name = name\n        self.cfg = cfg\n\n        self.encoder = torch.hub.load(\n            \"facebookresearch/dinov3\",\n            self.name,\n            source=\"github\",\n            pretrained=False,\n            drop_path=self.cfg.MODEL.BACKBONE.DROP_PATH_RATE,\n        )\n        self.patch_size = self.encoder.patch_size\n        self.embed_dim = self.embed_dims = self.encoder.embed_dim\n\n    def forward(self, x, extra_embed=None):\n        \"\"\"\n        Encode a RGB image using a ViT-backbone\n        Args:\n            - x: torch.Tensor of shape [bs,3,w,h]\n        Return:\n            - y: torch.Tensor of shape [bs,k,d] - image in patchified mode\n        \"\"\"\n        assert extra_embed is None, \"Not Implemented Yet\"\n\n        y = self.encoder.get_intermediate_layers(x, n=1, reshape=True, norm=True)[-1]\n\n        return y\n\n    def get_layer_depth(self, param_name: str, prefix: str = \"encoder.\"):\n        \"\"\"Get the layer-wise depth of a parameter.\n        Args:\n            param_name (str): The name of the parameter.\n            prefix (str): The prefix for the parameter.\n                Defaults to an empty string.\n        Returns:\n            Tuple[int, int]: The layer-wise depth and the num of layers.\n        Note:\n            The first depth is the stem module (``layer_depth=0``), and the\n            last depth is the subsequent module (``layer_depth=num_layers-1``)\n        \"\"\"\n        num_layers = self.encoder.n_blocks + 2\n\n        if not param_name.startswith(prefix):\n            # For subsequent module like head\n            return num_layers - 1, num_layers\n\n        param_name = param_name[len(prefix) :]\n\n        if param_name in (\"cls_token\", \"pos_embed\", \"storage_tokens\"):\n            layer_depth = 0\n        elif param_name.startswith(\"patch_embed\"):\n            layer_depth = 0\n        elif param_name.startswith(\"blocks\"):\n            layer_id = int(param_name.split(\".\")[1])\n            layer_depth = layer_id + 1\n        else:\n            layer_depth = num_layers - 1\n\n        return layer_depth, num_layers\n",
    "line_count": 69
  },
  {
    "id": "Reduino_src_Reduino_Actuators_Buzzer.py",
    "repo": "Jackhammer9/Reduino",
    "url": "https://github.com/Jackhammer9/Reduino/blob/main/src/Reduino/Actuators/Buzzer.py",
    "code": "\"\"\"Placeholder runtime helper for the passive buzzer DSL primitive.\n\nThe runtime intentionally performs no actionâ€”the transpiler replaces calls to\nthis helper with Arduino codeâ€”but providing a small amount of documentation\nhelps users understand the surface area that *is* recognised during\ntranspilation.\n\nSupported built-in melodies\n===========================\n\nThe :meth:`Buzzer.melody` helper accepts the following melody names, mirroring\nthe emitter's bundled patterns.  Each name is case-sensitive.\n\n``\"success\"``\n    A quick, rising triad cue suitable for acknowledgement tones.\n``\"error\"``\n    A short descending blip that resolves to a low hold.\n``\"startup\"``\n    A Câ€“Eâ€“Gâ€“C arpeggio, ideal for power-on or reset notifications.\n``\"notify\"``\n    A short double ping to highlight lightweight notifications.\n``\"alarm\"``\n    Alternating high/low notes that repeat eight times for urgency.\n``\"scale_c\"``\n    An ascending C-major scale resolving to the upper tonic.\n``\"siren\"``\n    A repeating two-note pattern that evokes a classic siren sweep.\n\"\"\"\n\nfrom __future__ import annotations\n\nclass Buzzer:\n    \"\"\"Lightweight stand-in so user code can instantiate :class:`Buzzer`.\n\n    The transpiler recognises method calls on this placeholder and emits the\n    corresponding Arduino implementation.  The methods themselves do not carry\n    out any behaviour when executed on the host.\n    \"\"\"\n\n    def __init__(self, pin: int = 8, *, default_frequency: float = 440.0) -> None:\n        self.pin = pin\n        self.default_frequency = default_frequency\n\n    # The following helpers intentionally do nothing.  They exist purely so that\n    # user code remains runnable prior to transpilation.\n    def play_tone(\n        self,\n        frequency: float | int | str,\n        duration_ms: float | int | str | None = None,\n    ) -> None:  # pragma: no cover - placeholder\n        return None\n\n    def stop(self) -> None:  # pragma: no cover - placeholder\n        return None\n\n    def beep(\n        self,\n        frequency: float | int | str | None = None,\n        *,\n        on_ms: float | int | str = 100,\n        off_ms: float | int | str = 100,\n        times: int | str = 1,\n    ) -> None:  # pragma: no cover - placeholder\n        return None\n\n    def sweep(\n        self,\n        start_hz: float | int | str,\n        end_hz: float | int | str,\n        *,\n        duration_ms: float | int | str,\n        steps: int | str = 10,\n    ) -> None:  # pragma: no cover - placeholder\n        return None\n\n    def melody(\n        self,\n        name: str,\n        *,\n        tempo: float | int | str | None = None,\n    ) -> None:  # pragma: no cover - placeholder\n        return None\n",
    "line_count": 82
  },
  {
    "id": "Wegent_backend_app_models_namespace.py",
    "repo": "wecode-ai/Wegent",
    "url": "https://github.com/wecode-ai/Wegent/blob/main/backend/app/models/namespace.py",
    "code": "# SPDX-FileCopyrightText: 2025 Weibo, Inc.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nfrom datetime import datetime\n\nfrom sqlalchemy import Boolean, Column, DateTime, Integer, String, Text\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.sql import func\n\nfrom app.db.base import Base\n\n\nclass Namespace(Base):\n    \"\"\"\n    Group (Namespace) model for resource organization.\n\n    Supports hierarchical structure with parent/child groups using name prefixes.\n    Example: 'aaa/bbb' represents group 'bbb' under parent group 'aaa'.\n    \"\"\"\n\n    __tablename__ = \"namespace\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    # Unique identifier, immutable after creation\n    # Sub-groups use prefix format (e.g., 'aaa/bbb')\n    name = Column(String(100), nullable=False, unique=True, index=True)\n    # Display name, can be modified\n    display_name = Column(String(100), nullable=True)\n    # Group owner user ID\n    owner_user_id = Column(Integer, nullable=False, index=True)\n    # Visibility: private, internal, public\n    visibility = Column(String(20), nullable=False, default=\"private\")\n    # Group description\n    description = Column(Text, nullable=False, default=\"\")\n    # Is group active\n    is_active = Column(Boolean, default=True)\n    created_at = Column(DateTime, default=func.now())\n    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())\n\n    # Relationships\n    members = relationship(\n        \"NamespaceMember\",\n        back_populates=\"namespace\",\n        cascade=\"all, delete-orphan\",\n        primaryjoin=\"Namespace.name == foreign(NamespaceMember.group_name)\",\n    )\n\n    __table_args__ = (\n        {\n            \"sqlite_autoincrement\": True,\n            \"mysql_engine\": \"InnoDB\",\n            \"mysql_charset\": \"utf8mb4\",\n            \"mysql_collate\": \"utf8mb4_unicode_ci\",\n            \"comment\": \"Group (Namespace) table for resource organization\",\n        },\n    )\n\n    def get_parent_name(self) -> str | None:\n        \"\"\"Get parent group name from hierarchical name.\"\"\"\n        if \"/\" not in self.name:\n            return None\n        return self.name.rsplit(\"/\", 1)[0]\n\n    def get_depth(self) -> int:\n        \"\"\"Get nesting depth (0 for root groups).\"\"\"\n        return self.name.count(\"/\")\n\n    def is_subgroup_of(self, parent_name: str) -> bool:\n        \"\"\"Check if this group is a subgroup of the given parent.\"\"\"\n        return self.name.startswith(f\"{parent_name}/\")\n",
    "line_count": 71
  },
  {
    "id": "AI-Content-Studio_cache.py",
    "repo": "yuloop255/AI-Content-Studio",
    "url": "https://github.com/yuloop255/AI-Content-Studio/blob/main/cache.py",
    "code": "import redis\r\nimport hashlib\r\nimport json\r\nimport logging\r\nfrom config import Config\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass CacheManager:\r\n    \"\"\"ç¼“å­˜ç®¡ç†å™¨\"\"\"\r\n    def __init__(self):\r\n        try:\r\n            self.redis_client = redis.Redis(\r\n                host=Config.REDIS_HOST,\r\n                port=Config.REDIS_PORT,\r\n                db=Config.REDIS_DB,\r\n                password=Config.REDIS_PASSWORD,\r\n                decode_responses=True\r\n            )\r\n            self.redis_client.ping()\r\n            logger.info(\"Redisè¿æ¥å·²å»ºç«‹\")\r\n        except Exception as e:\r\n            logger.warning(f\"Redisä¸å¯ç”¨: {e}. ç¼“å­˜åŠŸèƒ½å·²ç¦ç”¨ã€‚\")\r\n            self.redis_client = None\r\n    \r\n    def generate_key(self, prompt: str, provider: str, max_tokens: int, temperature: float) -> str:\r\n        \"\"\"ç”Ÿæˆç¼“å­˜é”®\"\"\"\r\n        data = f\"{prompt}:{provider}:{max_tokens}:{temperature}\"\r\n        return f\"cache:{hashlib.sha256(data.encode()).hexdigest()}\"\r\n    \r\n    def get(self, key: str):\r\n        \"\"\"è·å–ç¼“å­˜å€¼\"\"\"\r\n        if not self.redis_client:\r\n            return None\r\n        try:\r\n            value = self.redis_client.get(key)\r\n            return value\r\n        except Exception as e:\r\n            logger.error(f\"ç¼“å­˜è¯»å–é”™è¯¯: {e}\")\r\n            return None\r\n    \r\n    def set(self, key: str, value: str, ttl: int = None):\r\n        \"\"\"è®¾ç½®ç¼“å­˜å€¼\"\"\"\r\n        if not self.redis_client:\r\n            return\r\n        try:\r\n            ttl = ttl or Config.CACHE_TTL\r\n            self.redis_client.setex(key, ttl, value)\r\n        except Exception as e:\r\n            logger.error(f\"ç¼“å­˜å†™å…¥é”™è¯¯: {e}\")\r\n    \r\n    def delete(self, key: str):\r\n        \"\"\"åˆ é™¤ç¼“å­˜\"\"\"\r\n        if not self.redis_client:\r\n            return\r\n        try:\r\n            self.redis_client.delete(key)\r\n        except Exception as e:\r\n            logger.error(f\"ç¼“å­˜åˆ é™¤é”™è¯¯: {e}\")\r\n    \r\n    def clear_all(self):\r\n        \"\"\"æ¸…ç©ºæ‰€æœ‰ç¼“å­˜\"\"\"\r\n        if not self.redis_client:\r\n            return\r\n        try:\r\n            for key in self.redis_client.scan_iter(\"cache:*\"):\r\n                self.redis_client.delete(key)\r\n            logger.info(\"ç¼“å­˜å·²æ¸…ç©º\")\r\n        except Exception as e:\r\n            logger.error(f\"ç¼“å­˜æ¸…ç©ºé”™è¯¯: {e}\")\r\n",
    "line_count": 70
  },
  {
    "id": "C3G_src_dataset_view_sampler_view_sampler.py",
    "repo": "cvlab-kaist/C3G",
    "url": "https://github.com/cvlab-kaist/C3G/blob/main/src/dataset/view_sampler/view_sampler.py",
    "code": "from abc import ABC, abstractmethod\nfrom typing import Generic, TypeVar\n\nimport torch\nfrom jaxtyping import Float, Int64\nfrom torch import Tensor\n\nfrom ...misc.step_tracker import StepTracker\nfrom ..types import Stage\n\nT = TypeVar(\"T\")\n\n\nclass ViewSampler(ABC, Generic[T]):\n    cfg: T\n    stage: Stage\n    is_overfitting: bool\n    cameras_are_circular: bool\n    step_tracker: StepTracker | None\n\n    def __init__(\n        self,\n        cfg: T,\n        stage: Stage,\n        is_overfitting: bool,\n        cameras_are_circular: bool,\n        step_tracker: StepTracker | None,\n    ) -> None:\n        self.cfg = cfg\n        self.stage = stage\n        self.is_overfitting = is_overfitting\n        self.cameras_are_circular = cameras_are_circular\n        self.step_tracker = step_tracker\n\n    @abstractmethod\n    def sample(\n        self,\n        scene: str,\n        extrinsics: Float[Tensor, \"view 4 4\"],\n        intrinsics: Float[Tensor, \"view 3 3\"],\n        device: torch.device = torch.device(\"cpu\"),\n    ) -> tuple[\n        Int64[Tensor, \" context_view\"],  # indices for context views\n        Int64[Tensor, \" target_view\"],  # indices for target views\n        Float[Tensor, \" overlap\"],  # overlap\n    ]:\n        pass\n\n    @property\n    @abstractmethod\n    def num_target_views(self) -> int:\n        pass\n\n    @property\n    @abstractmethod\n    def num_context_views(self) -> int:\n        pass\n\n    @property\n    def global_step(self) -> int:\n        return 0 if self.step_tracker is None else self.step_tracker.get_step()\n",
    "line_count": 61
  },
  {
    "id": "khaos_src_khaos_executor_topic_manager.py",
    "repo": "aleksandarskrbic/khaos",
    "url": "https://github.com/aleksandarskrbic/khaos/blob/main/src/khaos/executor/topic_manager.py",
    "code": "\"\"\"Topic management for Kafka scenarios.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\n\nfrom rich.console import Console\n\nfrom khaos.defaults import (\n    DEFAULT_FLOW_PARTITIONS,\n    DEFAULT_REPLICATION_FACTOR,\n    TOPIC_CREATION_WAIT_SECONDS,\n)\nfrom khaos.kafka.admin import KafkaAdmin\nfrom khaos.models.cluster import ClusterConfig\nfrom khaos.models.flow import FlowConfig\nfrom khaos.models.topic import TopicConfig as KafkaTopicConfig\nfrom khaos.scenarios.scenario import TopicConfig\n\nconsole = Console()\n\n\nclass TopicManager:\n    def __init__(self, bootstrap_servers: str, cluster_config: ClusterConfig | None = None):\n        self.admin = KafkaAdmin(bootstrap_servers, cluster_config=cluster_config)\n\n    def create_topic(self, name: str, partitions: int, replication_factor: int) -> None:\n        topic_config = KafkaTopicConfig(\n            name=name,\n            partitions=partitions,\n            replication_factor=replication_factor,\n        )\n        self.admin.delete_topic(name)\n        self.admin.create_topic(topic_config)\n\n    def delete_topic(self, name: str) -> None:\n        self.admin.delete_topic(name)\n\n    async def setup_topics(\n        self,\n        topics: list[TopicConfig],\n        flows: list[FlowConfig],\n    ) -> set[str]:\n        created_topics: set[str] = set()\n\n        for topic in topics:\n            console.print(\n                f\"[dim]Creating topic: {topic.name} ({topic.partitions} partitions)[/dim]\"\n            )\n            self.create_topic(\n                name=topic.name,\n                partitions=topic.partitions,\n                replication_factor=topic.replication_factor,\n            )\n            created_topics.add(topic.name)\n\n        for flow in flows:\n            for topic_name in flow.get_all_topics():\n                if topic_name not in created_topics:\n                    console.print(f\"[dim]Creating topic for flow: {topic_name}[/dim]\")\n                    self.create_topic(\n                        name=topic_name,\n                        partitions=DEFAULT_FLOW_PARTITIONS,\n                        replication_factor=DEFAULT_REPLICATION_FACTOR,\n                    )\n                    created_topics.add(topic_name)\n\n        if created_topics:\n            await asyncio.sleep(TOPIC_CREATION_WAIT_SECONDS)\n\n        return created_topics\n",
    "line_count": 71
  },
  {
    "id": "Kylie_modules_schedules_context_generation.py",
    "repo": "jonathanmuk/Kylie",
    "url": "https://github.com/jonathanmuk/Kylie/blob/main/modules/schedules/context_generation.py",
    "code": "from datetime import datetime\nfrom typing import Dict, Optional\n\nfrom core.schedules import (\n    FRIDAY_SCHEDULE,\n    MONDAY_SCHEDULE,\n    SATURDAY_SCHEDULE,\n    SUNDAY_SCHEDULE,\n    THURSDAY_SCHEDULE,\n    TUESDAY_SCHEDULE,\n    WEDNESDAY_SCHEDULE,\n)\n\n\nclass ScheduleContextGenerator:\n    \"\"\"Class to generate context about Ava's current activity based on schedules.\"\"\"\n\n    SCHEDULES = {\n        0: MONDAY_SCHEDULE,  # Monday\n        1: TUESDAY_SCHEDULE,  # Tuesday\n        2: WEDNESDAY_SCHEDULE,  # Wednesday\n        3: THURSDAY_SCHEDULE,  # Thursday\n        4: FRIDAY_SCHEDULE,  # Friday\n        5: SATURDAY_SCHEDULE,  # Saturday\n        6: SUNDAY_SCHEDULE,  # Sunday\n    }\n\n    @staticmethod\n    def _parse_time_range(time_range: str) -> tuple[datetime.time, datetime.time]:\n        \"\"\"Parse a time range string (e.g., '06:00-07:00') into start and end times.\"\"\"\n        start_str, end_str = time_range.split(\"-\")\n        start_time = datetime.strptime(start_str, \"%H:%M\").time()\n        end_time = datetime.strptime(end_str, \"%H:%M\").time()\n        return start_time, end_time\n\n    @classmethod\n    def get_current_activity(cls) -> Optional[str]:\n        \"\"\"Get Ava's current activity based on the current time and day of the week.\n\n        Returns:\n            str: Description of current activity, or None if no matching time slot is found\n        \"\"\"\n        # Get current time and day of week (0 = Monday, 6 = Sunday)\n        current_datetime = datetime.now()\n        current_time = current_datetime.time()\n        current_day = current_datetime.weekday()\n\n        # Get schedule for current day\n        schedule = cls.SCHEDULES.get(current_day, {})\n\n        # Find matching time slot\n        for time_range, activity in schedule.items():\n            start_time, end_time = cls._parse_time_range(time_range)\n\n            # Handle overnight activities (e.g., 23:00-06:00)\n            if start_time > end_time:\n                if current_time >= start_time or current_time <= end_time:\n                    return activity\n            else:\n                if start_time <= current_time <= end_time:\n                    return activity\n\n        return None\n\n    @classmethod\n    def get_schedule_for_day(cls, day: int) -> Dict[str, str]:\n        \"\"\"Get the complete schedule for a specific day.\n\n        Args:\n            day: Day of week as integer (0 = Monday, 6 = Sunday)\n\n        Returns:\n            Dict[str, str]: Schedule for the specified day\n        \"\"\"\n        return cls.SCHEDULES.get(day, {})\n",
    "line_count": 75
  },
  {
    "id": "Live2D-Virtual-Girlfriend_src_loader.py",
    "repo": "chinokikiss/Live2D-Virtual-Girlfriend",
    "url": "https://github.com/chinokikiss/Live2D-Virtual-Girlfriend/blob/main/src/loader.py",
    "code": "import os\r\nimport re\r\nimport numpy as np\r\nfrom config import Global\r\nfrom src.rvc import RVC\r\nfrom src.mcp_client import MCPClient\r\nfrom src.graph_rag import RAGMemory\r\nfrom modelscope.pipelines import pipeline\r\nfrom modelscope.utils.constant import Tasks\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\ndevice = Global.device\r\n\r\nclass SpeakerVerification:\r\n    def __init__(self):\r\n        if Global.your_voices:\r\n            self.verification = pipeline(\r\n                task='speaker-verification',\r\n                model='iic/speech_campplus_sv_zh-cn_16k-common',\r\n                model_revision='v1.0.0',\r\n                device=device\r\n            )\r\n            \r\n            self._warmup()\r\n    \r\n    def _warmup(self):\r\n        self.my_voice_embs = self.verification(Global.your_voices, output_emb=True)['embs']\r\n    \r\n    def verify_speaker(self, audio_file):\r\n        voice_emb = self.verification([audio_file], output_emb=True)['embs'][0]\r\n        \r\n        similarities = np.dot(self.my_voice_embs, voice_emb) / (\r\n            np.linalg.norm(self.my_voice_embs, axis=1) * np.linalg.norm(voice_emb)\r\n        )\r\n        \r\n        return np.mean(similarities)\r\n    \r\nclass SenseVoice:\r\n    def __init__(self):\r\n        self.model = pipeline(\r\n            task=Tasks.auto_speech_recognition,\r\n            model='iic/SenseVoiceSmall',\r\n            model_revision=\"master\",\r\n            device=device,\r\n            disable_update=True\r\n        )\r\n\r\n        self._warmup()\r\n    \r\n    def _warmup(self):\r\n        if os.path.exists('temp\\\\temp.wav'):\r\n            self.infer()\r\n\r\n    def infer(self, voice_path='temp\\\\temp.wav'):\r\n        result = self.model(voice_path)\r\n        pattern = r\"<\\|(.+?)\\|><\\|(.+?)\\|><\\|(.+?)\\|><\\|(.+?)\\|>(.+)\"\r\n        match = re.match(pattern, result[0]['text'])\r\n        if match:\r\n            language, emotion, audio_type, itn, text = match.groups()\r\n            text = f\"<{emotion}>{text}\"\r\n        else:\r\n            text = ''\r\n        return text\r\n\r\nGlobal.rvc = RVC()\r\nwith ThreadPoolExecutor(max_workers=5) as executor:\r\n    futures = [\r\n        executor.submit(lambda: setattr(Global, 'sense_voice', SenseVoice())),\r\n        executor.submit(lambda: setattr(Global, 'speaker_verifier', SpeakerVerification())),\r\n        executor.submit(lambda: setattr(Global, 'memory', RAGMemory())),\r\n        executor.submit(lambda: setattr(Global, 'mcp_client', MCPClient()))\r\n    ]\r\n\r\n    if \"rvc_model\" in Global.character:\r\n        futures.append(executor.submit(Global.rvc.change_voice, Global.character[\"rvc_model\"]))\r\n    \r\n    results = []\r\n    for future in futures:\r\n        results.append(future.result())\r\n    \r\n    if not (\"rvc_model\" in Global.character and results[4]):\r\n        Global.rvc.ok = False\r\n        print('[!]RVCæœªå¯åŠ¨')",
    "line_count": 83
  },
  {
    "id": "eye_of_web_src_lib_flickr_crawler_flickr_modules_logger.py",
    "repo": "MehmetYukselSekeroglu/eye_of_web",
    "url": "https://github.com/MehmetYukselSekeroglu/eye_of_web/blob/main/src/lib/flickr_crawler/flickr_modules/logger.py",
    "code": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nFlickr Crawler Logger Module\n\nThis module provides a Logger class for the Flickr crawler.\n\"\"\"\n\nimport os\nimport logging\nimport sys\nfrom datetime import datetime\n\n\nclass Logger:\n    \"\"\"Logger class for the Flickr crawler.\"\"\"\n    \n    def __init__(self, output_dir=\"output_flickr\", log_level=logging.INFO):\n        \"\"\"\n        Initialize the logger.\n        \n        Args:\n            output_dir: Directory where log files will be stored.\n            log_level: Logging level.\n        \"\"\"\n        self.output_dir = output_dir\n        self.log_level = log_level\n        \n        # Create output directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Create logger\n        self.logger = logging.getLogger(\"flickr_crawler\")\n        self.logger.setLevel(log_level)\n        \n        # Remove existing handlers to avoid duplicates\n        if self.logger.handlers:\n            self.logger.handlers.clear()\n        \n        # Create console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(log_level)\n        console_formatter = logging.Formatter('%(message)s')  # Simple format for console\n        console_handler.setFormatter(console_formatter)\n        self.logger.addHandler(console_handler)\n        \n        # Create file handler\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        log_file = os.path.join(output_dir, f\"flickr_crawler_{timestamp}.log\")\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(log_level)\n        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n        file_handler.setFormatter(file_formatter)\n        self.logger.addHandler(file_handler)\n        \n        self.log_file_path = log_file\n        \n        # Show initialization message\n        self.info(f\"Logger initialized. Log file: {log_file}\")\n    \n    def debug(self, message):\n        \"\"\"Log a debug message.\"\"\"\n        self.logger.debug(message)\n    \n    def info(self, message):\n        \"\"\"Log an info message.\"\"\"\n        self.logger.info(message)\n    \n    def warning(self, message):\n        \"\"\"Log a warning message.\"\"\"\n        self.logger.warning(message)\n    \n    def error(self, message):\n        \"\"\"Log an error message.\"\"\"\n        self.logger.error(message)\n    \n    def critical(self, message):\n        \"\"\"Log a critical message.\"\"\"\n        self.logger.critical(message)\n        \n    def flush(self, message, with_newline=False):\n        \"\"\"\n        Print a message without logging and without newline.\n        \n        This is a workaround for end=\"\" functionality.\n        The message will be printed to stdout but not logged to file.\n        \n        Args:\n            message: The message to print.\n            with_newline: Whether to add a newline at the end.\n        \"\"\"\n        if with_newline:\n            print(message)\n        else:\n            print(message, end=\"\", flush=True) ",
    "line_count": 95
  },
  {
    "id": "aicon_backend_src_services_base.py",
    "repo": "869413421/aicon",
    "url": "https://github.com/869413421/aicon/blob/main/backend/src/services/base.py",
    "code": "\"\"\"\næœåŠ¡åŸºç±» - æä¾›ç»Ÿä¸€çš„æ•°æ®åº“ä¼šè¯ç®¡ç†å’ŒåŸºç¡€åŠŸèƒ½\n\"\"\"\n\nfrom typing import Optional, TYPE_CHECKING\n\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.core.database import AsyncSessionLocal\nfrom src.core.logging import get_logger\n\nif TYPE_CHECKING:\n    from typing import Any, Dict, Optional\n\nlogger = get_logger(__name__)\n\n\nclass BaseService:\n    \"\"\"\n    æœåŠ¡åŸºç±»\n    è¦æ±‚å¤–éƒ¨æ³¨å…¥ AsyncSessionã€‚\n    \"\"\"\n\n    def __init__(self, db_session: AsyncSession):\n        \"\"\"\n        åˆå§‹åŒ–æœåŠ¡å®ä¾‹\n        Args:\n            db_session: å¿…é¡»æä¾›å¼‚æ­¥æ•°æ®åº“ä¼šè¯\n        \"\"\"\n        self._db_session = db_session\n\n    @property\n    def db_session(self) -> AsyncSession:\n        \"\"\"è·å–å¹¶éªŒè¯å½“å‰ç»‘å®šçš„æ•°æ®åº“ä¼šè¯\"\"\"\n        if self._db_session is None:\n            raise RuntimeError(f\"{self.__class__.__name__} å°šæœªç»‘å®šæ•°æ®åº“ä¼šè¯\")\n        return self._db_session\n\n    async def commit(self):\n        \"\"\"æäº¤å½“å‰äº‹åŠ¡\"\"\"\n        await self.db_session.commit()\n\n    async def rollback(self):\n        \"\"\"å›æ»šå½“å‰äº‹åŠ¡\"\"\"\n        await self.db_session.rollback()\n\n    async def flush(self):\n        \"\"\"åˆ·æ–°å½“å‰ä¼šè¯\"\"\"\n        await self.db_session.flush()\n\n    async def refresh(self, obj):\n        \"\"\"åˆ·æ–°å¯¹è±¡æ•°æ®\"\"\"\n        await self.db_session.refresh(obj)\n\n    async def execute(self, query, params: Optional[dict] = None):\n        \"\"\"æ‰§è¡ŒSQLæŸ¥è¯¢\"\"\"\n        return await self.db_session.execute(query, params)\n\n    def add(self, obj):\n        \"\"\"æ·»åŠ å¯¹è±¡åˆ°ä¼šè¯\"\"\"\n        self.db_session.add(obj)\n\n    def delete(self, obj):\n        \"\"\"ä»ä¼šè¯ä¸­åˆ é™¤å¯¹è±¡\"\"\"\n        self.db_session.delete(obj)\n\n    async def get(self, model_class, identifier):\n        \"\"\"æ ¹æ®IDè·å–å¯¹è±¡\"\"\"\n        return await self.db_session.get(model_class, identifier)\n\n\n__all__ = [\"BaseService\"]\n",
    "line_count": 72
  },
  {
    "id": "muwanx_src_muwanx_project.py",
    "repo": "ttktjmt/muwanx",
    "url": "https://github.com/ttktjmt/muwanx/blob/main/src/muwanx/project.py",
    "code": "\"\"\"Project configuration and management.\n\nThis module defines the ProjectConfig dataclass and ProjectHandle class for\nmanaging projects containing multiple scenes.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any\n\nimport mujoco\n\nfrom .scene import SceneConfig, SceneHandle\n\nif TYPE_CHECKING:\n    from .builder import Builder\n\n\n@dataclass\nclass ProjectConfig:\n    \"\"\"Configuration for a project containing multiple scenes.\"\"\"\n\n    name: str\n    \"\"\"Name of the project.\"\"\"\n\n    id: str | None = None\n    \"\"\"Optional ID for the project used in URL routing (e.g., 'menagerie' for /#/menagerie/).\"\"\"\n\n    scenes: list[SceneConfig] = field(default_factory=list)\n    \"\"\"List of scenes in the project.\"\"\"\n\n\nclass ProjectHandle:\n    \"\"\"Handle for adding scenes and configuring a project.\n\n    This class provides methods for adding scenes and customizing project properties.\n    Similar to viser's server handle, this allows for hierarchical configuration.\n    \"\"\"\n\n    def __init__(self, project_config: ProjectConfig, builder: Builder) -> None:\n        self._config = project_config\n        self._builder = builder\n\n    @property\n    def name(self) -> str:\n        \"\"\"Name of the project.\"\"\"\n        return self._config.name\n\n    @property\n    def id(self) -> str | None:\n        \"\"\"Optional ID of the project for URL routing.\"\"\"\n        return self._config.id\n\n    def add_scene(\n        self,\n        model: mujoco.MjModel | str | Path,\n        name: str,\n        *,\n        metadata: dict[str, Any] | None = None,\n        source_path: str | None = None,\n    ) -> SceneHandle:\n        \"\"\"Add a MuJoCo scene to this project.\n\n        Args:\n            model: MuJoCo model for the scene, or a path to an MJCF XML file.\n            name: Name for the scene (displayed in the UI).\n            metadata: Optional metadata dictionary for the scene.\n            source_path: Optional MJCF XML path for asset copying.\n\n        Returns:\n            SceneHandle for adding policies and further configuration.\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n\n        if isinstance(model, (str, Path)):\n            source_path = str(model)\n            model = mujoco.MjModel.from_xml_path(str(model))\n\n        scene_config = SceneConfig(\n            name=name,\n            model=model,\n            metadata=metadata,\n            source_path=source_path,\n        )\n        self._config.scenes.append(scene_config)\n        return SceneHandle(scene_config, self)\n\n\n__all__ = [\"ProjectConfig\", \"ProjectHandle\"]\n",
    "line_count": 92
  },
  {
    "id": "nano-sglang_python_sglang_srt_sampling_params.py",
    "repo": "gogongxt/nano-sglang",
    "url": "https://github.com/gogongxt/nano-sglang/blob/master/python/sglang/srt/sampling_params.py",
    "code": "\"\"\"Sampling parameters for text generation.\"\"\"\n\nfrom typing import List, Optional, Union\n\n_SAMPLING_EPS = 1e-6\n\n\nclass SamplingParams:\n    def __init__(\n        self,\n        max_new_tokens: int = 16,\n        stop: Optional[Union[str, List[str]]] = None,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = -1,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        ignore_eos: bool = False,\n        skip_special_tokens: bool = True,\n        dtype: Optional[str] = None,\n        regex: Optional[str] = None,\n    ) -> None:\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n        self.stop_strs = stop\n        self.max_new_tokens = max_new_tokens\n        self.ignore_eos = ignore_eos\n        self.skip_special_tokens = skip_special_tokens\n        self.dtype = dtype\n        self.regex = regex\n\n        # Process some special cases\n        if self.temperature < _SAMPLING_EPS:\n            self.temperature = 1.0\n            self.top_k = 1\n        if self.top_k == -1:\n            self.top_k = 1 << 30  # whole vocabulary\n        if self.dtype == \"int\":\n            self.stop_strs = [\" \", \"\\n\"]\n\n    def verify(self):\n        if self.temperature < 0.0:\n            raise ValueError(\n                f\"temperature must be non-negative, got {self.temperature}.\"\n            )\n        if not 0.0 < self.top_p <= 1.0:\n            raise ValueError(f\"top_p must be in (0, 1], got {self.top_p}.\")\n        if self.top_k < -1 or self.top_k == 0:\n            raise ValueError(\n                f\"top_k must be -1 (disable), or at least 1, \" f\"got {self.top_k}.\"\n            )\n        if not -2.0 <= self.frequency_penalty <= 2.0:\n            raise ValueError(\n                \"frequency_penalty must be in [-2, 2], got \"\n                f\"{self.frequency_penalty}.\"\n            )\n        if not -2.0 <= self.presence_penalty <= 2.0:\n            raise ValueError(\n                \"presence_penalty must be in [-2, 2], got \" f\"{self.presence_penalty}.\"\n            )\n        if self.max_new_tokens < 0:\n            raise ValueError(\n                f\"max_new_tokens must be at least 0, got {self.max_new_tokens}.\"\n            )\n\n    def normalize(self, tokenizer):\n        # Process stop strings\n        if self.stop_strs is None:\n            self.stop_strs = []\n            self.stop_str_max_len = 0\n        else:\n            if isinstance(self.stop_strs, str):\n                self.stop_strs = [self.stop_strs]\n\n            stop_str_max_len = 0\n            for stop_str in self.stop_strs:\n                stop_str_ids = tokenizer.encode(stop_str, add_special_tokens=False)\n                stop_str_max_len = max(stop_str_max_len, len(stop_str_ids))\n            self.stop_str_max_len = stop_str_max_len\n",
    "line_count": 82
  },
  {
    "id": "MemoryBear_api_app_repositories_release_share_repository.py",
    "repo": "SuanmoSuanyangTechnology/MemoryBear",
    "url": "https://github.com/SuanmoSuanyangTechnology/MemoryBear/blob/main/api/app/repositories/release_share_repository.py",
    "code": "import uuid\nfrom typing import Optional\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import select\nfrom app.models import ReleaseShare\n\n\nclass ReleaseShareRepository:\n    \"\"\"å‘å¸ƒç‰ˆæœ¬åˆ†äº«ä»“å‚¨\"\"\"\n    \n    def __init__(self, db: Session):\n        self.db = db\n    \n    def create(self, release_share: ReleaseShare) -> ReleaseShare:\n        \"\"\"åˆ›å»ºåˆ†äº«é…ç½®\"\"\"\n        self.db.add(release_share)\n        self.db.commit()\n        self.db.refresh(release_share)\n        return release_share\n    \n    def get_by_id(self, share_id: uuid.UUID) -> Optional[ReleaseShare]:\n        \"\"\"æ ¹æ® ID è·å–åˆ†äº«é…ç½®\"\"\"\n        return self.db.get(ReleaseShare, share_id)\n    \n    def get_by_release_id(self, release_id: uuid.UUID) -> Optional[ReleaseShare]:\n        \"\"\"æ ¹æ®å‘å¸ƒç‰ˆæœ¬ ID è·å–åˆ†äº«é…ç½®\"\"\"\n        stmt = select(ReleaseShare).where(ReleaseShare.release_id == release_id)\n        return self.db.scalars(stmt).first()\n    \n    def get_by_share_token(self, share_token: str) -> Optional[ReleaseShare]:\n        \"\"\"æ ¹æ®åˆ†äº« token è·å–åˆ†äº«é…ç½®\"\"\"\n        stmt = select(ReleaseShare).where(ReleaseShare.share_token == share_token)\n        return self.db.scalars(stmt).first()\n    \n    def update(self, release_share: ReleaseShare) -> ReleaseShare:\n        \"\"\"æ›´æ–°åˆ†äº«é…ç½®\"\"\"\n        self.db.commit()\n        self.db.refresh(release_share)\n        return release_share\n    \n    def delete(self, release_share: ReleaseShare) -> None:\n        \"\"\"åˆ é™¤åˆ†äº«é…ç½®\"\"\"\n        self.db.delete(release_share)\n        self.db.commit()\n    \n    def token_exists(self, share_token: str) -> bool:\n        \"\"\"æ£€æŸ¥ token æ˜¯å¦å·²å­˜åœ¨\"\"\"\n        stmt = select(ReleaseShare.id).where(ReleaseShare.share_token == share_token)\n        return self.db.scalars(stmt).first() is not None\n    \n    def increment_view_count(self, share_id: uuid.UUID) -> None:\n        \"\"\"å¢åŠ è®¿é—®æ¬¡æ•°ï¼ˆå¼‚æ­¥æ›´æ–°ï¼Œä¸é˜»å¡ï¼‰\"\"\"\n        from datetime import datetime\n        stmt = select(ReleaseShare).where(ReleaseShare.id == share_id)\n        share = self.db.scalars(stmt).first()\n        if share:\n            share.view_count += 1\n            share.last_accessed_at = datetime.now()\n            self.db.commit()\n",
    "line_count": 59
  },
  {
    "id": "crypto-tax-calculator_src_logger.py",
    "repo": "ura-vf4/crypto-tax-calculator",
    "url": "https://github.com/ura-vf4/crypto-tax-calculator/blob/main/src/logger.py",
    "code": "import logging.handlers\n\nfrom .notifications import NotificationHandler\n\n\nclass Logger:\n    Logger = None\n    NotificationHandler = None \n\n    def __init__(self, logging_service=\"crypto_trading\", enable_notifications=True):\n        # Logger setup\n        self.Logger = logging.getLogger(f\"{logging_service}_logger\")\n        self.Logger.setLevel(logging.DEBUG)\n        self.Logger.propagate = False\n        formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n        # default is \"logs/crypto_trading.log\"\n        fh = logging.FileHandler(f\"logs/{logging_service}.log\")\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n        self.Logger.addHandler(fh)\n\n        # logging to console\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.INFO)\n        ch.setFormatter(formatter)\n        self.Logger.addHandler(ch)\n\n        # notification handler\n        self.NotificationHandler = NotificationHandler(enable_notifications)\n\n    def log(self, message, level=\"info\", notification=True):\n        if level == \"info\":\n            self.Logger.info(message)\n        elif level == \"warning\":\n            self.Logger.warning(message)\n        elif level == \"error\":\n            self.Logger.error(message)\n        elif level == \"debug\":\n            self.Logger.debug(message)\n\n        if notification and self.NotificationHandler.enabled:\n            self.NotificationHandler.send_notification(str(message))\n\n    def info(self, message, notification=True):\n        self.log(message, \"info\", notification)\n\n    def warning(self, message, notification=True):\n        self.log(message, \"warning\", notification)\n\n    def error(self, message, notification=True):\n        self.log(message, \"error\", notification)\n\n    def debug(self, message, notification=False):\n        self.log(message, \"debug\", notification)\n",
    "line_count": 54
  },
  {
    "id": "SPFSplat_src_dataset_view_sampler_view_sampler.py",
    "repo": "ranrhuang/SPFSplat",
    "url": "https://github.com/ranrhuang/SPFSplat/blob/master/src/dataset/view_sampler/view_sampler.py",
    "code": "from abc import ABC, abstractmethod\nfrom typing import Generic, TypeVar\n\nimport torch\nfrom jaxtyping import Float, Int64\nfrom torch import Tensor\n\nfrom ...misc.step_tracker import StepTracker\nfrom ..types import Stage\n\nT = TypeVar(\"T\")\n\n\nclass ViewSampler(ABC, Generic[T]):\n    cfg: T\n    stage: Stage\n    is_overfitting: bool\n    cameras_are_circular: bool\n    step_tracker: StepTracker | None\n\n    def __init__(\n        self,\n        cfg: T,\n        stage: Stage,\n        is_overfitting: bool,\n        cameras_are_circular: bool,\n        step_tracker: StepTracker | None,\n    ) -> None:\n        self.cfg = cfg\n        self.stage = stage\n        self.is_overfitting = is_overfitting\n        self.cameras_are_circular = cameras_are_circular\n        self.step_tracker = step_tracker\n\n    @abstractmethod\n    def sample(\n        self,\n        scene: str,\n        extrinsics: Float[Tensor, \"view 4 4\"],\n        intrinsics: Float[Tensor, \"view 3 3\"],\n        device: torch.device = torch.device(\"cpu\"),\n    ) -> tuple[\n        Int64[Tensor, \" context_view\"],  # indices for context views\n        Int64[Tensor, \" target_view\"],  # indices for target views\n        Float[Tensor, \" overlap\"],  # overlap\n    ]:\n        pass\n\n    @property\n    @abstractmethod\n    def num_target_views(self) -> int:\n        pass\n\n    @property\n    @abstractmethod\n    def num_context_views(self) -> int:\n        pass\n\n    @property\n    def global_step(self) -> int:\n        return 0 if self.step_tracker is None else self.step_tracker.get_step()\n",
    "line_count": 61
  },
  {
    "id": "4D-ARE_src_four_d_are_config.py",
    "repo": "ybeven/4D-ARE",
    "url": "https://github.com/ybeven/4D-ARE/blob/main/src/four_d_are/config.py",
    "code": "\"\"\"\n4D-ARE Configuration via Environment Variables\n\nUses pydantic-settings for validation and .env file support.\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Literal\n\nfrom pydantic import Field, SecretStr\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    \"\"\"Application settings loaded from environment variables.\"\"\"\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        extra=\"ignore\",\n    )\n\n    # API Configuration\n    openai_api_key: SecretStr = Field(\n        ...,  # Required\n        description=\"OpenAI API key (or compatible provider)\",\n    )\n    openai_base_url: str = Field(\n        default=\"https://api.openai.com/v1\",\n        description=\"API base URL for OpenAI-compatible providers\",\n    )\n\n    # Model Selection\n    model_agent: str = Field(\n        default=\"gpt-4o\",\n        description=\"Model for agent execution\",\n    )\n\n    # MCP Server Configuration\n    mcp_server_type: Literal[\"demo\", \"mysql\", \"postgres\", \"excel\"] = Field(\n        default=\"demo\",\n        description=\"Type of MCP server to use for data access\",\n    )\n\n    # MySQL Configuration\n    mysql_host: str = Field(default=\"localhost\")\n    mysql_port: int = Field(default=3306)\n    mysql_user: str = Field(default=\"root\")\n    mysql_password: SecretStr = Field(default=SecretStr(\"\"))\n    mysql_database: str = Field(default=\"analytics\")\n\n    # PostgreSQL Configuration\n    postgres_host: str = Field(default=\"localhost\")\n    postgres_port: int = Field(default=5432)\n    postgres_user: str = Field(default=\"postgres\")\n    postgres_password: SecretStr = Field(default=SecretStr(\"\"))\n    postgres_database: str = Field(default=\"analytics\")\n\n    # Excel Configuration\n    excel_file_path: Path = Field(default=Path(\"./data/metrics.xlsx\"))\n\n    # Output Configuration\n    output_dir: Path = Field(default=Path(\"./output\"))\n\n    @property\n    def scenarios_path(self) -> Path:\n        return self.output_dir / \"scenarios.json\"\n\n    @property\n    def results_path(self) -> Path:\n        return self.output_dir / \"results.csv\"\n\n    @property\n    def detailed_results_path(self) -> Path:\n        return self.output_dir / \"detailed_results.json\"\n\n\n# Singleton pattern for settings\n_settings: Settings | None = None\n\n\ndef get_settings() -> Settings:\n    \"\"\"Factory function to create or return cached settings instance.\"\"\"\n    global _settings\n    if _settings is None:\n        _settings = Settings()\n    return _settings\n\n\ndef reset_settings() -> None:\n    \"\"\"Reset cached settings (useful for testing).\"\"\"\n    global _settings\n    _settings = None\n",
    "line_count": 93
  },
  {
    "id": "Resume-Agent_backend_agent_flow_base.py",
    "repo": "WyRainBow/Resume-Agent",
    "url": "https://github.com/WyRainBow/Resume-Agent/blob/main/backend/agent/flow/base.py",
    "code": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Union\n\nfrom pydantic import BaseModel\n\nfrom backend.agent.agent.base import BaseAgent\n\n\nclass BaseFlow(BaseModel, ABC):\n    \"\"\"Base class for execution flows supporting multiple agents\"\"\"\n\n    agents: Dict[str, BaseAgent]\n    tools: Optional[List] = None\n    primary_agent_key: Optional[str] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __init__(\n        self, agents: Union[BaseAgent, List[BaseAgent], Dict[str, BaseAgent]], **data\n    ):\n        # Handle different ways of providing agents\n        if isinstance(agents, BaseAgent):\n            agents_dict = {\"default\": agents}\n        elif isinstance(agents, list):\n            agents_dict = {f\"agent_{i}\": agent for i, agent in enumerate(agents)}\n        else:\n            agents_dict = agents\n\n        # If primary agent not specified, use first agent\n        primary_key = data.get(\"primary_agent_key\")\n        if not primary_key and agents_dict:\n            primary_key = next(iter(agents_dict))\n            data[\"primary_agent_key\"] = primary_key\n\n        # Set the agents dictionary\n        data[\"agents\"] = agents_dict\n\n        # Initialize using BaseModel's init\n        super().__init__(**data)\n\n    @property\n    def primary_agent(self) -> Optional[BaseAgent]:\n        \"\"\"Get the primary agent for the flow\"\"\"\n        return self.agents.get(self.primary_agent_key)\n\n    def get_agent(self, key: str) -> Optional[BaseAgent]:\n        \"\"\"Get a specific agent by key\"\"\"\n        return self.agents.get(key)\n\n    def add_agent(self, key: str, agent: BaseAgent) -> None:\n        \"\"\"Add a new agent to the flow\"\"\"\n        self.agents[key] = agent\n\n    @abstractmethod\n    async def execute(self, input_text: str) -> str:\n        \"\"\"Execute the flow with given input\"\"\"\n",
    "line_count": 57
  },
  {
    "id": "RoboCOIN_src_lerobot_datasets_sampler.py",
    "repo": "FlagOpen/RoboCOIN",
    "url": "https://github.com/FlagOpen/RoboCOIN/blob/main/src/lerobot/datasets/sampler.py",
    "code": "#!/usr/bin/env python\n\n# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom collections.abc import Iterator\n\nimport torch\n\n\nclass EpisodeAwareSampler:\n    def __init__(\n        self,\n        episode_data_index: dict,\n        episode_indices_to_use: list | None = None,\n        drop_n_first_frames: int = 0,\n        drop_n_last_frames: int = 0,\n        shuffle: bool = False,\n    ):\n        \"\"\"Sampler that optionally incorporates episode boundary information.\n\n        Args:\n            episode_data_index: Dictionary with keys 'from' and 'to' containing the start and end indices of each episode.\n            episode_indices_to_use: List of episode indices to use. If None, all episodes are used.\n                                    Assumes that episodes are indexed from 0 to N-1.\n            drop_n_first_frames: Number of frames to drop from the start of each episode.\n            drop_n_last_frames: Number of frames to drop from the end of each episode.\n            shuffle: Whether to shuffle the indices.\n        \"\"\"\n        indices = []\n        for episode_idx, (start_index, end_index) in enumerate(\n            zip(episode_data_index[\"from\"], episode_data_index[\"to\"], strict=True)\n        ):\n            if episode_indices_to_use is None or episode_idx in episode_indices_to_use:\n                indices.extend(\n                    range(start_index.item() + drop_n_first_frames, end_index.item() - drop_n_last_frames)\n                )\n\n        self.indices = indices\n        self.shuffle = shuffle\n\n    def __iter__(self) -> Iterator[int]:\n        if self.shuffle:\n            for i in torch.randperm(len(self.indices)):\n                yield self.indices[i]\n        else:\n            for i in self.indices:\n                yield i\n\n    def __len__(self) -> int:\n        return len(self.indices)\n",
    "line_count": 61
  },
  {
    "id": "promptmanager_src_promptmanager_compression_tokenizers_hf_counter.py",
    "repo": "h9-tec/promptmanager",
    "url": "https://github.com/h9-tec/promptmanager/blob/main/src/promptmanager/compression/tokenizers/hf_counter.py",
    "code": "\"\"\"HuggingFace tokenizer-based token counter.\"\"\"\n\nfrom typing import List, Optional\nfrom .base import TokenCounter\n\n\nclass HuggingFaceCounter(TokenCounter):\n    \"\"\"\n    Token counter using HuggingFace transformers tokenizers.\n\n    Supports any model available on HuggingFace Hub.\n    \"\"\"\n\n    def __init__(self, model_name: str = \"gpt2\"):\n        \"\"\"\n        Initialize HuggingFaceCounter for a specific model.\n\n        Args:\n            model_name: HuggingFace model name (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\")\n        \"\"\"\n        try:\n            from transformers import AutoTokenizer\n            self._AutoTokenizer = AutoTokenizer\n        except ImportError:\n            raise ImportError(\n                \"transformers is required for HuggingFaceCounter. \"\n                \"Install with: pip install transformers\"\n            )\n\n        self._model_name = model_name\n        self.name = f\"hf-{model_name.split('/')[-1]}\"\n\n        # Load tokenizer\n        self._tokenizer = self._AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True\n        )\n\n    def count(self, text: str) -> int:\n        \"\"\"Count tokens using HuggingFace tokenizer.\"\"\"\n        return len(self._tokenizer.encode(text, add_special_tokens=False))\n\n    def encode(self, text: str) -> List[int]:\n        \"\"\"Encode text to token IDs.\"\"\"\n        return self._tokenizer.encode(text, add_special_tokens=False)\n\n    def decode(self, tokens: List[int]) -> str:\n        \"\"\"Decode token IDs to text.\"\"\"\n        return self._tokenizer.decode(tokens)\n\n    def count_with_special_tokens(self, text: str) -> int:\n        \"\"\"Count tokens including special tokens (BOS, EOS, etc.).\"\"\"\n        return len(self._tokenizer.encode(text, add_special_tokens=True))\n\n    def get_vocab_size(self) -> int:\n        \"\"\"Get the vocabulary size.\"\"\"\n        return self._tokenizer.vocab_size\n\n    def tokenize(self, text: str) -> List[str]:\n        \"\"\"Get string tokens (not IDs).\"\"\"\n        return self._tokenizer.tokenize(text)\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"Get the model name.\"\"\"\n        return self._model_name\n\n    @classmethod\n    def list_common_models(cls) -> List[str]:\n        \"\"\"List commonly used models.\"\"\"\n        return [\n            \"gpt2\",\n            \"gpt2-medium\",\n            \"gpt2-large\",\n            \"gpt2-xl\",\n            \"meta-llama/Llama-2-7b-hf\",\n            \"meta-llama/Llama-2-13b-hf\",\n            \"meta-llama/Llama-2-70b-hf\",\n            \"mistralai/Mistral-7B-v0.1\",\n            \"mistralai/Mixtral-8x7B-v0.1\",\n            \"google/gemma-2b\",\n            \"google/gemma-7b\",\n            \"Qwen/Qwen-7B\",\n            \"bigscience/bloom-560m\",\n            \"EleutherAI/gpt-neo-125m\",\n            \"EleutherAI/gpt-neo-1.3B\",\n            \"EleutherAI/gpt-neo-2.7B\",\n            \"EleutherAI/gpt-j-6B\",\n        ]\n",
    "line_count": 89
  },
  {
    "id": "FinSight_src_tools_base.py",
    "repo": "RUC-NLPIR/FinSight",
    "url": "https://github.com/RUC-NLPIR/FinSight/blob/main/src/tools/base.py",
    "code": "import pandas as pd\nimport uuid\n\nclass Tool:\n    def __init__(\n        self,\n        name: str,\n        description: str,\n        parameters: list[dict]\n    ):\n        self.name = name\n        self.type = f'tool_{name}'\n        self.id = f\"tool_{name}_{uuid.uuid4().hex[:8]}\"\n        self.short_description = description\n        self.parameters = parameters\n\n    def prepare_params(self, task) -> dict:\n        \"\"\"\n        Optional hook to derive API parameters from a task payload.\n        \"\"\"\n        return {}\n    \n    @property\n    def description(self):\n        params_str = \", \".join([\n            f\"{p['name']}: {p['type']} ({p['description']})\"\n                for p in self.parameters\n        ])\n        return f\"Tool name: {self.name}\\nDescription: {self.short_description}\\nParameters: {params_str}\\n\"\n\n    async def api_function(self, **kwargs):\n        \"\"\"\n        Execute the underlying API and return structured data.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_data(self, task):\n        params = self.prepare_params(task)\n        try:\n            data = await self.api_function(**params)\n            task.all_results.extend(data)\n            return data\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return []\n\n\nclass ToolResult:\n    def __init__(self, name, description, data, source = \"\"):\n        self.name = name\n        self.description = description\n        if isinstance(data, list) and len(data) == 1:\n            data = data[0]\n        self.data = data\n        self.data_type = type(data)\n        self.source = source  # str, data source\n\n    def brief_str(self):\n        return self.__str__()\n\n    def get_full_string(self):\n        if isinstance(self.data, pd.DataFrame):\n            return self.data.to_string()\n        else:\n            return str(self.data)\n\n    def __str__(self):\n        base_string = f\"Data name: {self.name}\\nDescription: {self.description}\\nSource: {self.source}\\n\"\n        base_string += f\"Data type: {type(self.data)}\\n\"\n        if isinstance(self.data, pd.DataFrame):\n            format_string = \"\"\n            format_string += f\"First five rows:\\n{self.data.head().to_string()}\\n\"\n        elif isinstance(self.data, dict):\n            format_string = \"Partial data preview: \"\n            format_string += str(self.data)[:100]\n        elif isinstance(self.data, list):\n            format_string = \"Partial data preview: \"\n            format_string += str(self.data)[:100]\n        else:\n            format_string = \"Partial data preview: \"\n            format_string += str(self.data)[:100]\n\n        return base_string + format_string\n\n    def __repr__(self):\n        return self.__str__()\n    \n    def __hash__(self):\n        return hash(self.name+self.description)\n    \n    def __eq__(self, other):\n        return self.name == other.name and self.description == other.description",
    "line_count": 92
  },
  {
    "id": "forgetful_app_protocols_activity_protocol.py",
    "repo": "ScottRBK/forgetful",
    "url": "https://github.com/ScottRBK/forgetful/blob/main/app/protocols/activity_protocol.py",
    "code": "\"\"\"\nProtocol definition for the Activity Repository.\n\nDefines the contract for persisting and querying activity events\nacross different database backends (SQLite, PostgreSQL).\n\"\"\"\n\nfrom datetime import datetime\nfrom typing import Protocol\nfrom uuid import UUID\n\nfrom app.models.activity_models import (\n    ActivityEvent,\n    ActivityLogEntry,\n    EntityType,\n    ActionType,\n    ActorType,\n)\n\n\nclass ActivityRepository(Protocol):\n    \"\"\"Contract for the Activity Repository.\"\"\"\n\n    async def save_event(\n        self,\n        user_id: UUID,\n        event: ActivityEvent,\n    ) -> ActivityLogEntry:\n        \"\"\"\n        Persist an activity event to the database.\n\n        Args:\n            user_id: User ID for ownership\n            event: The activity event to persist\n\n        Returns:\n            The persisted activity log entry with database ID\n        \"\"\"\n        ...\n\n    async def query_events(\n        self,\n        user_id: UUID,\n        entity_type: EntityType | None = None,\n        action: ActionType | None = None,\n        entity_id: int | None = None,\n        actor: ActorType | None = None,\n        since: datetime | None = None,\n        until: datetime | None = None,\n        limit: int = 50,\n        offset: int = 0,\n    ) -> tuple[list[ActivityLogEntry], int]:\n        \"\"\"\n        Query activity events with filtering and pagination.\n\n        Args:\n            user_id: User ID for ownership filtering\n            entity_type: Filter by entity type (memory, project, etc.)\n            action: Filter by action (created, updated, deleted, read, queried)\n            entity_id: Filter by specific entity ID\n            actor: Filter by actor (user, system, llm-maintenance)\n            since: Only events after this timestamp\n            until: Only events before this timestamp\n            limit: Maximum results to return (1-100)\n            offset: Skip N results for pagination\n\n        Returns:\n            Tuple of (events, total_count) where total_count is\n            the count BEFORE limit/offset applied (for pagination)\n        \"\"\"\n        ...\n\n    async def cleanup_expired(\n        self,\n        user_id: UUID,\n        retention_days: int,\n    ) -> int:\n        \"\"\"\n        Delete activity events older than the retention period.\n\n        Args:\n            user_id: User ID for ownership filtering\n            retention_days: Delete events older than this many days\n\n        Returns:\n            Number of events deleted\n        \"\"\"\n        ...\n\n    async def count_events(\n        self,\n        user_id: UUID,\n        entity_type: EntityType | None = None,\n        action: ActionType | None = None,\n    ) -> int:\n        \"\"\"\n        Count activity events matching filters.\n\n        Args:\n            user_id: User ID for ownership filtering\n            entity_type: Filter by entity type (optional)\n            action: Filter by action (optional)\n\n        Returns:\n            Total count of matching events\n        \"\"\"\n        ...\n",
    "line_count": 107
  },
  {
    "id": "poker_solver_python_src_algorithms_infoset.py",
    "repo": "noambrown/poker_solver",
    "url": "https://github.com/noambrown/poker_solver/blob/main/python/src/algorithms/infoset.py",
    "code": "from __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom math import pow\nfrom typing import List\n\n\n@dataclass\nclass InfoSet:\n    actions: List[str]\n    regret_sum: List[float] = field(init=False)\n    strategy_sum: List[float] = field(init=False)\n    last_dcfr_iter: int = 0\n\n    def __post_init__(self) -> None:\n        self.regret_sum = [0.0 for _ in self.actions]\n        self.strategy_sum = [0.0 for _ in self.actions]\n\n    def current_strategy(self) -> List[float]:\n        # Regret-matching: normalize only positive regrets.\n        positive_regrets = [max(r, 0.0) for r in self.regret_sum]\n        normalizing = sum(positive_regrets)\n        if normalizing > 0.0:\n            return [r / normalizing for r in positive_regrets]\n        return [1.0 / len(self.actions) for _ in self.actions]\n\n    def average_strategy(self) -> List[float]:\n        # Average strategy uses cumulative reach-weighted strategy sums.\n        normalizing = sum(self.strategy_sum)\n        if normalizing > 0.0:\n            return [s / normalizing for s in self.strategy_sum]\n        return [1.0 / len(self.actions) for _ in self.actions]\n\n    def apply_dcfr_discount(self, iteration: int, alpha: float, beta: float, gamma: float) -> None:\n        if self.last_dcfr_iter == iteration:\n            return\n        # Apply DCFR decay from the last applied iteration up to current.\n        for t in range(self.last_dcfr_iter + 1, iteration + 1):\n            pos_base = pow(float(t), alpha)\n            neg_base = pow(float(t), beta)\n            pos_scale = pos_base / (pos_base + 1.0)\n            neg_scale = neg_base / (neg_base + 1.0)\n            strat_scale = pow(float(t) / (float(t) + 1.0), gamma)\n            for idx, regret in enumerate(self.regret_sum):\n                if regret > 0.0:\n                    self.regret_sum[idx] = regret * pos_scale\n                elif regret < 0.0:\n                    self.regret_sum[idx] = regret * neg_scale\n            for idx, value in enumerate(self.strategy_sum):\n                self.strategy_sum[idx] = value * strat_scale\n        self.last_dcfr_iter = iteration\n",
    "line_count": 51
  },
  {
    "id": "gh-space-shooter_src_gh_space_shooter_console_printer.py",
    "repo": "czl9707/gh-space-shooter",
    "url": "https://github.com/czl9707/gh-space-shooter/blob/main/src/gh_space_shooter/console_printer.py",
    "code": "\"\"\"Console output formatting and display functions.\"\"\"\n\nfrom rich.console import Console\nfrom rich.text import Text\n\nfrom .github_client import ContributionData\n\nconsole = Console()\n\nclass ContributionConsolePrinter:\n    def display_stats(self, data: ContributionData) -> None:\n        \"\"\"Display contribution statistics in a one-liner.\"\"\"\n        # Get date range\n        all_days = [day for week in data[\"weeks\"] for day in week[\"days\"]]\n        if all_days:\n            start_date = all_days[0][\"date\"]\n            end_date = all_days[-1][\"date\"]\n\n            console.print(\n                f\"\\n[bold green]âœ“[/bold green] @{data['username']}: \"\n                f\"{data['total_contributions']} contributions from {start_date} to {end_date}, \"\n                f\"{len(data['weeks'])} weeks in total.\\n\"\n            )\n\n    def display_contribution_graph(self, data: ContributionData) -> None:\n        \"\"\"Display a GitHub-style contribution graph.\"\"\"\n        weeks = data[\"weeks\"]\n        day_labels = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n\n        console.print(\"[bold]Contribution Graph:[/bold]\\n\")\n\n        for day_idx in range(7):  # 0=Sunday, 6=Saturday\n            console.print(f\"  {day_labels[day_idx]} \", end=\"\")\n\n            # Print colored blocks for this day across all weeks\n            for week in weeks:\n                \n                if day_idx < len(week[\"days\"]):\n                    day = week[\"days\"][day_idx]\n                    level = day[\"level\"]\n                else:\n                    level = 0\n                self._print_block(level)\n\n            console.print()  # New line after each day row\n\n        # Print legend\n        console.print(\"\\n  Less \", end=\"\")\n        for level in range(5):\n            self._print_block(level)\n            console.print(\"  \", end=\"\")\n        console.print(\"More\")\n\n    COLOR_MAP = {\n        0: \"\",        # Transparent\n        1: \"on rgb(0,109,50)\",           # Light green\n        2: \"on rgb(38,166,65)\",          # Medium green\n        3: \"on rgb(57,211,83)\",          # Bright green\n        4: \"on rgb(87,242,135)\",         # Very bright green\n    }\n\n    def _print_block(self, level: int) -> None:\n        \"\"\"Print a colored block based on contribution level.\"\"\"\n        text = Text(\"  \", style=self.COLOR_MAP.get(level, \"\"))\n        console.print(text, end=\"\")\n",
    "line_count": 65
  }
]