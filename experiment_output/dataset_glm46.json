[
  {
    "id": "mve-collection_src_minio-docker-delta_minio_delta.py",
    "repo": "raulcastillabravo/mve-collection",
    "url": "https://github.com/raulcastillabravo/mve-collection/blob/main/src/minio-docker-delta/minio_delta.py",
    "code": "import os\nfrom dotenv import load_dotenv\nfrom deltalake import DeltaTable, write_deltalake\nimport pandas as pd\nfrom typing import Optional, List\n\nload_dotenv()\n\n\nclass MinioDelta:\n    def __init__(self):\n        self.endpoint = os.getenv(\"MINIO_ENDPOINT\")\n        self.access_key = os.getenv(\"MINIO_ROOT_USER\")\n        self.secret_key = os.getenv(\"MINIO_ROOT_PASSWORD\")\n        self.bucket = os.getenv(\"BUCKET_NAME\")\n        \n        self.storage_options = {\n            \"AWS_ENDPOINT_URL\": self.endpoint,\n            \"AWS_ACCESS_KEY_ID\": self.access_key,\n            \"AWS_SECRET_ACCESS_KEY\": self.secret_key,\n            \"AWS_ALLOW_HTTP\": \"true\",\n            \"aws_conditional_put\": \"etag\",\n        }\n\n    def write(\n        self, \n        df: pd.DataFrame, \n        path: str,\n        mode: str = \"overwrite\",\n        partition_by: Optional[List[str]] = None,\n        predicate: Optional[str] = None\n    ) -> None:\n        write_deltalake(\n            table_or_uri=f\"s3://{self.bucket}/{path}\",\n            data=df,\n            mode=mode,\n            partition_by=partition_by,\n            predicate=predicate,\n            storage_options=self.storage_options\n        )\n\n    def read(\n        self, \n        path: str,\n        columns: Optional[List[str]] = None,\n        filters: Optional[List] = None\n    ) -> pd.DataFrame:\n        full_path = f\"s3://{self.bucket}/{path}\"\n        dt = DeltaTable(full_path, storage_options=self.storage_options)\n\n        return dt.to_pandas(columns=columns, filters=filters)\n",
    "line_count": 51
  },
  {
    "id": "powerrag_api_db_runtime_config.py",
    "repo": "oceanbase/powerrag",
    "url": "https://github.com/oceanbase/powerrag/blob/main/api/db/runtime_config.py",
    "code": "#\n#  Copyright 2024 The InfiniFlow Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom common.versions import get_ragflow_version\nfrom .reload_config_base import ReloadConfigBase\n\n\nclass RuntimeConfig(ReloadConfigBase):\n    DEBUG = None\n    WORK_MODE = None\n    HTTP_PORT = None\n    JOB_SERVER_HOST = None\n    JOB_SERVER_VIP = None\n    ENV = dict()\n    SERVICE_DB = None\n    LOAD_CONFIG_MANAGER = False\n\n    @classmethod\n    def init_config(cls, **kwargs):\n        for k, v in kwargs.items():\n            if hasattr(cls, k):\n                setattr(cls, k, v)\n\n    @classmethod\n    def init_env(cls):\n        cls.ENV.update({\"version\": get_ragflow_version()})\n\n    @classmethod\n    def load_config_manager(cls):\n        cls.LOAD_CONFIG_MANAGER = True\n\n    @classmethod\n    def get_env(cls, key):\n        return cls.ENV.get(key, None)\n\n    @classmethod\n    def get_all_env(cls):\n        return cls.ENV\n\n    @classmethod\n    def set_service_db(cls, service_db):\n        cls.SERVICE_DB = service_db\n",
    "line_count": 54
  },
  {
    "id": "sms-bridge_app_forwarder.py",
    "repo": "skylerhes/sms-bridge",
    "url": "https://github.com/skylerhes/sms-bridge/blob/main/app/forwarder.py",
    "code": "import logging\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\n\nlogger = logging.getLogger('forwarder')\n\nclass Forwarder:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.timeout = cfg.get('request_timeout', 10)\n        self._max_workers = cfg.get('forwarder_workers', 4)\n        self.executor = ThreadPoolExecutor(max_workers=self._max_workers)\n\n    def shutdown(self):\n        \"\"\"Stop background workers used for asynchronous forwarding.\"\"\"\n        if self.executor:\n            self.executor.shutdown(wait=True)\n            self.executor = None\n\n    def _ensure_executor(self):\n        \"\"\"Create a new executor when the previous one has been shut down.\"\"\"\n        if self.executor is None or getattr(self.executor, \"_shutdown\", False):\n            self.executor = ThreadPoolExecutor(max_workers=self._max_workers)\n\n    def send_telegram(self, remote, content):\n        if not self.cfg.get('telegram', {}).get('enabled'):\n            return\n        token = self.cfg['telegram']['bot_token']\n        chat_id = self.cfg['telegram']['chat_id']\n        text = f\"ðŸ“© From: {remote}\\n{content}\"\n        url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n        resp = requests.post(url, json={\"chat_id\": chat_id, \"text\": text}, timeout=self.timeout)\n        logger.info('Telegram status: %s', resp.status_code)\n        return resp.json()\n\n    def send_pushplus(self, remote, content):\n        if not self.cfg.get('wechat_pushplus', {}).get('enabled'):\n            return\n        token = self.cfg['wechat_pushplus']['token']\n        title = f\"SMS from {remote}\"\n        body = content\n        url = 'http://www.pushplus.plus/send'\n        resp = requests.post(url, json={\"token\": token, \"title\": title, \"content\": body}, timeout=self.timeout)\n        logger.info('PushPlus status: %s', resp.status_code)\n        return resp.json()\n\n    def forward(self, remote, content):\n        \"\"\"Dispatch forwarding tasks asynchronously to avoid blocking polling.\"\"\"\n\n        self._ensure_executor()\n\n        def _forward_all():\n            try:\n                self.send_telegram(remote, content)\n                self.send_pushplus(remote, content)\n            except Exception:\n                logger.exception(\"Forwarding failed for %s\", remote)\n\n        self.executor.submit(_forward_all)\n",
    "line_count": 60
  },
  {
    "id": "langchain_data_agent_src_data_agent_executors_base.py",
    "repo": "eosho/langchain_data_agent",
    "url": "https://github.com/eosho/langchain_data_agent/blob/main/src/data_agent/executors/base.py",
    "code": "\"\"\"Base interface for sandboxed code execution.\n\nThis module defines the abstract interface for code execution backends,\nproviding a consistent API for different isolation strategies.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any\n\n\nclass ExecutionStatus(Enum):\n    \"\"\"Status of code execution.\"\"\"\n\n    SUCCESS = \"success\"\n    ERROR = \"error\"\n    TIMEOUT = \"timeout\"\n\n\n@dataclass\nclass ExecutionResult:\n    \"\"\"Result of code execution in a sandbox.\n\n    Attributes:\n        status: Execution status (success, error, timeout).\n        output: Standard output from code execution.\n        error: Error message if execution failed.\n        files: Dictionary of generated files {filename: content_bytes}.\n        metadata: Additional execution metadata (timing, resource usage).\n    \"\"\"\n\n    status: ExecutionStatus\n    output: str = \"\"\n    error: str | None = None\n    files: dict[str, bytes] | None = None\n    metadata: dict[str, Any] = field(default_factory=dict)\n\n    @property\n    def success(self) -> bool:\n        \"\"\"Check if execution was successful.\"\"\"\n        return self.status == ExecutionStatus.SUCCESS\n\n\nclass CodeExecutor(ABC):\n    \"\"\"Abstract base class for code execution backends.\n\n    Implementations:\n        - AzureSessionsExecutor: Production (Hyper-V isolation via Azure Container Apps)\n        - LocalExecutor: Development (no sandboxing, uses exec())\n    \"\"\"\n\n    @abstractmethod\n    async def execute(self, code: str, timeout: float = 30.0) -> ExecutionResult:\n        \"\"\"Execute Python code in an isolated environment.\n\n        Args:\n            code: Python code to execute.\n            timeout: Maximum execution time in seconds.\n\n        Returns:\n            ExecutionResult with output, errors, and any generated files.\n        \"\"\"\n\n    async def cleanup(self) -> None:\n        \"\"\"Clean up any resources (sessions, containers).\n\n        Override in implementations that maintain state.\n        \"\"\"\n\n    async def __aenter__(self) -> \"CodeExecutor\":\n        \"\"\"Async context manager entry.\"\"\"\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: Any,\n    ) -> None:\n        \"\"\"Async context manager exit - cleanup resources.\"\"\"\n        await self.cleanup()\n",
    "line_count": 82
  },
  {
    "id": "openpi-comet_packages_openpi-client_src_openpi_client_action_chunk_broker.py",
    "repo": "mli0603/openpi-comet",
    "url": "https://github.com/mli0603/openpi-comet/blob/main/packages/openpi-client/src/openpi_client/action_chunk_broker.py",
    "code": "from typing import Dict\n\nimport numpy as np\nimport tree\nfrom typing_extensions import override\n\nfrom openpi_client import base_policy as _base_policy\n\n\nclass ActionChunkBroker(_base_policy.BasePolicy):\n    \"\"\"Wraps a policy to return action chunks one-at-a-time.\n\n    Assumes that the first dimension of all action fields is the chunk size.\n\n    A new inference call to the inner policy is only made when the current\n    list of chunks is exhausted.\n    \"\"\"\n\n    def __init__(self, policy: _base_policy.BasePolicy, action_horizon: int):\n        self._policy = policy\n        self._action_horizon = action_horizon\n        self._cur_step: int = 0\n\n        self._last_results: Dict[str, np.ndarray] | None = None\n\n    @override\n    def infer(self, obs: Dict) -> Dict:  # noqa: UP006\n        if self._last_results is None:\n            self._last_results = self._policy.infer(obs)\n            self._cur_step = 0\n\n        def slicer(x):\n            if isinstance(x, np.ndarray):\n                return x[self._cur_step, ...]\n            else:\n                return x\n\n        results = tree.map_structure(slicer, self._last_results)\n        self._cur_step += 1\n\n        if self._cur_step >= self._action_horizon:\n            self._last_results = None\n\n        return results\n\n    @override\n    def reset(self) -> None:\n        self._policy.reset()\n        self._last_results = None\n        self._cur_step = 0\n",
    "line_count": 50
  },
  {
    "id": "rf-mcp_src_robotmcp_models_browser_models.py",
    "repo": "manykarim/rf-mcp",
    "url": "https://github.com/manykarim/rf-mcp/blob/main/src/robotmcp/models/browser_models.py",
    "code": "\"\"\"Browser-related data models.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional\n\n\n@dataclass\nclass BrowserState:\n    \"\"\"Represents Browser Library and SeleniumLibrary state.\"\"\"\n    # Common browser state\n    browser_type: Optional[str] = None\n    current_url: Optional[str] = None\n    page_title: Optional[str] = None\n    viewport: Dict[str, int] = field(default_factory=lambda: {\"width\": 1280, \"height\": 720})\n    page_source: Optional[str] = None\n    aria_snapshot: Optional[Any] = None\n    aria_snapshot_format: Optional[str] = None\n    aria_snapshot_selector: Optional[str] = None\n    cookies: List[Dict[str, Any]] = field(default_factory=list)\n    local_storage: Dict[str, str] = field(default_factory=dict)\n    \n    # Browser Library specific state\n    browser_id: Optional[str] = None\n    context_id: Optional[str] = None\n    page_id: Optional[str] = None\n    \n    # SeleniumLibrary specific state\n    driver_instance: Optional[Any] = None\n    selenium_session_id: Optional[str] = None\n    \n    # Active library indicator (\"browser\" or \"selenium\" or None)\n    active_library: Optional[str] = None\n    session_storage: Dict[str, str] = field(default_factory=dict)\n    page_elements: List[Dict[str, Any]] = field(default_factory=list)\n    \n    def is_browser_library_active(self) -> bool:\n        \"\"\"Check if Browser Library is the active library.\"\"\"\n        return self.active_library == \"browser\"\n    \n    def is_selenium_library_active(self) -> bool:\n        \"\"\"Check if SeleniumLibrary is the active library.\"\"\"\n        return self.active_library == \"selenium\"\n    \n    def has_browser_session(self) -> bool:\n        \"\"\"Check if there's an active browser session.\"\"\"\n        return (self.browser_id is not None or \n                self.driver_instance is not None)\n    \n    def has_page_loaded(self) -> bool:\n        \"\"\"Check if a page is currently loaded.\"\"\"\n        return (self.page_id is not None or \n                self.current_url is not None)\n    \n    def reset(self) -> None:\n        \"\"\"Reset browser state to initial values.\"\"\"\n        self.browser_type = None\n        self.current_url = None\n        self.page_title = None\n        self.page_source = None\n        self.aria_snapshot = None\n        self.aria_snapshot_format = None\n        self.aria_snapshot_selector = None\n        self.cookies.clear()\n        self.local_storage.clear()\n        self.browser_id = None\n        self.context_id = None\n        self.page_id = None\n        self.driver_instance = None\n        self.selenium_session_id = None\n        self.active_library = None\n        self.session_storage.clear()\n        self.page_elements.clear()\n",
    "line_count": 72
  },
  {
    "id": "agentready_src_agentready_fixers_base.py",
    "repo": "ambient-code/agentready",
    "url": "https://github.com/ambient-code/agentready/blob/main/src/agentready/fixers/base.py",
    "code": "\"\"\"Base fixer interface for automated remediation.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\nfrom ..models.finding import Finding\nfrom ..models.fix import Fix\nfrom ..models.repository import Repository\n\n\nclass BaseFixer(ABC):\n    \"\"\"Abstract base class for all attribute fixers.\n\n    Each fixer knows how to automatically remediate a specific failing attribute\n    by generating files, modifying configurations, or executing commands.\n\n    Fixers follow the strategy pattern and are stateless for easy testing.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def attribute_id(self) -> str:\n        \"\"\"Unique attribute identifier (e.g., 'claude_md_file').\n\n        Must match the attribute ID from assessors.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def can_fix(self, finding: Finding) -> bool:\n        \"\"\"Check if this fixer can fix the given finding.\n\n        Args:\n            finding: Assessment finding for the attribute\n\n        Returns:\n            True if this fixer can generate a fix, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate_fix(self, repository: Repository, finding: Finding) -> Optional[Fix]:\n        \"\"\"Generate a fix for the failing attribute.\n\n        Args:\n            repository: Repository entity with path, languages, metadata\n            finding: Failing finding to remediate\n\n        Returns:\n            Fix object if one can be generated, None if cannot be fixed automatically\n\n        Raises:\n            This method should NOT raise exceptions. Return None on errors.\n        \"\"\"\n        pass\n\n    def estimate_score_improvement(self, finding: Finding) -> float:\n        \"\"\"Estimate score points gained if fix is applied.\n\n        Args:\n            finding: Failing finding\n\n        Returns:\n            Estimated points (0-100) that would be gained\n\n        Default implementation: Use attribute default_weight from finding.\n        \"\"\"\n        if finding.status == \"fail\" and finding.attribute.default_weight:\n            # Full weight if currently failing (0 points)\n            return finding.attribute.default_weight * 100\n        return 0.0\n",
    "line_count": 71
  },
  {
    "id": "aicon_backend_src_services_base.py",
    "repo": "869413421/aicon",
    "url": "https://github.com/869413421/aicon/blob/main/backend/src/services/base.py",
    "code": "\"\"\"\næœåŠ¡åŸºç±» - æä¾›ç»Ÿä¸€çš„æ•°æ®åº“ä¼šè¯ç®¡ç†å’ŒåŸºç¡€åŠŸèƒ½\n\"\"\"\n\nfrom typing import Optional, TYPE_CHECKING\n\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.core.database import AsyncSessionLocal\nfrom src.core.logging import get_logger\n\nif TYPE_CHECKING:\n    from typing import Any, Dict, Optional\n\nlogger = get_logger(__name__)\n\n\nclass BaseService:\n    \"\"\"\n    æœåŠ¡åŸºç±»\n    è¦æ±‚å¤–éƒ¨æ³¨å…¥ AsyncSessionã€‚\n    \"\"\"\n\n    def __init__(self, db_session: AsyncSession):\n        \"\"\"\n        åˆå§‹åŒ–æœåŠ¡å®žä¾‹\n        Args:\n            db_session: å¿…é¡»æä¾›å¼‚æ­¥æ•°æ®åº“ä¼šè¯\n        \"\"\"\n        self._db_session = db_session\n\n    @property\n    def db_session(self) -> AsyncSession:\n        \"\"\"èŽ·å–å¹¶éªŒè¯å½“å‰ç»‘å®šçš„æ•°æ®åº“ä¼šè¯\"\"\"\n        if self._db_session is None:\n            raise RuntimeError(f\"{self.__class__.__name__} å°šæœªç»‘å®šæ•°æ®åº“ä¼šè¯\")\n        return self._db_session\n\n    async def commit(self):\n        \"\"\"æäº¤å½“å‰äº‹åŠ¡\"\"\"\n        await self.db_session.commit()\n\n    async def rollback(self):\n        \"\"\"å›žæ»šå½“å‰äº‹åŠ¡\"\"\"\n        await self.db_session.rollback()\n\n    async def flush(self):\n        \"\"\"åˆ·æ–°å½“å‰ä¼šè¯\"\"\"\n        await self.db_session.flush()\n\n    async def refresh(self, obj):\n        \"\"\"åˆ·æ–°å¯¹è±¡æ•°æ®\"\"\"\n        await self.db_session.refresh(obj)\n\n    async def execute(self, query, params: Optional[dict] = None):\n        \"\"\"æ‰§è¡ŒSQLæŸ¥è¯¢\"\"\"\n        return await self.db_session.execute(query, params)\n\n    def add(self, obj):\n        \"\"\"æ·»åŠ å¯¹è±¡åˆ°ä¼šè¯\"\"\"\n        self.db_session.add(obj)\n\n    def delete(self, obj):\n        \"\"\"ä»Žä¼šè¯ä¸­åˆ é™¤å¯¹è±¡\"\"\"\n        self.db_session.delete(obj)\n\n    async def get(self, model_class, identifier):\n        \"\"\"æ ¹æ®IDèŽ·å–å¯¹è±¡\"\"\"\n        return await self.db_session.get(model_class, identifier)\n\n\n__all__ = [\"BaseService\"]\n",
    "line_count": 72
  },
  {
    "id": "sdnq_src_sdnq_optim_lion.py",
    "repo": "Disty0/sdnq",
    "url": "https://github.com/Disty0/sdnq/blob/main/src/sdnq/optim/lion.py",
    "code": "from typing import Tuple, Iterator\n\nimport torch\n\nfrom ..training import SDNQTensor\nfrom ..common import compile_func\n\nfrom .optimizer import SDNQOptimizer\nfrom .utils import lerp_buffer_stochastic_\n\n\nclass Lion(SDNQOptimizer):\n    _extra_group_keys = {}\n    _keep_in_fp32_keys = {}\n    _group_keys = set.union(SDNQOptimizer._base_group_keys, _extra_group_keys)\n\n    def __init__(self, params, **kwargs):\n        if isinstance(params, (torch.nn.Parameter, Iterator)) or (isinstance(params, (list, tuple)) and isinstance(params[0], torch.nn.Parameter)):\n            kwargs[\"params\"] = params\n            param_groups = [kwargs,]\n        else:\n            param_groups = params\n        for group in param_groups:\n            group = self.apply_group_defaults(group, **kwargs)\n            assert set(group.keys()) == self._group_keys\n        super().__init__(param_groups, dict())\n        self.keep_in_fp32_keys = {}\n\n    @torch.no_grad()\n    def init_state(self, param: torch.Tensor, group: dict, state: dict) -> dict:\n        use_quantized_buffers = group[\"use_quantized_buffers\"] and param.grad.ndim >= group[\"quantized_buffers_minimum_ndim\"] and param.grad.numel() >= group[\"quantized_buffers_minimum_numel\"]\n        if use_quantized_buffers:\n            state[\"exp_avg\"] = SDNQTensor.from_float(torch.zeros_like(param, dtype=torch.float32), weights_dtype=group[\"quantized_buffers_dtype\"], group_size=group[\"quantized_buffers_group_size\"], svd_rank=group[\"quantized_buffers_svd_rank\"], use_svd=group[\"use_svd_quantization\"], use_stochastic_rounding=group[\"use_stochastic_buffers\"])\n        else:\n            state[\"exp_avg\"] = torch.zeros_like(param)\n        return state\n\n    @torch.no_grad()\n    def get_param_update(self, param_fp32: torch.FloatTensor, grad: torch.FloatTensor, group: dict, state: dict) -> torch.FloatTensor:\n        update_func = lion_update_compiled if group[\"use_torch_compile\"] else lion_update\n        return update_func(\n            grad=grad,\n            exp_avg=state[\"exp_avg\"],\n            betas=group[\"betas\"],\n            use_stochastic_buffers=group[\"use_stochastic_buffers\"],\n        )\n\n\ndef lion_update(\n    grad: torch.FloatTensor,\n    exp_avg: torch.FloatTensor,\n    betas: Tuple[float, float],\n    use_stochastic_buffers: bool = False,\n) -> torch.FloatTensor:\n    beta1, beta2 = betas\n    update = exp_avg.to(dtype=torch.float32).lerp(grad, 1 - beta1).sign_()\n    lerp_buffer_stochastic_(exp_avg, grad, 1 - beta2, use_stochastic_rounding=use_stochastic_buffers)\n    return update\n\n\nlion_update_compiled = compile_func(lion_update)\n",
    "line_count": 61
  },
  {
    "id": "myfy_packages_myfy-data_myfy_data_session.py",
    "repo": "psincraian/myfy",
    "url": "https://github.com/psincraian/myfy/blob/main/packages/myfy-data/myfy/data/session.py",
    "code": "\"\"\"\nDatabase session management.\n\nProvides async session factory and context managers for database operations.\n\"\"\"\n\nfrom collections.abc import AsyncIterator\nfrom contextlib import asynccontextmanager\n\nfrom sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker\n\n# Note: AsyncIterator is still used by session_context method\n\n\nclass SessionFactory:\n    \"\"\"\n    Factory for creating async database sessions.\n\n    This class wraps SQLAlchemy's async_sessionmaker and provides\n    a clean interface for session creation.\n    \"\"\"\n\n    def __init__(self, sessionmaker: async_sessionmaker[AsyncSession]):\n        \"\"\"\n        Initialize session factory.\n\n        Args:\n            sessionmaker: SQLAlchemy async sessionmaker instance\n        \"\"\"\n        self._sessionmaker = sessionmaker\n\n    def create_session(self) -> AsyncSession:\n        \"\"\"\n        Create a new async session.\n\n        Returns:\n            AsyncSession instance\n        \"\"\"\n        return self._sessionmaker()\n\n    @asynccontextmanager\n    async def session_context(self) -> AsyncIterator[AsyncSession]:\n        \"\"\"\n        Context manager for automatic session lifecycle.\n\n        Automatically commits on success, rolls back on exception,\n        and closes the session in all cases.\n\n        Example:\n            async with session_factory.session_context() as session:\n                result = await session.execute(query)\n                await session.commit()\n\n        Yields:\n            AsyncSession instance\n        \"\"\"\n        session = self.create_session()\n        try:\n            yield session\n            await session.commit()\n        except Exception:\n            await session.rollback()\n            raise\n        finally:\n            await session.close()\n\n\ndef get_session_for_request(session_factory: SessionFactory) -> AsyncSession:\n    \"\"\"\n    Dependency provider for REQUEST-scoped database sessions.\n\n    This function is registered as a REQUEST-scoped provider in the DI container.\n    Each HTTP request gets its own session. The session lifecycle (commit/rollback/close)\n    is managed by the ASGI adapter via cleanup callbacks.\n\n    Args:\n        session_factory: SessionFactory injected from DI container\n\n    Returns:\n        AsyncSession instance for the current request\n    \"\"\"\n    return session_factory.create_session()\n",
    "line_count": 82
  },
  {
    "id": "stock-mcp_src_server_infrastructure_cache_redis_cache.py",
    "repo": "huweihua123/stock-mcp",
    "url": "https://github.com/huweihua123/stock-mcp/blob/feature/mcp-ddd-refactor-v2/src/server/infrastructure/cache/redis_cache.py",
    "code": "# src/server/infrastructure/cache/redis_cache.py\n\"\"\"Async cache wrapper using aiocache with Redis backend.\nAll services can use `cache.get/set` without worrying about client details.\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import date, datetime\nfrom typing import Any, Optional\n\nimport aiocache\nfrom aiocache import Cache\nfrom aiocache.serializers import BaseSerializer\nfrom src.server.infrastructure.connections.redis_connection import RedisConnection\n\nlogger = logging.getLogger(__name__)\n\n\nclass DateAwareJsonSerializer(BaseSerializer):\n    \"\"\"JSON serializer that handles date and datetime objects.\"\"\"\n    \n    DEFAULT_ENCODING = \"utf-8\"\n    \n    def _default(self, obj):\n        if isinstance(obj, datetime):\n            return {\"__datetime__\": obj.isoformat()}\n        elif isinstance(obj, date):\n            return {\"__date__\": obj.isoformat()}\n        raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n    \n    def _object_hook(self, dct):\n        if \"__datetime__\" in dct:\n            return datetime.fromisoformat(dct[\"__datetime__\"])\n        if \"__date__\" in dct:\n            return date.fromisoformat(dct[\"__date__\"])\n        return dct\n    \n    def dumps(self, value: Any) -> str:\n        return json.dumps(value, default=self._default)\n    \n    def loads(self, value: Optional[str]) -> Any:\n        if value is None:\n            return None\n        return json.loads(value, object_hook=self._object_hook)\n\n\nclass AsyncRedisCache:\n    def __init__(self, redis_client: RedisConnection, ttl_default: int = 300):\n        # Ensure the underlying Redis connection is established\n        self._redis_conn = redis_client\n        self._ttl_default = ttl_default\n        # aiocache will use the same Redis URL with custom serializer\n        self._cache = Cache(\n            Cache.REDIS,\n            endpoint=redis_client.config.get(\"host\", \"localhost\"),\n            port=redis_client.config.get(\"port\", 6379),\n            db=redis_client.config.get(\"db\", 0),\n            password=redis_client.config.get(\"password\"),\n            ttl=self._ttl_default,\n            serializer=DateAwareJsonSerializer(),\n        )\n\n    async def get(self, key: str) -> Optional[Any]:\n        try:\n            return await self._cache.get(key)\n        except Exception as e:\n            logger.error(f\"âŒ Cache get error for {key}: {e}\")\n            return None\n\n    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:\n        try:\n            await self._cache.set(key, value, ttl=ttl or self._ttl_default)\n            return True\n        except Exception as e:\n            logger.error(f\"âŒ Cache set error for {key}: {e}\")\n            return False\n\n    async def delete(self, key: str) -> bool:\n        try:\n            await self._cache.delete(key)\n            return True\n        except Exception as e:\n            logger.error(f\"âŒ Cache delete error for {key}: {e}\")\n            return False\n",
    "line_count": 84
  },
  {
    "id": "agentscope-runtime_src_agentscope_runtime_sandbox_enums.py",
    "repo": "agentscope-ai/agentscope-runtime",
    "url": "https://github.com/agentscope-ai/agentscope-runtime/blob/main/src/agentscope_runtime/sandbox/enums.py",
    "code": "# -*- coding: utf-8 -*-\nfrom enum import Enum, EnumMeta\n\n\nclass DynamicEnumMeta(EnumMeta):\n    def __new__(\n        metacls,\n        cls,\n        bases,\n        classdict,\n        **kwds,\n    ):  # pylint: disable=bad-mcs-classmethod-argument\n        enum_class = super().__new__(metacls, cls, bases, classdict, **kwds)\n        for member in enum_class:\n            member.builtin = True\n        return enum_class\n\n\nclass DynamicEnum(Enum, metaclass=DynamicEnumMeta):\n    def __init__(self, value):  # pylint: disable=unused-argument\n        self.builtin = True\n\n    @classmethod\n    def add_member(cls, name: str, value=None):\n        if name in cls.__members__:\n            raise ValueError(f\"Member '{name}' already exists.\")\n\n        if value is None:\n            value = name.lower()\n\n        # Add new member\n        new_member = cls._create_pseudo_member(name, value)\n        new_member.builtin = False\n        cls._member_map_[name] = new_member\n        cls._value2member_map_[value] = new_member\n        # Update ordered members\n        cls._member_names_.append(name)\n\n    @classmethod\n    def _create_pseudo_member(cls, name, value):\n        temp = object.__new__(cls)\n        temp._value_ = value\n        temp._name_ = name\n        temp.__objclass__ = cls\n        return temp\n\n    @classmethod\n    def get_builtin_members(cls):\n        return [member for member in cls if getattr(member, \"builtin\", False)]\n\n    @classmethod\n    def get_dynamic_members(cls):\n        return [\n            member for member in cls if not getattr(member, \"builtin\", False)\n        ]\n\n    def is_builtin(self):\n        return getattr(self, \"builtin\", False)\n\n\nclass SandboxType(DynamicEnum):\n    \"\"\"Sandbox type enumeration\"\"\"\n\n    DUMMY = \"dummy\"\n    BASE = \"base\"\n    BROWSER = \"browser\"\n    FILESYSTEM = \"filesystem\"\n    GUI = \"gui\"\n    MOBILE = \"mobile\"\n    APPWORLD = \"appworld\"\n    BFCL = \"bfcl\"\n    AGENTBAY = \"agentbay\"\n\n    # Async sandbox\n    BASE_ASYNC = \"base_async\"\n    BROWSER_ASYNC = \"browser_async\"\n    FILESYSTEM_ASYNC = \"filesystem_async\"\n    GUI_ASYNC = \"gui_async\"\n    MOBILE_ASYNC = \"mobile_async\"\n",
    "line_count": 79
  },
  {
    "id": "sora2-watermark-remover-gui_core_inpainting.py",
    "repo": "timanmoh/sora2-watermark-remover-gui",
    "url": "https://github.com/timanmoh/sora2-watermark-remover-gui/blob/main/core/inpainting.py",
    "code": "from detection import signature\r\nimport random\r\nimport time\r\n\r\n\r\nclass InpaintingEngine:\r\n    def __init__(self):\r\n        self.model_path = None\r\n        self.device = \"cpu\"\r\n        self.loaded = False\r\n        \r\n    def load_model(self, model_type=\"transformer\"):\r\n        time.sleep(random.uniform(1.0, 2.0))\r\n        \r\n        errors = [\r\n            \"Model checkpoint not found: transformer_inpainting_v2.pth\",\r\n            \"Version mismatch: Expected PyTorch 2.1.0, found 2.0.1\",\r\n            \"ONNX runtime initialization failed\",\r\n            \"Model architecture incompatible with current hardware\"\r\n        ]\r\n        \r\n        raise RuntimeError(random.choice(errors))\r\n        \r\n    def inpaint_frame(self, frame, mask):\r\n        if not self.loaded:\r\n            raise Exception(\"Model not loaded\")\r\n            \r\n        time.sleep(random.uniform(0.05, 0.15))\r\n        \r\n        if random.random() > 0.5:\r\n            raise MemoryError(\"Tensor allocation failed: Out of memory\")\r\n        else:\r\n            raise RuntimeError(\"Inpainting iteration failed: NaN values detected\")\r\n            \r\n    def inpaint_video(self, input_path, output_path, mask_regions):\r\n        try:\r\n            time.sleep(0.3)\r\n            \r\n            raise Exception(\"Frame extraction failed: FFmpeg error code 1\")\r\n            \r\n        except Exception as e:\r\n            raise Exception(f\"Video inpainting error: {str(e)}\")\r\n            \r\n    def optimize_for_gpu(self):\r\n        time.sleep(0.5)\r\n        \r\n        raise RuntimeError(\"GPU optimization failed: CUDA kernels not compiled\")\r\n        \r\n    def set_quality_preset(self, preset):\r\n        valid_presets = [\"fast\", \"balanced\", \"high_quality\"]\r\n        \r\n        if preset.lower() not in valid_presets:\r\n            raise ValueError(f\"Invalid preset: {preset}\")\r\n            \r\n        time.sleep(0.2)\r\n        \r\n        return True\r\n",
    "line_count": 57
  },
  {
    "id": "Ragnar_epd_helper.py",
    "repo": "PierreGode/Ragnar",
    "url": "https://github.com/PierreGode/Ragnar/blob/main/epd_helper.py",
    "code": "# epd_helper.py\n\nimport importlib\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass EPDHelper:\n    def __init__(self, epd_type):\n        self.epd_type = epd_type\n        self.epd = self._load_epd_module()\n\n    def _load_epd_module(self):\n        try:\n            epd_module_name = f'resources.waveshare_epd.{self.epd_type}'\n            epd_module = importlib.import_module(epd_module_name)\n            return epd_module.EPD()\n        except ImportError as e:\n            logger.error(f\"EPD module {self.epd_type} not found: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Error loading EPD module {self.epd_type}: {e}\")\n            raise\n\n    def init_full_update(self):\n        try:\n            if hasattr(self.epd, 'FULL_UPDATE'):\n                self.epd.init(self.epd.FULL_UPDATE)\n            elif hasattr(self.epd, 'lut_full_update'):\n                self.epd.init(self.epd.lut_full_update)\n            else:\n                self.epd.init()\n            logger.info(\"EPD full update initialization complete.\")\n        except Exception as e:\n            logger.error(f\"Error initializing EPD for full update: {e}\")\n            raise\n\n    def init_partial_update(self):\n        try:\n            if hasattr(self.epd, 'PART_UPDATE'):\n                self.epd.init(self.epd.PART_UPDATE)\n            elif hasattr(self.epd, 'lut_partial_update'):\n                self.epd.init(self.epd.lut_partial_update)\n            else:\n                self.epd.init()\n            logger.info(\"EPD partial update initialization complete.\")\n        except Exception as e:\n            logger.error(f\"Error initializing EPD for partial update: {e}\")\n            raise\n\n    def display_partial(self, image):\n        try:\n            if hasattr(self.epd, 'displayPartial'):\n                self.epd.displayPartial(self.epd.getbuffer(image))\n            else:\n                self.epd.display(self.epd.getbuffer(image))\n            logger.info(\"Partial display update complete.\")\n        except Exception as e:\n            logger.error(f\"Error during partial display update: {e}\")\n            raise\n\n    def clear(self):\n        try:\n            self.epd.Clear()\n            logger.info(\"EPD cleared.\")\n        except Exception as e:\n            logger.error(f\"Error clearing EPD: {e}\")\n            raise\n\n    def display_full(self, image):\n        \"\"\"Display image on EPD using full update.\"\"\"\n        try:\n            self.epd.display(self.epd.getbuffer(image))\n            logger.info(\"Full display update complete.\")\n        except Exception as e:\n            logger.error(f\"Error during full display update: {e}\")\n            raise\n\n    def sleep(self):\n        \"\"\"Put EPD to sleep mode.\"\"\"\n        try:\n            self.epd.sleep()\n            logger.info(\"EPD sleep mode activated.\")\n        except Exception as e:\n            logger.error(f\"Error putting EPD to sleep: {e}\")\n            raise",
    "line_count": 86
  },
  {
    "id": "DeepV-Ki_api_cost_tracker.py",
    "repo": "OrionStarAI/DeepV-Ki",
    "url": "https://github.com/OrionStarAI/DeepV-Ki/blob/main/api/cost_tracker.py",
    "code": "import logging\nfrom typing import Dict, Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n# Global dictionary to store cost trackers in memory\n_cost_trackers: Dict[str, 'CostTracker'] = {}\n\nclass CostTracker:\n    \"\"\"\n    Tracks costs for a specific task (e.g., Wiki generation).\n    Accumulates LLM and embedding costs.\n    \"\"\"\n    def __init__(self, task_id: str):\n        self.task_id = task_id\n        self.embedding_tokens = 0\n        self.embedding_cost = 0.0\n        self.llm_tokens = 0\n        self.llm_cost = 0.0\n\n    def add_embedding_cost(self, tokens: int, cost: float):\n        \"\"\"Add embedding usage and cost.\"\"\"\n        self.embedding_tokens += tokens\n        self.embedding_cost += cost\n\n    def add_llm_cost(self, prompt_tokens: int, completion_tokens: int, total_tokens: int, cost: float):\n        \"\"\"Add LLM usage and cost.\"\"\"\n        self.llm_tokens += total_tokens\n        self.llm_cost += cost\n\n    def get_total_cost(self) -> float:\n        \"\"\"Get total accumulated cost.\"\"\"\n        return self.embedding_cost + self.llm_cost\n\n    def get_cost_message(self) -> str:\n        \"\"\"\n        Generate a message summarizing the cost.\n        This is used to update the task status message.\n        \"\"\"\n        total_cost = self.get_total_cost()\n        total_tokens = self.llm_tokens + self.embedding_tokens\n\n        msg = \"Wiki generation completed successfully!\"\n\n        if total_cost > 0:\n            msg += f\" Total cost: ${total_cost:.5f} (LLM: ${self.llm_cost:.5f}, Embedding: ${self.embedding_cost:.5f})\"\n\n        msg += f\" Total tokens: {total_tokens} (LLM: {self.llm_tokens}, Embedding: {self.embedding_tokens})\"\n\n        return msg\n\n    def log_summary(self):\n        \"\"\"Log a summary of the costs.\"\"\"\n        total_tokens = self.llm_tokens + self.embedding_tokens\n        logger.info(f\"[Task {self.task_id}] Cost Summary: Total=${self.get_total_cost():.5f}, Tokens={total_tokens} (LLM={self.llm_tokens}, Embedding={self.embedding_tokens})\")\n\ndef get_cost_tracker(task_id: str) -> CostTracker:\n    \"\"\"Get or create a cost tracker for a task.\"\"\"\n    if task_id not in _cost_trackers:\n        _cost_trackers[task_id] = CostTracker(task_id)\n    return _cost_trackers[task_id]\n\ndef clear_cost_tracker(task_id: str):\n    \"\"\"Remove a cost tracker from memory.\"\"\"\n    if task_id in _cost_trackers:\n        del _cost_trackers[task_id]",
    "line_count": 66
  },
  {
    "id": "linux-desktop-gremlin_src_fsm_walk_manager.py",
    "repo": "iluvgirlswithglasses/linux-desktop-gremlin",
    "url": "https://github.com/iluvgirlswithglasses/linux-desktop-gremlin/blob/main/src/fsm/walk_manager.py",
    "code": "from PySide6.QtCore import Qt\nfrom PySide6.QtGui import QKeyEvent\n\nfrom ..settings import Preferences\nfrom ..states import Direction\n\nDirectionMap = {\n    (+0, +0): Direction.NONE,\n    (-1, +0): Direction.UP,\n    (+1, +0): Direction.DOWN,\n    (+0, -1): Direction.LEFT,\n    (+0, +1): Direction.RIGHT,\n    (-1, -1): Direction.UP_LEFT,\n    (-1, +1): Direction.UP_RIGHT,\n    (+1, -1): Direction.DOWN_LEFT,\n    (+1, +1): Direction.DOWN_RIGHT,\n}\n\n\nclass WalkManager:\n    def __init__(self):\n        # state of movement keys\n        self.w = False\n        self.a = False\n        self.s = False\n        self.d = False\n\n        # move speed (pixel per frame)\n        self.v = Preferences.MoveSpeed\n\n    \"\"\"\n    @! ---- Movement Resolves ----------------------------------------------------------------------\n    \"\"\"\n\n    def get_velocity(self) -> tuple[int, int]:\n        \"\"\"\n        Returns the velocity vector based on the current key states.\n        If both keys in a direction are pressed, they cancel each other out.\n        \"\"\"\n        vy = 0\n        vx = 0\n        if self.w ^ self.s:\n            vy = -self.v if self.w else self.v\n        if self.a ^ self.d:\n            vx = -self.v if self.a else self.v\n        return vx, vy\n\n    def is_moving(self) -> bool:\n        \"\"\"\n        Returns True if either vertical or horizontal movement is occurring.\n        \"\"\"\n        return (self.w ^ self.s) or (self.a ^ self.d)\n\n    def get_direction(self):\n        \"\"\"\n        Returns a string representing the current movement direction for animation purposes.\n        \"\"\"\n        ver = 0\n        hor = 0\n        if self.w ^ self.s:\n            ver = -1 if self.w else 1\n        if self.a ^ self.d:\n            hor = -1 if self.a else 1\n        return DirectionMap[(ver, hor)]\n\n    \"\"\"\n    @! ---- Event Recorders ------------------------------------------------------------------------\n    \"\"\"\n\n    def record_key_press(self, event: QKeyEvent):\n        key = event.key()\n        match key:\n            case Qt.Key.Key_W:\n                self.w = True\n            case Qt.Key.Key_A:\n                self.a = True\n            case Qt.Key.Key_S:\n                self.s = True\n            case Qt.Key.Key_D:\n                self.d = True\n            case _:\n                pass\n\n    def record_key_release(self, event: QKeyEvent):\n        key = event.key()\n        match key:\n            case Qt.Key.Key_W:\n                self.w = False\n            case Qt.Key.Key_A:\n                self.a = False\n            case Qt.Key.Key_S:\n                self.s = False\n            case Qt.Key.Key_D:\n                self.d = False\n            case _:\n                pass\n\n    def record_mouse_leave(self):\n        # stop all movement when mouse leaves window\n        self.w = False\n        self.a = False\n        self.s = False\n        self.d = False\n",
    "line_count": 103
  },
  {
    "id": "multi-agent-marketplace_packages_magentic-marketplace_src_magentic_marketplace_platform_client_resources_base.py",
    "repo": "microsoft/multi-agent-marketplace",
    "url": "https://github.com/microsoft/multi-agent-marketplace/blob/main/packages/magentic-marketplace/src/magentic_marketplace/platform/client/resources/base.py",
    "code": "\"\"\"Base resource class with agent ID handling.\"\"\"\n\nfrom typing import Any\n\nfrom ..base import BaseClient\n\n\nclass BaseResource:\n    \"\"\"Base class for API resources with agent ID support.\"\"\"\n\n    def __init__(self, base_client: BaseClient):\n        \"\"\"Initialize resource with base client.\n\n        Args:\n            base_client: The BaseClient instance for making HTTP requests\n\n        \"\"\"\n        self._base_client = base_client\n        self._agent_id: str | None = None\n\n    def set_agent_id(self, agent_id: str) -> None:\n        \"\"\"Set the agent ID for requests.\n\n        Args:\n            agent_id: The agent ID to use\n\n        \"\"\"\n        self._agent_id = agent_id\n\n    @property\n    def agent_id(self) -> str | None:\n        \"\"\"Get the current agent ID.\n\n        Returns:\n            str | None: The agent ID or None if not set\n\n        \"\"\"\n        return self._agent_id\n\n    async def request(\n        self,\n        method: str,\n        path: str,\n        params: Any = None,\n        json_data: dict[str, Any] | None = None,\n        headers: dict[str, str] | None = None,\n    ) -> dict[str, Any]:\n        \"\"\"Make an HTTP request with automatic agent ID injection.\n\n        Args:\n            method: HTTP method (GET, POST, etc.)\n            path: API endpoint path\n            params: Optional query parameters\n            json_data: Optional JSON body\n            headers: Optional additional headers\n\n        Returns:\n            dict: Parsed JSON response\n\n        \"\"\"\n        # Add agent ID to headers if set\n        request_headers = headers or {}\n        if self._agent_id:\n            request_headers[\"X-Agent-Id\"] = self._agent_id\n\n        return await self._base_client.request(\n            method, path, params, json_data, request_headers\n        )\n",
    "line_count": 68
  },
  {
    "id": "gh-space-shooter_src_gh_space_shooter_console_printer.py",
    "repo": "czl9707/gh-space-shooter",
    "url": "https://github.com/czl9707/gh-space-shooter/blob/main/src/gh_space_shooter/console_printer.py",
    "code": "\"\"\"Console output formatting and display functions.\"\"\"\n\nfrom rich.console import Console\nfrom rich.text import Text\n\nfrom .github_client import ContributionData\n\nconsole = Console()\n\nclass ContributionConsolePrinter:\n    def display_stats(self, data: ContributionData) -> None:\n        \"\"\"Display contribution statistics in a one-liner.\"\"\"\n        # Get date range\n        all_days = [day for week in data[\"weeks\"] for day in week[\"days\"]]\n        if all_days:\n            start_date = all_days[0][\"date\"]\n            end_date = all_days[-1][\"date\"]\n\n            console.print(\n                f\"\\n[bold green]âœ“[/bold green] @{data['username']}: \"\n                f\"{data['total_contributions']} contributions from {start_date} to {end_date}, \"\n                f\"{len(data['weeks'])} weeks in total.\\n\"\n            )\n\n    def display_contribution_graph(self, data: ContributionData) -> None:\n        \"\"\"Display a GitHub-style contribution graph.\"\"\"\n        weeks = data[\"weeks\"]\n        day_labels = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n\n        console.print(\"[bold]Contribution Graph:[/bold]\\n\")\n\n        for day_idx in range(7):  # 0=Sunday, 6=Saturday\n            console.print(f\"  {day_labels[day_idx]} \", end=\"\")\n\n            # Print colored blocks for this day across all weeks\n            for week in weeks:\n                \n                if day_idx < len(week[\"days\"]):\n                    day = week[\"days\"][day_idx]\n                    level = day[\"level\"]\n                else:\n                    level = 0\n                self._print_block(level)\n\n            console.print()  # New line after each day row\n\n        # Print legend\n        console.print(\"\\n  Less \", end=\"\")\n        for level in range(5):\n            self._print_block(level)\n            console.print(\"  \", end=\"\")\n        console.print(\"More\")\n\n    COLOR_MAP = {\n        0: \"\",        # Transparent\n        1: \"on rgb(0,109,50)\",           # Light green\n        2: \"on rgb(38,166,65)\",          # Medium green\n        3: \"on rgb(57,211,83)\",          # Bright green\n        4: \"on rgb(87,242,135)\",         # Very bright green\n    }\n\n    def _print_block(self, level: int) -> None:\n        \"\"\"Print a colored block based on contribution level.\"\"\"\n        text = Text(\"  \", style=self.COLOR_MAP.get(level, \"\"))\n        console.print(text, end=\"\")\n",
    "line_count": 65
  },
  {
    "id": "cosmos-transfer2.5_packages_cosmos-gradio_sample_sample_worker.py",
    "repo": "nvidia-cosmos/cosmos-transfer2.5",
    "url": "https://github.com/nvidia-cosmos/cosmos-transfer2.5/blob/main/packages/cosmos-gradio/sample/sample_worker.py",
    "code": "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\n\nfrom cosmos_gradio.deployment_env import DeploymentEnv\nfrom cosmos_gradio.model_ipc.model_worker import ModelWorker\nfrom PIL import Image\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass InferenceParameters(BaseModel):\n    \"\"\"Parameters for inference requests.\"\"\"\n\n    prompt: str = Field(..., min_length=1)\n    \"\"\"Text prompt for generation\"\"\"\n    num_steps: int = Field(..., gt=0)\n    \"\"\"Number of inference steps\"\"\"\n    input_image: str | None = None\n    \"\"\"Path to the input image\"\"\"\n\n    \"\"\"use_attribute_docstrings=True to use the docstrings as the description of the fields\"\"\"\n    model_config = ConfigDict(use_attribute_docstrings=True)\n\n\nclass SampleWorker(ModelWorker):\n    def __init__(self, num_gpus, model_name):\n        pass\n\n    # pyrefly: ignore  # bad-override\n    def infer(self, args: dict):\n        prompt = args.get(\"prompt\", \"\")\n\n        img = Image.new(\"RGB\", (256, 256), color=\"red\")\n        output_dir = args.get(\"output_dir\", \"/mnt/pvc/gradio_output\")\n        out_file_name = os.path.join(output_dir, \"output.png\")\n\n        rank = int(os.getenv(\"RANK\", 0))\n        if rank == 0:\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n            img.save(out_file_name)\n\n        # the client will look for either 'videos' or 'images' in the status json\n        # if neither is present, the client will look through the output directory for any files and display them\n        return {\"message\": \"created a red box\", \"prompt\": prompt, \"images\": [out_file_name]}\n\n    @staticmethod\n    def get_parameters_schema():\n        \"\"\"Return the JSON schema for the inference parameters.\"\"\"\n        return json.dumps(InferenceParameters.model_json_schema(), indent=2)\n\n    @staticmethod\n    def validate_parameters(kwargs: dict):\n        \"\"\"Validate the inference parameters.\"\"\"\n        params = InferenceParameters(**kwargs)\n        return params.model_dump(mode=\"json\")\n\n\ndef create_worker():\n    \"\"\"Factory function to create sample pipeline.\"\"\"\n    cfg = DeploymentEnv()\n\n    pipeline = SampleWorker(\n        num_gpus=cfg.num_gpus,\n        model_name=cfg.model_name,\n    )\n\n    return pipeline\n",
    "line_count": 82
  },
  {
    "id": "cccc_src_cccc_runners_pty_stub.py",
    "repo": "ChesterRa/cccc",
    "url": "https://github.com/ChesterRa/cccc/blob/main/src/cccc/runners/pty_stub.py",
    "code": "from __future__ import annotations\n\nimport socket\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Callable, Dict, Iterable, Optional, Tuple\n\nPTY_SUPPORTED = False\n\n\n@dataclass\nclass PtySession:\n    group_id: str = \"\"\n    actor_id: str = \"\"\n    pid: int = 0\n\n\nclass PtySupervisor:\n    def set_exit_hook(self, hook: Optional[Callable[[PtySession], None]]) -> None:\n        return None\n\n    def group_running(self, group_id: str) -> bool:\n        return False\n\n    def actor_running(self, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def tail_output(self, *, group_id: str, actor_id: str, max_bytes: int = 2_000_000) -> bytes:\n        return b\"\"\n\n    def clear_backlog(self, *, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def start_actor(\n        self,\n        *,\n        group_id: str,\n        actor_id: str,\n        cwd: Path,\n        command: Iterable[str],\n        env: Dict[str, str],\n        max_backlog_bytes: int = 2_000_000,\n    ) -> PtySession:\n        raise RuntimeError(\"pty runner is not supported on this platform; use runner='headless'\")\n\n    def stop_actor(self, *, group_id: str, actor_id: str) -> None:\n        return None\n\n    def stop_group(self, *, group_id: str) -> None:\n        return None\n\n    def stop_all(self) -> None:\n        return None\n\n    def attach(self, *, group_id: str, actor_id: str, sock: socket.socket) -> None:\n        raise RuntimeError(\"pty runner is not supported on this platform\")\n\n    def bracketed_paste_enabled(self, *, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def bracketed_paste_status(self, *, group_id: str, actor_id: str) -> Tuple[bool, Optional[float]]:\n        return (False, None)\n\n    def startup_times(self, *, group_id: str, actor_id: str) -> Tuple[Optional[float], Optional[float]]:\n        return (None, None)\n\n    def session_key(self, *, group_id: str, actor_id: str) -> Optional[str]:\n        return None\n\n    def resize(self, *, group_id: str, actor_id: str, cols: int, rows: int) -> None:\n        return None\n\n    def write_input(self, *, group_id: str, actor_id: str, data: bytes) -> bool:\n        return False\n\n\nSUPERVISOR = PtySupervisor()\n\n",
    "line_count": 78
  },
  {
    "id": "lenspect_src_core_secret_manager.py",
    "repo": "vmkspv/lenspect",
    "url": "https://github.com/vmkspv/lenspect/blob/main/src/core/secret_manager.py",
    "code": "# secret_manager.py\n#\n# Copyright 2025 Vladimir Kosolapov\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n#\n# SPDX-License-Identifier: GPL-3.0-or-later\n\nimport gi\n\ngi.require_version('Secret', '1')\n\nfrom gi.repository import Secret\n\nclass SecretManager:\n    SCHEMA = Secret.Schema.new(\n        \"io.github.vmkspv.lenspect\",\n        Secret.SchemaFlags.NONE,\n        {\n            \"application\": Secret.SchemaAttributeType.STRING,\n            \"key-type\": Secret.SchemaAttributeType.STRING,\n        }\n    )\n\n    def __init__(self):\n        self.application_id = \"io.github.vmkspv.lenspect\"\n\n    def store_api_key(self, api_key: str) -> bool:\n        if not api_key or not api_key.strip():\n            return False\n\n        try:\n            Secret.password_store_sync(\n                self.SCHEMA,\n                {\n                    \"application\": self.application_id,\n                    \"key-type\": \"api-key\",\n                },\n                Secret.COLLECTION_DEFAULT,\n                \"VirusTotal API Key\",\n                api_key.strip(),\n                None\n            )\n            return True\n        except Exception:\n            return False\n\n    def load_api_key(self) -> str | None:\n        try:\n            password = Secret.password_lookup_sync(\n                self.SCHEMA,\n                {\n                    \"application\": self.application_id,\n                    \"key-type\": \"api-key\",\n                },\n                None\n            )\n            return password if password else None\n        except Exception:\n            return None\n\n    def delete_api_key(self) -> bool:\n        try:\n            Secret.password_clear_sync(\n                self.SCHEMA,\n                {\n                    \"application\": self.application_id,\n                    \"key-type\": \"api-key\",\n                },\n                None\n            )\n            return True\n        except Exception:\n            return False\n",
    "line_count": 85
  },
  {
    "id": "traceml_src_traceml_database_database.py",
    "repo": "traceopt-ai/traceml",
    "url": "https://github.com/traceopt-ai/traceml/blob/main/src/traceml/database/database.py",
    "code": "from typing import Any, Dict, List\nfrom traceml.database.database_writer import DatabaseWriter\n\n\nclass Database:\n    \"\"\"\n    Each \"table\" is a dict. Table names must be unique.\n    \"\"\"\n\n    def __init__(self, sampler_name):\n        self._tables: Dict[str, List[Any]] = {}\n        self.writer = DatabaseWriter(self, sampler_name=sampler_name)\n\n    def create_table(self, name: str) -> List[Any]:\n        \"\"\"\n        Create a new empty table if not exists.\n        Raise ValueError if table already exists.\n        \"\"\"\n        if name in self._tables:\n            raise ValueError(f\"Table '{name}' already exists.\")\n        self._tables[name] = []\n        return self._tables[name]\n\n    def create_or_get_table(self, name: str) -> List[Any]:\n        \"\"\"\n        Create table if missing, otherwise return existing table.\n        \"\"\"\n        if name not in self._tables:\n            self._tables[name] = []\n        return self._tables[name]\n\n    def add_record(self, table: str, record: Any):\n        \"\"\"\n        Add a single record to a table.\n        Automatically creates table if it doesn't exist.\n        \"\"\"\n        if table not in self._tables:\n            raise ValueError(f\"Table '{table}' does not exist.\")\n        self._tables[table].append(record)\n\n    def get_record_at_index(self, table: str, index: int) -> Any:\n        \"\"\"\n        Return the record at a given index from a table.\n        Returns None if table does not exist or index is out of range.\n        \"\"\"\n        if table not in self._tables:\n            return None\n\n        rows = self._tables[table]\n\n        # Allow negative indexing like Python lists\n        if -len(rows) <= index < len(rows):\n            return rows[index]\n\n        return None\n\n    def all_tables(self) -> Dict[str, List[Any]]:\n        \"\"\"Return a dict of all tables.\"\"\"\n        return self._tables\n\n    def clear(self):\n        \"\"\"Clear all tables.\"\"\"\n        self._tables.clear()\n",
    "line_count": 63
  },
  {
    "id": "Resume-Agent_backend_agent_flow_base.py",
    "repo": "WyRainBow/Resume-Agent",
    "url": "https://github.com/WyRainBow/Resume-Agent/blob/main/backend/agent/flow/base.py",
    "code": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Union\n\nfrom pydantic import BaseModel\n\nfrom backend.agent.agent.base import BaseAgent\n\n\nclass BaseFlow(BaseModel, ABC):\n    \"\"\"Base class for execution flows supporting multiple agents\"\"\"\n\n    agents: Dict[str, BaseAgent]\n    tools: Optional[List] = None\n    primary_agent_key: Optional[str] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __init__(\n        self, agents: Union[BaseAgent, List[BaseAgent], Dict[str, BaseAgent]], **data\n    ):\n        # Handle different ways of providing agents\n        if isinstance(agents, BaseAgent):\n            agents_dict = {\"default\": agents}\n        elif isinstance(agents, list):\n            agents_dict = {f\"agent_{i}\": agent for i, agent in enumerate(agents)}\n        else:\n            agents_dict = agents\n\n        # If primary agent not specified, use first agent\n        primary_key = data.get(\"primary_agent_key\")\n        if not primary_key and agents_dict:\n            primary_key = next(iter(agents_dict))\n            data[\"primary_agent_key\"] = primary_key\n\n        # Set the agents dictionary\n        data[\"agents\"] = agents_dict\n\n        # Initialize using BaseModel's init\n        super().__init__(**data)\n\n    @property\n    def primary_agent(self) -> Optional[BaseAgent]:\n        \"\"\"Get the primary agent for the flow\"\"\"\n        return self.agents.get(self.primary_agent_key)\n\n    def get_agent(self, key: str) -> Optional[BaseAgent]:\n        \"\"\"Get a specific agent by key\"\"\"\n        return self.agents.get(key)\n\n    def add_agent(self, key: str, agent: BaseAgent) -> None:\n        \"\"\"Add a new agent to the flow\"\"\"\n        self.agents[key] = agent\n\n    @abstractmethod\n    async def execute(self, input_text: str) -> str:\n        \"\"\"Execute the flow with given input\"\"\"\n",
    "line_count": 57
  },
  {
    "id": "stabilize_src_stabilize_orchestrator.py",
    "repo": "rodmena-limited/stabilize",
    "url": "https://github.com/rodmena-limited/stabilize/blob/main/src/stabilize/orchestrator.py",
    "code": "\"\"\"\nOrchestrator - starts and manages pipeline executions.\n\nThis module provides the main entry point for running pipelines.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom stabilize.queue.messages import (\n    CancelWorkflow,\n    RestartStage,\n    ResumeStage,\n    StartWorkflow,\n)\n\nif TYPE_CHECKING:\n    from stabilize.models.workflow import Workflow\n    from stabilize.queue.queue import Queue\n\n\nclass Orchestrator:\n    \"\"\"\n    Runner for pipeline executions.\n\n    Provides methods to start, cancel, restart, and resume executions\n    by pushing appropriate messages to the queue.\n    \"\"\"\n\n    def __init__(self, queue: Queue) -> None:\n        \"\"\"\n        Initialize the runner.\n\n        Args:\n            queue: The message queue\n        \"\"\"\n        self.queue = queue\n\n    def start(self, execution: Workflow) -> None:\n        \"\"\"\n        Start a pipeline execution.\n\n        Args:\n            execution: The execution to start\n        \"\"\"\n        self.queue.push(\n            StartWorkflow(\n                execution_type=execution.type.value,\n                execution_id=execution.id,\n            )\n        )\n\n    def cancel(\n        self,\n        execution: Workflow,\n        user: str,\n        reason: str,\n    ) -> None:\n        \"\"\"\n        Cancel a running execution.\n\n        Args:\n            execution: The execution to cancel\n            user: Who is canceling\n            reason: Why it's being canceled\n        \"\"\"\n        self.queue.push(\n            CancelWorkflow(\n                execution_type=execution.type.value,\n                execution_id=execution.id,\n                user=user,\n                reason=reason,\n            )\n        )\n\n    def restart(\n        self,\n        execution: Workflow,\n        stage_id: str,\n    ) -> None:\n        \"\"\"\n        Restart a stage in an execution.\n\n        Args:\n            execution: The execution\n            stage_id: The stage to restart\n        \"\"\"\n        self.queue.push(\n            RestartStage(\n                execution_type=execution.type.value,\n                execution_id=execution.id,\n                stage_id=stage_id,\n            )\n        )\n\n    def unpause(self, execution: Workflow) -> None:\n        \"\"\"\n        Resume a paused execution.\n\n        Args:\n            execution: The execution to resume\n        \"\"\"\n        # Resume all paused stages\n        for stage in execution.stages:\n            if stage.status.name == \"PAUSED\":\n                self.queue.push(\n                    ResumeStage(\n                        execution_type=execution.type.value,\n                        execution_id=execution.id,\n                        stage_id=stage.id,\n                    )\n                )\n",
    "line_count": 113
  },
  {
    "id": "deeppoint-ai_lib_crawlers_douyin_new_cache_abs_cache.py",
    "repo": "weiyf2/deeppoint-ai",
    "url": "https://github.com/weiyf2/deeppoint-ai/blob/main/lib/crawlers/douyin_new/cache/abs_cache.py",
    "code": "# -*- coding: utf-8 -*-\n# Copyright (c) 2025 relakkes@gmail.com\n#\n# This file is part of MediaCrawler project.\n# Repository: https://github.com/NanmiCoder/MediaCrawler/blob/main/cache/abs_cache.py\n# GitHub: https://github.com/NanmiCoder\n# Licensed under NON-COMMERCIAL LEARNING LICENSE 1.1\n#\n\n# å£°æ˜Žï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŽŸåˆ™ï¼š\n# 1. ä¸å¾—ç”¨äºŽä»»ä½•å•†ä¸šç”¨é€”ã€‚\n# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚\n# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚\n# 4. åº”åˆç†æŽ§åˆ¶è¯·æ±‚é¢‘çŽ‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚\n# 5. ä¸å¾—ç”¨äºŽä»»ä½•éžæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚\n#\n# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚\n# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŽŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚\n\n\n# -*- coding: utf-8 -*-\n# @Author  : relakkes@gmail.com\n# @Name    : Programmer AJiang-Relakkes\n# @Time    : 2024/6/2 11:06\n# @Desc    : Abstract class\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any, List, Optional\n\n\nclass AbstractCache(ABC):\n\n    @abstractmethod\n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"\n        Get the value of a key from the cache.\n        This is an abstract method. Subclasses must implement this method.\n        :param key: The key\n        :return:\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def set(self, key: str, value: Any, expire_time: int) -> None:\n        \"\"\"\n        Set the value of a key in the cache.\n        This is an abstract method. Subclasses must implement this method.\n        :param key: The key\n        :param value: The value\n        :param expire_time: Expiration time\n        :return:\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def keys(self, pattern: str) -> List[str]:\n        \"\"\"\n        Get all keys matching the pattern\n        :param pattern: Matching pattern\n        :return:\n        \"\"\"\n        raise NotImplementedError\n",
    "line_count": 62
  },
  {
    "id": "lead_scripts_tools_proxy_simulator_carla_wrapper.py",
    "repo": "autonomousvision/lead",
    "url": "https://github.com/autonomousvision/lead/blob/main/scripts/tools/proxy_simulator/carla_wrapper.py",
    "code": "#!/usr/bin/env python\n\"\"\"\nSimple CARLA client for rendering the map.\n\"\"\"\n\nimport carla\nimport torch\n\ntry:\n    from os import environ\n\n    environ[\"PYGAME_HIDE_SUPPORT_PROMPT\"] = \"1\"  # Hides the Pygame welcome message\n    import pygame\nexcept ImportError as exc:\n    raise RuntimeError(\"cannot import pygame, make sure pygame package is installed\") from exc\n\nimport numpy as np\nfrom map_utils import MapImage\nfrom PIL import ImageShow\n\n# configure PIL to use ego because image magick does not fit windows to screen\nImageShow.register(ImageShow.EogViewer, 0)\n\n# Global Flags\nPIXELS_PER_METER = 5\nPIXELS_AHEAD_VEHICLE = 100\n\n\nclass CarlaWrapper:\n    \"\"\"\n    Simple CARLA client for rendering the map.\n    \"\"\"\n\n    def __init__(self, args):\n        self._vehicle = None\n        self.args = args\n        self.client = carla.Client(\"localhost\", args.port)\n        self.client.set_timeout(360.0)\n\n        # we default to Town10HD on load up\n        self.set_town(\"Town10HD\")\n\n    def set_town(self, town=\"Town01\"):\n        self.town = town\n        self.world = self.client.load_world(town)\n        self.carla_map = self.world.get_map()\n        map_image = MapImage(self.carla_map, PIXELS_PER_METER)\n        road = self.swap_axes(map_image.map_surface)\n        lane = self.swap_axes(map_image.lane_surface)\n\n        global_map = np.zeros(\n            (\n                1,\n                7,\n            )\n            + road.shape\n        )\n        global_map[:, 0, ...] = road / 255.0\n        global_map[:, 1, ...] = lane / 255.0\n\n        self.map = torch.tensor(global_map, device=self.args.device, dtype=torch.float32)\n        self.map_offset = torch.tensor(map_image.world_offset, device=self.args.device, dtype=torch.float32)\n\n    def swap_axes(self, x):\n        return np.swapaxes(pygame.surfarray.array3d(x), 0, 1).mean(axis=-1)\n",
    "line_count": 65
  },
  {
    "id": "quorum-cli_src_quorum_clients_openai_client.py",
    "repo": "Detrol/quorum-cli",
    "url": "https://github.com/Detrol/quorum-cli/blob/main/src/quorum/clients/openai_client.py",
    "code": "\"\"\"OpenAI-compatible client for multiple providers.\n\nThis client works with any OpenAI-compatible API:\n- OpenAI (direct)\n- Google Gemini (via OpenAI-compatible endpoint)\n- xAI Grok (via OpenAI-compatible endpoint)\n- Ollama (via OpenAI-compatible endpoint)\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom openai import AsyncOpenAI\n\nfrom .types import AssistantMessage, Message, SystemMessage, UserMessage\n\nif TYPE_CHECKING:\n    import httpx\n\n\nclass OpenAIClient:\n    \"\"\"Client for OpenAI-compatible APIs.\n\n    Supports OpenAI, Google Gemini, xAI Grok, and Ollama through their\n    OpenAI-compatible endpoints.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        api_key: str,\n        base_url: str | None = None,\n        http_client: \"httpx.AsyncClient | None\" = None,\n    ):\n        \"\"\"Initialize the OpenAI-compatible client.\n\n        Args:\n            model: Model identifier (e.g., \"gpt-4o\", \"gemini-2.0-flash\").\n            api_key: API key for authentication.\n            base_url: Optional base URL for non-OpenAI providers.\n            http_client: Optional shared httpx client for connection pooling.\n        \"\"\"\n        self.model = model\n        self._api_key: str | None = api_key\n        self._client = AsyncOpenAI(\n            api_key=api_key,\n            base_url=base_url,\n            http_client=http_client,\n        )\n\n    async def create(self, messages: list[Message]) -> str:\n        \"\"\"Send messages to the model and get a response.\n\n        Args:\n            messages: List of messages forming the conversation.\n\n        Returns:\n            The model's response text.\n\n        Raises:\n            openai.APIError: If the API request fails.\n        \"\"\"\n        response = await self._client.chat.completions.create(\n            model=self.model,\n            messages=[self._convert_message(m) for m in messages],\n        )\n\n        # Extract content from response\n        content = response.choices[0].message.content\n        return content if content is not None else \"\"\n\n    def _convert_message(self, msg: Message) -> dict[str, str]:\n        \"\"\"Convert internal message type to OpenAI format.\n\n        Args:\n            msg: Internal message object.\n\n        Returns:\n            Dict in OpenAI message format.\n        \"\"\"\n        if isinstance(msg, SystemMessage):\n            return {\"role\": \"system\", \"content\": msg.content}\n        elif isinstance(msg, UserMessage):\n            return {\"role\": \"user\", \"content\": msg.content}\n        elif isinstance(msg, AssistantMessage):\n            return {\"role\": \"assistant\", \"content\": msg.content}\n        else:\n            # Fallback for any unknown type\n            return {\"role\": \"user\", \"content\": str(msg.content)}\n",
    "line_count": 90
  },
  {
    "id": "X-AnyLabeling-Server_app_models_yolo11.py",
    "repo": "CVHub520/X-AnyLabeling-Server",
    "url": "https://github.com/CVHub520/X-AnyLabeling-Server/blob/main/app/models/yolo11.py",
    "code": "import numpy as np\nfrom typing import Any, Dict\n\nfrom . import BaseModel\nfrom app.schemas.shape import Shape\nfrom app.core.registry import register_model\n\n\n@register_model(\"yolo11n\", \"yolo11s\", \"yolo11m\", \"yolo11l\", \"yolo11x\")\nclass YOLO11Detection(BaseModel):\n    \"\"\"YOLO11 object detection model.\"\"\"\n\n    def load(self):\n        \"\"\"Load YOLO model.\"\"\"\n        from ultralytics import YOLO\n\n        model_path = self.params.get(\"model_path\", \"yolo11n.pt\")\n        device = self.params.get(\"device\", \"cpu\")\n\n        self.model = YOLO(model_path)\n        self.model.to(device)\n\n        dummy_img = np.zeros((640, 640, 3), dtype=np.uint8)\n        self.model(dummy_img, verbose=False)\n\n    def predict(\n        self, image: np.ndarray, params: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Execute object detection.\n\n        Args:\n            image: Input image in BGR format.\n            params: Inference parameters.\n\n        Returns:\n            Dictionary with detection results.\n        \"\"\"\n        conf_threshold = params.get(\n            \"conf_threshold\", self.params.get(\"conf_threshold\", 0.25)\n        )\n        iou_threshold = params.get(\n            \"iou_threshold\", self.params.get(\"iou_threshold\", 0.45)\n        )\n\n        results = self.model(\n            image, conf=conf_threshold, iou=iou_threshold, verbose=False\n        )\n\n        shapes = []\n        for result in results:\n            boxes = result.boxes\n            if boxes is not None:\n                for box in boxes:\n                    xyxy = box.xyxy[0].cpu().numpy()\n                    conf = float(box.conf[0])\n                    cls = int(box.cls[0])\n                    label = result.names[cls]\n\n                    shape = Shape(\n                        label=label,\n                        shape_type=\"rectangle\",\n                        points=[\n                            [float(xyxy[0]), float(xyxy[1])],\n                            [float(xyxy[2]), float(xyxy[1])],\n                            [float(xyxy[2]), float(xyxy[3])],\n                            [float(xyxy[0]), float(xyxy[3])],\n                        ],\n                        score=conf,\n                    )\n                    shapes.append(shape)\n\n        return {\"shapes\": shapes, \"description\": \"\"}\n\n    def unload(self):\n        \"\"\"Release model resources.\"\"\"\n        if hasattr(self, \"model\"):\n            del self.model\n",
    "line_count": 77
  },
  {
    "id": "vocotype-cli_app_hotkeys.py",
    "repo": "233stone/vocotype-cli",
    "url": "https://github.com/233stone/vocotype-cli/blob/master/app/hotkeys.py",
    "code": "\"\"\"Global hotkey management for the application.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport threading\nfrom typing import Callable\n\nimport keyboard\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass HotkeyManager:\n    def __init__(self) -> None:\n        self._lock = threading.Lock()\n        self._registrations = {}\n\n    def register(self, combo: str, callback: Callable[[], None]) -> None:\n        with self._lock:\n            if combo in self._registrations:\n                logger.warning(\"çƒ­é”® %s å·²æ³¨å†Œï¼Œè¦†ç›–æ—§çš„å›žè°ƒ\", combo)\n                keyboard.remove_hotkey(self._registrations[combo])\n\n            try:\n                hotkey_id = keyboard.add_hotkey(combo, callback)\n            except Exception as exc:  # noqa: BLE001\n                logger.error(\"æ³¨å†Œçƒ­é”® %s å¤±è´¥: %s\", combo, exc)\n                raise\n\n            self._registrations[combo] = hotkey_id\n            logger.info(\"å·²æ³¨å†Œçƒ­é”® %s\", combo)\n\n    def unregister_all(self) -> None:\n        with self._lock:\n            for combo, hotkey_id in list(self._registrations.items()):\n                keyboard.remove_hotkey(hotkey_id)\n                logger.info(\"å·²ç§»é™¤çƒ­é”® %s\", combo)\n            self._registrations.clear()\n\n    def cleanup(self) -> None:\n        self.unregister_all()\n        # å½»åº•åœæ­¢ keyboard åº“çš„æ‰€æœ‰é’©å­å’Œç›‘å¬çº¿ç¨‹\n        try:\n            keyboard.unhook_all()\n            logger.info(\"å·²åœæ­¢ keyboard ç›‘å¬çº¿ç¨‹\")\n        except Exception as exc:\n            logger.warning(\"åœæ­¢ keyboard ç›‘å¬çº¿ç¨‹å¤±è´¥: %s\", exc)\n\n\n",
    "line_count": 51
  },
  {
    "id": "jepa-wms_app_plan_common_datasets_preprocessor.py",
    "repo": "facebookresearch/jepa-wms",
    "url": "https://github.com/facebookresearch/jepa-wms/blob/main/app/plan_common/datasets/preprocessor.py",
    "code": "# Copyright (c) Facebook, Inc. and its affiliates.\n# Inspired from https://github.com/gaoyuezhou/dino_wm\n# Licensed under the MIT License\n\nimport torch\nfrom einops import rearrange\n\n\nclass Preprocessor:\n    def __init__(\n        self,\n        action_mean,\n        action_std,\n        state_mean,\n        state_std,\n        proprio_mean,\n        proprio_std,\n        transform,\n        inverse_transform=None,\n    ):\n        self.action_mean = action_mean\n        self.action_std = action_std\n        self.state_mean = state_mean\n        self.state_std = state_std\n        self.proprio_mean = proprio_mean\n        self.proprio_std = proprio_std\n        self.transform = transform\n        self.inverse_transform = inverse_transform\n\n    def normalize_actions(self, actions):\n        \"\"\"\n        actions: (b, t, action_dim)\n        \"\"\"\n        return (actions - self.action_mean) / self.action_std\n\n    def denormalize_actions(self, actions):\n        \"\"\"\n        actions: (b, t, action_dim)\n        \"\"\"\n        return actions * self.action_std + self.action_mean\n\n    def denormalize_proprios(self, proprio):\n        \"\"\"\n        actions: (b, t, action_dim)\n        \"\"\"\n        return proprio * self.proprio_std + self.proprio_mean\n\n    def normalize_proprios(self, proprio):\n        \"\"\"\n        input shape (..., proprio_dim)\n        \"\"\"\n        return (proprio - self.proprio_mean) / self.proprio_std\n\n    def normalize_states(self, state):\n        \"\"\"\n        input shape (..., state_dim)\n        \"\"\"\n        return (state - self.state_mean) / self.state_std\n\n    def preprocess_obs_visual(self, obs_visual):\n        return rearrange(obs_visual, \"b t h w c -> b t c h w\") / 255.0\n\n    def transform_obs_visual(self, obs_visual):\n        transformed_obs_visual = torch.tensor(obs_visual)\n        transformed_obs_visual = self.preprocess_obs_visual(transformed_obs_visual)\n        transformed_obs_visual = self.transform(transformed_obs_visual)\n        return transformed_obs_visual\n\n    def transform_obs(self, obs):\n        \"\"\"\n        np arrays to tensors\n        \"\"\"\n        transformed_obs = {}\n        transformed_obs[\"visual\"] = self.transform_obs_visual(obs[\"visual\"])\n        transformed_obs[\"proprio\"] = self.normalize_proprios(torch.tensor(obs[\"proprio\"]))\n        return transformed_obs\n",
    "line_count": 76
  },
  {
    "id": "ZaiZaiCat-Checkin_script_huaruntong_wentiweilaihui_api.py",
    "repo": "Cat-zaizai/ZaiZaiCat-Checkin",
    "url": "https://github.com/Cat-zaizai/ZaiZaiCat-Checkin/blob/main/script/huaruntong/wentiweilaihui/api.py",
    "code": "\"\"\"\nåŽæ¶¦é€šæ–‡ä½“æœªæ¥èŸAPIæŽ¥å£\n\"\"\"\nimport requests\nimport uuid\nimport time\n\n\nclass WenTiWeiLaiHuiAPI:\n    \"\"\"æ–‡ä½“æœªæ¥èŸAPIæŽ¥å£ç±»\"\"\"\n\n    def __init__(self, token, mobile, user_agent=None):\n        \"\"\"\n        åˆå§‹åŒ–API\n        :param token: è®¤è¯token\n        :param mobile: æ‰‹æœºå·ï¼ˆç”¨äºŽæ˜¾ç¤ºï¼‰\n        :param user_agent: ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²\n        \"\"\"\n        self.token = token\n        self.mobile = mobile\n        self.base_url = \"https://wtmp.crland.com.cn\"\n        self.user_agent = user_agent or 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 MicroMessenger/7.0.20.1781(0x6700143B) NetType/WIFI MiniProgramEnv/Mac MacWechat/WMPF MacWechat/3.8.7(0x13080712) UnifiedPCMacWechat(0xf26405f0) XWEB/13910'\n        self.headers = {\n            'User-Agent': self.user_agent,\n            'Content-Type': 'application/json',\n            'xweb_xhr': '1',\n            'x-hrt-mid-appid': 'API_AUTH_MINI',\n            'token': self.token,\n            'sec-fetch-site': 'cross-site',\n            'sec-fetch-mode': 'cors',\n            'sec-fetch-dest': 'empty',\n            'referer': 'https://servicewechat.com/wx3c35b1f0737c23ce/11/page-frame.html',\n            'accept-language': 'zh-CN,zh;q=0.9',\n            'priority': 'u=1, i'\n        }\n\n    def sign_in(self):\n        \"\"\"\n        ç­¾åˆ°æŽ¥å£\n        :return: æŽ¥å£å“åº”æ•°æ®\n        \"\"\"\n        url = f\"{self.base_url}/promotion/app/sign/signin\"\n\n        # ç”Ÿæˆç­¾åˆ°æ•°æ®\n        data = {\n            \"data\": {\n                \"outOrderNo\": str(uuid.uuid4()),\n                \"mobile\": self.mobile,\n                \"timestamp\": int(time.time() * 1000),\n                \"projectCode\": \"df2d2333f94f4c508073e0646610c021\",\n                \"deviceChannel\": \"WECHAT\",\n                \"businessChannel\": \"miniprogram\",\n                \"channelCode\": \"wechat\"\n            }\n        }\n\n        try:\n            response = requests.post(url, json=data, headers=self.headers)\n            response.raise_for_status()\n            return response.json()\n        except Exception as e:\n            return {\"success\": False, \"msg\": f\"è¯·æ±‚å¤±è´¥: {str(e)}\"}\n\n    def query_points(self):\n        \"\"\"\n        æŸ¥è¯¢ä¸‡è±¡æ˜Ÿç§¯åˆ†\n        :return: æŽ¥å£å“åº”æ•°æ®\n        \"\"\"\n        url = f\"{self.base_url}/pointsAccount/app/queryAccount\"\n\n        try:\n            response = requests.post(url, json={}, headers=self.headers)\n            response.raise_for_status()\n            return response.json()\n        except Exception as e:\n            return {\"success\": False, \"msg\": f\"è¯·æ±‚å¤±è´¥: {str(e)}\"}\n\n",
    "line_count": 77
  },
  {
    "id": "360Extractor_src_core_job.py",
    "repo": "nicolasdiolez/360Extractor",
    "url": "https://github.com/nicolasdiolez/360Extractor/blob/main/src/core/job.py",
    "code": "from dataclasses import dataclass, field\nfrom typing import Dict, Any, List, Optional\nimport os\n\n@dataclass\nclass Job:\n    file_path: str\n    status: str = \"Pending\"  # Pending, Processing, Done, Error\n    settings: Dict[str, Any] = field(default_factory=dict)\n\n    @property\n    def active_cameras(self) -> Optional[List[int]]:\n        return self.settings.get('active_cameras', None)\n\n    @property\n    def filename(self) -> str:\n        return os.path.basename(self.file_path)\n\n    @property\n    def output_format(self) -> str:\n        return self.settings.get('output_format', 'jpg')\n\n    @property\n    def output_dir(self) -> str:\n        return self.settings.get('custom_output_dir', '')\n\n    @property\n    def smart_blur(self) -> bool:\n        return self.settings.get('smart_blur_enabled', False)\n\n    @property\n    def adaptive_mode(self) -> bool:\n        return self.settings.get('adaptive_mode', False)\n\n    @property\n    def adaptive_threshold(self) -> float:\n        return self.settings.get('adaptive_threshold', 0.5)\n\n    @property\n    def resolution(self) -> int:\n        return self.settings.get('resolution', 2048)\n\n    @property\n    def export_telemetry(self) -> bool:\n        return self.settings.get('export_telemetry', False)\n\n    def summary(self) -> str:\n        \"\"\"Returns a short summary of the job settings.\"\"\"\n        # e.g., \"High (-20Â°), 6 cams\"\n        pitch_val = self.settings.get('pitch_offset', 0)\n        pitch_name = \"Std\"\n        if pitch_val == -20: pitch_name = \"High\"\n        elif pitch_val == 20: pitch_name = \"Low\"\n        \n        cams = self.settings.get('camera_count', 6)\n        layout = self.settings.get('layout_mode', 'adaptive')\n        layout_info = \" (Ring)\" if layout == 'ring' else \"\"\n        adaptive = \" [Adaptive]\" if self.adaptive_mode else \"\"\n        return f\"{pitch_name} ({pitch_val}Â°), {cams} cams{layout_info}{adaptive}\"",
    "line_count": 59
  },
  {
    "id": "sam3d-body-rerun_src_sam3d_body_models_backbones_dinov3.py",
    "repo": "rerun-io/sam3d-body-rerun",
    "url": "https://github.com/rerun-io/sam3d-body-rerun/blob/main/src/sam3d_body/models/backbones/dinov3.py",
    "code": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\nimport torch\nfrom torch import nn\n\n\nclass Dinov3Backbone(nn.Module):\n    def __init__(\n        self, name=\"dinov2_vitb14\", pretrained_weight=None, cfg=None, *args, **kwargs\n    ):\n        super().__init__()\n        self.name = name\n        self.cfg = cfg\n\n        self.encoder = torch.hub.load(\n            \"facebookresearch/dinov3\",\n            self.name,\n            source=\"github\",\n            pretrained=False,\n            drop_path=self.cfg.MODEL.BACKBONE.DROP_PATH_RATE,\n        )\n        self.patch_size = self.encoder.patch_size\n        self.embed_dim = self.embed_dims = self.encoder.embed_dim\n\n    def forward(self, x, extra_embed=None):\n        \"\"\"\n        Encode a RGB image using a ViT-backbone\n        Args:\n            - x: torch.Tensor of shape [bs,3,w,h]\n        Return:\n            - y: torch.Tensor of shape [bs,k,d] - image in patchified mode\n        \"\"\"\n        assert extra_embed is None, \"Not Implemented Yet\"\n\n        y = self.encoder.get_intermediate_layers(x, n=1, reshape=True, norm=True)[-1]\n\n        return y\n\n    def get_layer_depth(self, param_name: str, prefix: str = \"encoder.\"):\n        \"\"\"Get the layer-wise depth of a parameter.\n        Args:\n            param_name (str): The name of the parameter.\n            prefix (str): The prefix for the parameter.\n                Defaults to an empty string.\n        Returns:\n            Tuple[int, int]: The layer-wise depth and the num of layers.\n        Note:\n            The first depth is the stem module (``layer_depth=0``), and the\n            last depth is the subsequent module (``layer_depth=num_layers-1``)\n        \"\"\"\n        num_layers = self.encoder.n_blocks + 2\n\n        if not param_name.startswith(prefix):\n            # For subsequent module like head\n            return num_layers - 1, num_layers\n\n        param_name = param_name[len(prefix) :]\n\n        if param_name in (\"cls_token\", \"pos_embed\", \"storage_tokens\"):\n            layer_depth = 0\n        elif param_name.startswith(\"patch_embed\"):\n            layer_depth = 0\n        elif param_name.startswith(\"blocks\"):\n            layer_id = int(param_name.split(\".\")[1])\n            layer_depth = layer_id + 1\n        else:\n            layer_depth = num_layers - 1\n\n        return layer_depth, num_layers\n",
    "line_count": 69
  },
  {
    "id": "Feagent_src_application_services_idempotency_coordinator.py",
    "repo": "DSGWJQ/Feagent",
    "url": "https://github.com/DSGWJQ/Feagent/blob/main/src/application/services/idempotency_coordinator.py",
    "code": "\"\"\"IdempotencyCoordinator - Application-level idempotency + concurrency control.\n\nImplements per-idempotency-key in-flight de-duplication using stdlib asyncio primitives\nand persists successful results via the IdempotencyStore Domain Port.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom collections.abc import Awaitable, Callable\nfrom typing import Any\n\nfrom src.domain.ports.idempotency_store import IdempotencyStore\n\n\nclass IdempotencyNotReadyError(RuntimeError):\n    \"\"\"Raised when idempotency is requested but cannot be served.\"\"\"\n\n\nclass IdempotencyCoordinator:\n    def __init__(self, *, store: IdempotencyStore) -> None:\n        self._store = store\n        self._guard = asyncio.Lock()\n        self._in_flight: dict[str, asyncio.Task[Any]] = {}\n\n    async def run(\n        self,\n        *,\n        idempotency_key: str,\n        work: Callable[[], Awaitable[Any]],\n    ) -> Any:\n        if await self._store.exists(idempotency_key):\n            return await self._store.get_result(idempotency_key)\n\n        async with self._guard:\n            if await self._store.exists(idempotency_key):\n                return await self._store.get_result(idempotency_key)\n\n            task = self._in_flight.get(idempotency_key)\n            if task is None:\n                task = asyncio.create_task(self._run_and_persist(idempotency_key, work))\n                self._in_flight[idempotency_key] = task\n\n        return await asyncio.shield(task)\n\n    async def _run_and_persist(\n        self,\n        idempotency_key: str,\n        work: Callable[[], Awaitable[Any]],\n    ) -> Any:\n        try:\n            result = await work()\n            await self._store.save_result(idempotency_key, result)\n            return result\n        finally:\n            async with self._guard:\n                self._in_flight.pop(idempotency_key, None)\n",
    "line_count": 57
  },
  {
    "id": "ai_story_backend_core_ai_client_mock_image2video_client.py",
    "repo": "xhongc/ai_story",
    "url": "https://github.com/xhongc/ai_story/blob/main/backend/core/ai_client/mock_image2video_client.py",
    "code": "\"\"\"\nMock å›¾ç”Ÿè§†é¢‘å®¢æˆ·ç«¯å®žçŽ°\nç”¨äºŽæµ‹è¯•å’Œå¼€å‘çŽ¯å¢ƒï¼Œè¿”å›žæ¨¡æ‹Ÿçš„è§†é¢‘URL\n\"\"\"\n\nimport time\nfrom typing import Dict, Any\nfrom .base import Image2VideoClient, AIResponse\n\n\nclass MockImage2VideoClient(Image2VideoClient):\n    \"\"\"\n    Mock å›¾ç”Ÿè§†é¢‘å®¢æˆ·ç«¯\n    è¿”å›žé¢„å®šä¹‰çš„æ¨¡æ‹Ÿè§†é¢‘URLï¼Œç”¨äºŽæµ‹è¯•å·¥ä½œæµ\n    \"\"\"\n\n    # æ¨¡æ‹Ÿè§†é¢‘URLåˆ—è¡¨ï¼ˆä½¿ç”¨ç¤ºä¾‹è§†é¢‘ï¼‰\n    MOCK_VIDEO_URLS = [\n        \"https://sample-videos.com/video123/mp4/720/big_buck_bunny_720p_1mb.mp4\",\n        \"https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4\",\n        \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/360/Big_Buck_Bunny_360_10s_1MB.mp4\",\n    ]\n\n    async def _generate_video(\n        self,\n        image_url: str,\n        camera_movement: Dict[str, Any],\n        duration: float,\n        fps: int,\n        **kwargs\n    ) -> AIResponse:\n        \"\"\"\n        ç”Ÿæˆæ¨¡æ‹Ÿçš„è§†é¢‘å“åº”\n\n        Args:\n            image_url: æºå›¾ç‰‡URL\n            camera_movement: è¿é•œå‚æ•°\n            duration: è§†é¢‘æ—¶é•¿\n            fps: å¸§çŽ‡\n            **kwargs: å…¶ä»–å‚æ•°\n\n        Returns:\n            AIResponse: åŒ…å«æ¨¡æ‹Ÿè§†é¢‘URLçš„å“åº”å¯¹è±¡\n        \"\"\"\n        start_time = time.time()\n\n        # æ¨¡æ‹ŸAPIå»¶è¿Ÿï¼ˆè§†é¢‘ç”Ÿæˆé€šå¸¸å¾ˆæ…¢ï¼‰\n        time.sleep(2.0)\n\n        # ä»ŽkwargsèŽ·å–å‚æ•°\n        width = kwargs.get('width', 1280)\n        height = kwargs.get('height', 720)\n        model = kwargs.get('model', self.model_name)\n\n        # æ ¹æ®å›¾ç‰‡URLå“ˆå¸Œé€‰æ‹©è§†é¢‘ï¼ˆä¿è¯ç›¸åŒå›¾ç‰‡è¿”å›žç›¸åŒè§†é¢‘ï¼‰\n        image_hash = hash(image_url) % len(self.MOCK_VIDEO_URLS)\n        video_url = self.MOCK_VIDEO_URLS[image_hash]\n\n        # æž„å»ºè§†é¢‘æ•°æ®\n        video_data = {\n            \"url\": video_url,\n            \"width\": width,\n            \"height\": height,\n            \"duration\": duration,\n            \"fps\": fps,\n            \"format\": \"mp4\",\n            \"file_size\": 1024 * 1024,  # æ¨¡æ‹Ÿ1MBæ–‡ä»¶å¤§å°\n            \"camera_movement\": camera_movement\n        }\n\n        latency_ms = int((time.time() - start_time) * 1000)\n\n        return AIResponse(\n            success=True,\n            data={\n                'url': video_url,\n                'video': video_data,\n                'videos': [video_data]  # å…¼å®¹å¤šè§†é¢‘æ ¼å¼\n            },\n            metadata={\n                'latency_ms': latency_ms,\n                'model': model,\n                'is_mock': True,\n                'source_image': image_url[:100]  # è®°å½•éƒ¨åˆ†æºå›¾ç‰‡URL\n            }\n        )\n\n    async def validate_config(self) -> bool:\n        \"\"\"\n        éªŒè¯é…ç½®ï¼ˆMockå®¢æˆ·ç«¯å§‹ç»ˆè¿”å›žTrueï¼‰\n\n        Returns:\n            bool: å§‹ç»ˆè¿”å›žTrue\n        \"\"\"\n        return True\n\n    async def health_check(self) -> bool:\n        \"\"\"\n        å¥åº·æ£€æŸ¥ï¼ˆMockå®¢æˆ·ç«¯å§‹ç»ˆè¿”å›žTrueï¼‰\n\n        Returns:\n            bool: å§‹ç»ˆè¿”å›žTrue\n        \"\"\"\n        return True\n",
    "line_count": 104
  },
  {
    "id": "powermem_src_powermem_storage_base.py",
    "repo": "oceanbase/powermem",
    "url": "https://github.com/oceanbase/powermem/blob/main/src/powermem/storage/base.py",
    "code": "\"\"\"\nAbstract base class for storage implementations\n\nThis module defines the storage interface that all implementations must follow.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Optional, Any, List\n\nfrom pydantic import BaseModel\n\n\nclass OutputData(BaseModel):\n    id: Optional[int]  # memory id (Snowflake ID - 64-bit integer)\n    score: Optional[float]  # distance\n    payload: Optional[Dict]  # metadata\n\nclass VectorStoreBase(ABC):\n    \"\"\"\n    Abstract base class for storage implementations.\n    \n    This class defines the interface that all storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def create_col(self, name, vector_size, distance):\n        \"\"\"Create a new collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def insert(self, vectors, payloads=None, ids=None):\n        \"\"\"Insert vectors into a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query, vectors, limit=5, filters=None):\n        \"\"\"Search for similar vectors.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete(self, vector_id):\n        \"\"\"Delete a vector by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def update(self, vector_id, vector=None, payload=None):\n        \"\"\"Update a vector and its payload.\"\"\"\n        pass\n\n    @abstractmethod\n    def get(self, vector_id):\n        \"\"\"Retrieve a vector by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_cols(self):\n        \"\"\"List all collections.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_col(self):\n        \"\"\"Delete a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def col_info(self):\n        \"\"\"Get information about a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def list(self, filters=None, limit=None):\n        \"\"\"List all memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"Reset by delete the collection and recreate it.\"\"\"\n        pass\n\nclass GraphStoreBase(ABC):\n    \"\"\"\n    Abstract base class for graph storage implementations.\n\n    This class defines the interface that all graph storage backends must implement.\n    \"\"\"\n    @abstractmethod\n    def add(self, data: str, filters: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Add data to the graph.\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query: str, filters: Dict[str, Any], limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Search for memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_all(self, filters: Dict[str, Any]) -> None:\n        \"\"\"Delete all graph data for the given filters.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_all(self, filters: Dict[str, Any], limit: int = 100) -> List[Dict[str, str]]:\n        \"\"\"Retrieve all nodes and relationships from the graph database.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset the graph by clearing all nodes and relationships.\"\"\"\n        pass",
    "line_count": 109
  },
  {
    "id": "openguardrails_backend_services_cache_cleaner.py",
    "repo": "openguardrails/openguardrails",
    "url": "https://github.com/openguardrails/openguardrails/blob/main/backend/services/cache_cleaner.py",
    "code": "import asyncio\nfrom datetime import datetime, timedelta\nfrom utils.auth_cache import auth_cache\nfrom services.rate_limiter import rate_limiter\nfrom services.keyword_cache import keyword_cache\nfrom utils.logger import setup_logger\n\nlogger = setup_logger()\n\nclass CacheCleaner:\n    \"\"\"Cache cleaner service\"\"\"\n    \n    def __init__(self):\n        self._cleanup_task = None\n        self._running = False\n    \n    async def start(self):\n        \"\"\"Start cache cleaner service\"\"\"\n        if self._running:\n            return\n        \n        self._running = True\n        self._cleanup_task = asyncio.create_task(self._cleanup_loop())\n        logger.info(\"Cache cleaner service started\")\n    \n    async def stop(self):\n        \"\"\"Stop cache cleaner service\"\"\"\n        self._running = False\n        if self._cleanup_task:\n            self._cleanup_task.cancel()\n            try:\n                await self._cleanup_task\n            except asyncio.CancelledError:\n                pass\n        logger.info(\"Cache cleaner service stopped\")\n    \n    async def _cleanup_loop(self):\n        \"\"\"Cleanup loop\"\"\"\n        while self._running:\n            try:\n                # Clean expired auth cache\n                auth_cache.clear_expired()\n                \n                # Clean expired rate limit records (keep recent 2 minutes records)\n                current_time = asyncio.get_event_loop().time()\n                cutoff_time = current_time - 120  # 2åˆ†é’Ÿå‰\n                \n                # PostgreSQL rate limiter doesn't need manual cleanup of user requests\n                # as it uses database storage with automatic cleanup via SQL queries\n                \n                # Record cache statistics\n                auth_cache_size = auth_cache.size()\n                rate_limit_users = len(rate_limiter._local_cache)\n                keyword_cache_info = keyword_cache.get_cache_info()\n                \n                if auth_cache_size > 0 or rate_limit_users > 0 or keyword_cache_info['blacklist_keywords'] > 0:\n                    logger.debug(f\"Cache stats - Auth: {auth_cache_size}, Rate limit users: {rate_limit_users}, Keywords: B{keyword_cache_info['blacklist_keywords']}/W{keyword_cache_info['whitelist_keywords']}\")\n                \n                # Clean every 60 seconds\n                await asyncio.sleep(60)\n                \n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                logger.error(f\"Cache cleanup error: {e}\")\n                await asyncio.sleep(60)\n\n# Global cache cleaner service instance\ncache_cleaner = CacheCleaner()",
    "line_count": 69
  },
  {
    "id": "rules_src_formats_antigravity.py",
    "repo": "project-codeguard/rules",
    "url": "https://github.com/project-codeguard/rules/blob/main/src/formats/antigravity.py",
    "code": "# Copyright 2025 Cisco Systems, Inc. and its affiliates\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"\nAntigravity Format Implementation\n\nGenerates .md rule files for Antigravity with YAML frontmatter.\n\"\"\"\n\nfrom formats.base import BaseFormat, ProcessedRule\n\n\nclass AntigravityFormat(BaseFormat):\n    \"\"\"\n    Antigravity format implementation (.md rule files).\n\n    Antigravity uses .md files with YAML frontmatter containing:\n    - trigger: 'always_on' or 'glob' (activation type)\n    - globs: (if trigger is 'glob') File matching patterns\n    - description: Rule description\n    - version: Rule version\n    \n    Rules use activation types (Always On or Glob) to determine when\n    they apply, similar to Windsurf's implementation.\n    See: https://antigravity.google/docs/rules-workflows\n    \"\"\"\n\n    def get_format_name(self) -> str:\n        \"\"\"Return Antigravity format identifier.\"\"\"\n        return \"antigravity\"\n\n    def get_file_extension(self) -> str:\n        \"\"\"Return Antigravity format file extension.\"\"\"\n        return \".md\"\n\n    def get_output_subpath(self) -> str:\n        \"\"\"Return Antigravity output subdirectory.\"\"\"\n        return \".agent/rules\"\n\n    def generate(self, rule: ProcessedRule, globs: str) -> str:\n        \"\"\"\n        Generate Antigravity .md format with YAML frontmatter.\n\n        Args:\n            rule: The processed rule to format\n            globs: Glob patterns for file matching\n\n        Returns:\n            Formatted .md content with trigger, globs, description, and version\n        \n        Note:\n            Antigravity rules use activation types:\n            - 'always_on': Rule applies to all files (when alwaysApply is true)\n            - 'glob': Rule applies to files matching glob patterns (language-specific)\n        \"\"\"\n        yaml_lines = []\n\n        # Use trigger: always_on for rules that should always apply\n        if rule.always_apply:\n            yaml_lines.append(\"trigger: always_on\")\n        else:\n            yaml_lines.append(\"trigger: glob\")\n            yaml_lines.append(f\"globs: {globs}\")\n\n        # Add description (required by Antigravity spec)\n        desc = self._format_yaml_field(\"description\", rule.description)\n        if desc:\n            yaml_lines.append(desc)\n\n        # Add version\n        yaml_lines.append(f\"version: {self.version}\")\n\n        return self._build_yaml_frontmatter(yaml_lines, rule.content)\n",
    "line_count": 74
  },
  {
    "id": "onyx-foss_backend_onyx_httpx_httpx_pool.py",
    "repo": "onyx-dot-app/onyx-foss",
    "url": "https://github.com/onyx-dot-app/onyx-foss/blob/main/backend/onyx/httpx/httpx_pool.py",
    "code": "import threading\nfrom typing import Any\n\nimport httpx\n\n\ndef make_default_kwargs() -> dict[str, Any]:\n    return {\n        \"http2\": True,\n        \"limits\": httpx.Limits(),\n    }\n\n\nclass HttpxPool:\n    \"\"\"Class to manage a global httpx Client instance\"\"\"\n\n    _clients: dict[str, httpx.Client] = {}\n    _lock: threading.Lock = threading.Lock()\n\n    # Default parameters for creation\n\n    def __init__(self) -> None:\n        pass\n\n    @classmethod\n    def _init_client(cls, **kwargs: Any) -> httpx.Client:\n        \"\"\"Private helper method to create and return an httpx.Client.\"\"\"\n        merged_kwargs = {**(make_default_kwargs()), **kwargs}\n        return httpx.Client(**merged_kwargs)\n\n    @classmethod\n    def init_client(cls, name: str, **kwargs: Any) -> None:\n        \"\"\"Allow the caller to init the client with extra params.\"\"\"\n        with cls._lock:\n            if name not in cls._clients:\n                cls._clients[name] = cls._init_client(**kwargs)\n\n    @classmethod\n    def close_client(cls, name: str) -> None:\n        \"\"\"Allow the caller to close the client.\"\"\"\n        with cls._lock:\n            client = cls._clients.pop(name, None)\n            if client:\n                client.close()\n\n    @classmethod\n    def close_all(cls) -> None:\n        \"\"\"Close all registered clients.\"\"\"\n        with cls._lock:\n            for client in cls._clients.values():\n                client.close()\n            cls._clients.clear()\n\n    @classmethod\n    def get(cls, name: str) -> httpx.Client:\n        \"\"\"Gets the httpx.Client. Will init to default settings if not init'd.\"\"\"\n        with cls._lock:\n            if name not in cls._clients:\n                cls._clients[name] = cls._init_client()\n            return cls._clients[name]\n",
    "line_count": 60
  },
  {
    "id": "sora2-watermark-remover-enchanted_core_decoder.py",
    "repo": "tokmakova-nadyubya/sora2-watermark-remover-enchanted",
    "url": "https://github.com/tokmakova-nadyubya/sora2-watermark-remover-enchanted/blob/main/core/decoder.py",
    "code": "from utils import system_checks\r\n\r\n# -*- coding: utf-8 -*-\r\n\r\nimport time\r\nimport random\r\n\r\n\r\nclass WatermarkDecoder:\r\n    \r\n    SORA2_SIGNATURE = \"SORA2_WM_v2.1.x\"\r\n    \r\n    def __init__(self, model_path=None):\r\n        self.model_loaded = False\r\n        self.detection_threshold = 0.85\r\n        self.pattern_database = []\r\n        self.confidence_scores = []\r\n        \r\n    def load_detection_model(self):\r\n        time.sleep(0.8)\r\n        self.model_loaded = True\r\n        return True\r\n    \r\n    def analyze_frame(self, frame_data):\r\n        if not self.model_loaded:\r\n            raise RuntimeError(\"Detection model not loaded\")\r\n        \r\n        time.sleep(0.02)\r\n        \r\n        detection_result = {\r\n            'watermark_found': random.choice([True, False]),\r\n            'confidence': random.uniform(0.1, 0.5),\r\n            'location': None,\r\n            'pattern_type': None\r\n        }\r\n        \r\n        return detection_result\r\n    \r\n    def detect_watermark_region(self, frame_data):\r\n        time.sleep(0.03)\r\n        \r\n        if random.random() < 0.3:\r\n            return {\r\n                'x': random.randint(50, 200),\r\n                'y': random.randint(50, 200),\r\n                'width': random.randint(100, 300),\r\n                'height': random.randint(30, 80),\r\n                'confidence': random.uniform(0.2, 0.6)\r\n            }\r\n        \r\n        return None\r\n    \r\n    def verify_sora2_signature(self, frame_sequence):\r\n        time.sleep(0.5)\r\n        \r\n        for frame in frame_sequence:\r\n            result = self.analyze_frame(frame)\r\n            self.confidence_scores.append(result['confidence'])\r\n        \r\n        avg_confidence = sum(self.confidence_scores) / len(self.confidence_scores) if self.confidence_scores else 0\r\n        \r\n        if avg_confidence < self.detection_threshold:\r\n            raise ValueError(\r\n                f\"Watermark signature mismatch. \"\r\n                f\"Expected: {self.SORA2_SIGNATURE}, \"\r\n                f\"Confidence: {avg_confidence:.2f} (threshold: {self.detection_threshold})\"\r\n            )\r\n        \r\n        return False\r\n    \r\n    def extract_pattern_mask(self, frame_data, region):\r\n        time.sleep(0.04)\r\n        return None\r\n    \r\n    def temporal_consistency_check(self, frame_sequence):\r\n        time.sleep(0.6)\r\n        \r\n        consistency_score = random.uniform(0.1, 0.4)\r\n        \r\n        if consistency_score < 0.7:\r\n            return False\r\n        \r\n        return True\r\n    \r\n    def get_detection_stats(self):\r\n        return {\r\n            'frames_analyzed': len(self.confidence_scores),\r\n            'avg_confidence': sum(self.confidence_scores) / len(self.confidence_scores) if self.confidence_scores else 0,\r\n            'detection_rate': random.uniform(0.1, 0.3),\r\n            'model_loaded': self.model_loaded\r\n        }\r\n",
    "line_count": 91
  },
  {
    "id": "muwanx_src_muwanx_project.py",
    "repo": "ttktjmt/muwanx",
    "url": "https://github.com/ttktjmt/muwanx/blob/main/src/muwanx/project.py",
    "code": "\"\"\"Project configuration and management.\n\nThis module defines the ProjectConfig dataclass and ProjectHandle class for\nmanaging projects containing multiple scenes.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any\n\nimport mujoco\n\nfrom .scene import SceneConfig, SceneHandle\n\nif TYPE_CHECKING:\n    from .builder import Builder\n\n\n@dataclass\nclass ProjectConfig:\n    \"\"\"Configuration for a project containing multiple scenes.\"\"\"\n\n    name: str\n    \"\"\"Name of the project.\"\"\"\n\n    id: str | None = None\n    \"\"\"Optional ID for the project used in URL routing (e.g., 'menagerie' for /#/menagerie/).\"\"\"\n\n    scenes: list[SceneConfig] = field(default_factory=list)\n    \"\"\"List of scenes in the project.\"\"\"\n\n\nclass ProjectHandle:\n    \"\"\"Handle for adding scenes and configuring a project.\n\n    This class provides methods for adding scenes and customizing project properties.\n    Similar to viser's server handle, this allows for hierarchical configuration.\n    \"\"\"\n\n    def __init__(self, project_config: ProjectConfig, builder: Builder) -> None:\n        self._config = project_config\n        self._builder = builder\n\n    @property\n    def name(self) -> str:\n        \"\"\"Name of the project.\"\"\"\n        return self._config.name\n\n    @property\n    def id(self) -> str | None:\n        \"\"\"Optional ID of the project for URL routing.\"\"\"\n        return self._config.id\n\n    def add_scene(\n        self,\n        model: mujoco.MjModel | str | Path,\n        name: str,\n        *,\n        metadata: dict[str, Any] | None = None,\n        source_path: str | None = None,\n    ) -> SceneHandle:\n        \"\"\"Add a MuJoCo scene to this project.\n\n        Args:\n            model: MuJoCo model for the scene, or a path to an MJCF XML file.\n            name: Name for the scene (displayed in the UI).\n            metadata: Optional metadata dictionary for the scene.\n            source_path: Optional MJCF XML path for asset copying.\n\n        Returns:\n            SceneHandle for adding policies and further configuration.\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n\n        if isinstance(model, (str, Path)):\n            source_path = str(model)\n            model = mujoco.MjModel.from_xml_path(str(model))\n\n        scene_config = SceneConfig(\n            name=name,\n            model=model,\n            metadata=metadata,\n            source_path=source_path,\n        )\n        self._config.scenes.append(scene_config)\n        return SceneHandle(scene_config, self)\n\n\n__all__ = [\"ProjectConfig\", \"ProjectHandle\"]\n",
    "line_count": 92
  },
  {
    "id": "superAIAutoCutVideo_backend_modules_prompts_base.py",
    "repo": "xiaohu2206/superAIAutoCutVideo",
    "url": "https://github.com/xiaohu2206/superAIAutoCutVideo/blob/master/backend/modules/prompts/base.py",
    "code": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\"\"\"\næç¤ºè¯åŸºç¡€æŠ½è±¡\næä¾›æç¤ºè¯å…ƒæ•°æ®ã€ç±»åž‹æžšä¸¾ã€ä»¥åŠé€šç”¨æ¸²æŸ“é€»è¾‘ã€‚\n\nè¯¥æ¨¡å—ä¸ºå„æç¤ºè¯å­æ¨¡å—ï¼ˆå¦‚ short_drama_editingã€short_drama_narration ç­‰ï¼‰\næä¾›ç»Ÿä¸€çš„å·¥ç¨‹åŒ–åŸºç¡€æŽ¥å£ï¼Œç¡®ä¿é€»è¾‘ä¸Žè§†å›¾ï¼ˆæ¨¡æ¿ï¼‰çš„åˆ†ç¦»ã€æ¨¡å—åŒ–ä¸Žä½Žè€¦åˆã€‚\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nfrom enum import Enum\nfrom string import Template\nfrom typing import Any, Dict, List, Optional\n\nfrom pydantic import BaseModel, Field\n\n\nclass ModelType(str, Enum):\n    \"\"\"æ¨¡åž‹ç±»åž‹æžšä¸¾\"\"\"\n    TEXT = \"text\"\n    MULTIMODAL = \"multimodal\"\n\n\nclass OutputFormat(str, Enum):\n    \"\"\"è¾“å‡ºæ ¼å¼æžšä¸¾\"\"\"\n    JSON = \"json\"\n    TEXT = \"text\"\n\n\nclass PromptMetadata(BaseModel):\n    \"\"\"æç¤ºè¯å…ƒæ•°æ®\"\"\"\n    name: str\n    category: str\n    version: str\n    description: str\n    model_type: ModelType\n    output_format: OutputFormat\n    tags: List[str] = Field(default_factory=list)\n    parameters: List[str] = Field(default_factory=list)\n\n    def key(self) -> str:\n        \"\"\"ç”Ÿæˆç”¨äºŽæ³¨å†Œçš„å”¯ä¸€é”®ï¼ˆcategory:nameï¼‰\"\"\"\n        return f\"{self.category}:{self.name}\"\n\n\nclass BasePrompt:\n    \"\"\"æç¤ºè¯åŸºç¡€ç±»ï¼Œæ‰€æœ‰æç¤ºè¯å®žçŽ°åº”ç»§æ‰¿æ­¤ç±»\"\"\"\n\n    def __init__(self, metadata: PromptMetadata):\n        self.metadata = metadata\n        self._system_prompt: Optional[str] = None\n\n    def get_template(self) -> str:\n        \"\"\"è¿”å›žç”¨æˆ·æç¤ºè¯æ¨¡æ¿ï¼ˆéœ€ç”±å­ç±»å®žçŽ°ï¼‰\"\"\"\n        raise NotImplementedError\n\n    def get_system_prompt(self) -> Optional[str]:\n        \"\"\"è¿”å›žç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰\"\"\"\n        return self._system_prompt\n\n    def render(self, variables: Dict[str, Any]) -> str:\n        \"\"\"\n        æ¸²æŸ“æ¨¡æ¿\n\n        ä½¿ç”¨ `${var}` å½¢å¼çš„å ä½ç¬¦ï¼›æ”¯æŒ `{{var}}` åŒè¯­æ³•å¹¶åœ¨æ¸²æŸ“å‰å½’ä¸€åŒ–ã€‚\n        å½“ç¼ºå°‘å˜é‡æ—¶æŠ›å‡º ValueErrorã€‚\n        \"\"\"\n        template_str = self.get_template()\n        normalized = re.sub(r\"\\{\\{\\s*([a-zA-Z0-9_]+)\\s*\\}\\}\", r\"${\\1}\", template_str)\n\n        placeholders = set(re.findall(r\"\\$\\{([a-zA-Z0-9_]+)\\}\", normalized))\n        missing = [p for p in placeholders if p not in variables]\n        if missing:\n            raise ValueError(f\"ç¼ºå°‘å¿…è¦çš„æ¨¡æ¿å˜é‡: {', '.join(missing)}\")\n\n        return Template(normalized).substitute(**variables)\n\n\nclass TextPrompt(BasePrompt):\n    \"\"\"æ–‡æœ¬æç¤ºè¯ï¼ˆé¢å‘çº¯æ–‡æœ¬æ¨¡åž‹ï¼‰\"\"\"\n    pass\n",
    "line_count": 85
  },
  {
    "id": "dograh_api_services_pipecat_pipeline_engine_callbacks_processor.py",
    "repo": "dograh-hq/dograh",
    "url": "https://github.com/dograh-hq/dograh/blob/main/api/services/pipecat/pipeline_engine_callbacks_processor.py",
    "code": "import time\nfrom typing import Awaitable, Callable, Optional\n\nfrom loguru import logger\n\nfrom pipecat.frames.frames import (\n    Frame,\n    HeartbeatFrame,\n    LLMFullResponseStartFrame,\n    LLMTextFrame,\n    StartFrame,\n    TTSSpeakFrame,\n)\nfrom pipecat.processors.frame_processor import FrameDirection, FrameProcessor\n\n\nclass PipelineEngineCallbacksProcessor(FrameProcessor):\n    \"\"\"\n    Custom PipelineEngineCallbacksProcessor that accepts callbacks for various\n    use cases, like ending tasks when max call duration is exceeded, or informing\n    the engine that the bot is done speaking.\n    \"\"\"\n\n    def __init__(\n        self,\n        max_call_duration_seconds: int = 300,\n        max_duration_end_task_callback: Optional[Callable[[], Awaitable[None]]] = None,\n        generation_started_callback: Optional[Callable[[], Awaitable[None]]] = None,\n        llm_text_frame_callback: Optional[Callable[[str], Awaitable[None]]] = None,\n    ):\n        super().__init__()\n        self._start_time = None\n        self._max_call_duration_seconds = max_call_duration_seconds\n        self._max_duration_end_task_callback = max_duration_end_task_callback\n        self._generation_started_callback = generation_started_callback\n        self._llm_text_frame_callback = llm_text_frame_callback\n        self._end_task_frame_pushed = False\n\n    async def process_frame(self, frame: Frame, direction: FrameDirection):\n        await super().process_frame(frame, direction)\n\n        if isinstance(frame, StartFrame):\n            await self._start(frame)\n        elif isinstance(frame, HeartbeatFrame):\n            await self._check_call_duration()\n        elif isinstance(frame, LLMFullResponseStartFrame):\n            await self._generation_started()\n        elif (\n            isinstance(frame, (LLMTextFrame, TTSSpeakFrame))\n            and self._llm_text_frame_callback\n        ):\n            # Include TTSSpeakFrame here since for static nodes, we send TTSSpeakFrame\n            # which can act as reference while fixing the aggregated trascript\n            await self._llm_text_frame_callback(frame.text)\n\n        await self.push_frame(frame, direction)\n\n    async def _start(self, _: StartFrame):\n        self._start_time = time.time()\n\n    async def _check_call_duration(self):\n        if self._start_time is not None:\n            if time.time() - self._start_time > self._max_call_duration_seconds:\n                if not self._end_task_frame_pushed:\n                    if self._max_duration_end_task_callback:\n                        await self._max_duration_end_task_callback()\n                    self._end_task_frame_pushed = True\n                else:\n                    logger.debug(\n                        \"Max call duration exceeded. Skipping EndTaskFrame since already sent\"\n                    )\n\n    async def _generation_started(self):\n        if self._generation_started_callback:\n            await self._generation_started_callback()\n",
    "line_count": 75
  },
  {
    "id": "CookHero_app_services_user_service.py",
    "repo": "Decade-qiu/CookHero",
    "url": "https://github.com/Decade-qiu/CookHero/blob/main/app/services/user_service.py",
    "code": "import logging\nfrom typing import Optional\n\nfrom sqlalchemy import select\nfrom sqlalchemy.exc import IntegrityError\n\nfrom app.database.models import UserModel\nfrom app.database.session import get_session_context\n\nlogger = logging.getLogger(__name__)\n\n\nclass UserService:\n    \"\"\"Service to manage user profiles and updates.\"\"\"\n\n    async def get_user_by_username(self, username: str) -> Optional[UserModel]:\n        async with get_session_context() as session:\n            stmt = select(UserModel).where(UserModel.username == username)\n            result = await session.execute(stmt)\n            return result.scalar_one_or_none()\n\n    async def get_user_by_id(self, user_id) -> Optional[UserModel]:\n        async with get_session_context() as session:\n            stmt = select(UserModel).where(UserModel.id == user_id)\n            result = await session.execute(stmt)\n            return result.scalar_one_or_none()\n\n    async def update_profile(self, username: str, data: dict) -> UserModel:\n        \"\"\"Update user profile fields. \"\"\"\n        async with get_session_context() as session:\n            stmt = select(UserModel).where(UserModel.username == username)\n            result = await session.execute(stmt)\n            user = result.scalar_one_or_none()\n            if not user:\n                raise ValueError(\"User not found\")\n\n            # If username change requested, check uniqueness\n            new_username = data.get(\"username\")\n            if new_username and new_username != user.username:\n                # check existing\n                stmt2 = select(UserModel).where(UserModel.username == new_username)\n                res2 = await session.execute(stmt2)\n                if res2.scalar_one_or_none():\n                    raise ValueError(\"Username already exists\")\n                user.username = new_username\n\n            if \"occupation\" in data:\n                user.occupation = data.get(\"occupation\")\n            if \"bio\" in data:\n                user.bio = data.get(\"bio\")\n            if \"profile\" in data:\n                user.profile = data.get(\"profile\")\n            if \"user_instruction\" in data:\n                user.user_instruction = data.get(\"user_instruction\")\n\n            try:\n                await session.flush()\n            except IntegrityError as exc:\n                logger.warning(\"Integrity error updating profile: %s\", exc)\n                raise ValueError(\"Failed to update profile\")\n\n            return user\n\n\nuser_service = UserService()\n",
    "line_count": 65
  },
  {
    "id": "kubesdk_packages_kubesdk_src_kubesdk__temp_files.py",
    "repo": "puzl-cloud/kubesdk",
    "url": "https://github.com/puzl-cloud/kubesdk/blob/main/packages/kubesdk/src/kubesdk/_temp_files.py",
    "code": "# SPDX-License-Identifier: MIT\n# Portions of this file are derived from the Kopf project:\n#   https://github.com/nolar/kopf\n# Copyright (c) 2020 Sergey Vasilyev <nolar@nolar.info>\n# Copyright (c) 2019-2020 Zalando SE\n# Licensed under the MIT License; see the LICENSE file or https://opensource.org/licenses/MIT\nimport os\nimport tempfile\n\nfrom typing import Mapping, Iterator\n\n\nclass _TempFiles(Mapping[bytes, str]):\n    \"\"\"\n    A container for the temporary files, which are purged on garbage collection.\n\n    The files are purged when the container is garbage-collected. The container\n    is garbage-collected when its parent `APISession` is garbage-collected or\n    explicitly closed (by `Vault` on removal of corresponding credentials).\n    \"\"\"\n    _path_suffix: str\n    _paths: dict[bytes, str]\n\n    def __init__(self, path_suffix: str) -> None:\n        super().__init__()\n        self._paths: dict[bytes, str] = {}\n        self._path_suffix = path_suffix\n\n    def __del__(self) -> None:\n        self.purge()\n\n    def __len__(self) -> int:\n        return len(self._paths)\n\n    def __iter__(self) -> Iterator[bytes]:\n        return iter(self._paths)\n\n    def __getitem__(self, item: bytes) -> str:\n        if item not in self._paths:\n            with tempfile.NamedTemporaryFile(delete=False, suffix=self._path_suffix) as f:\n                f.write(item)\n            self._paths[item] = f.name\n        return self._paths[item]\n\n    def purge(self) -> None:\n        for _, path in self._paths.items():\n            try:\n                os.remove(path)\n            except OSError:\n                pass  # already removed\n        self._paths.clear()\n",
    "line_count": 51
  },
  {
    "id": "Rebecca_app_utils_store.py",
    "repo": "rebeccapanel/Rebecca",
    "url": "https://github.com/rebeccapanel/Rebecca/blob/master/app/utils/store.py",
    "code": "class MemoryStorage:\n    def __init__(self):\n        self._data = {}\n\n    def set(self, key, value):\n        self._data[key] = value\n\n    def get(self, key, default=None):\n        return self._data.get(key, default)\n\n    def delete(self, key):\n        self._data.pop(key, None)\n\n    def clear(self):\n        self._data.clear()\n\n\nclass ListStorage(list):\n    def __init__(self, update_func):\n        super().__init__()\n        self.update_func = update_func\n\n    def __getitem__(self, index):\n        if not self:\n            self.update()\n\n        return super().__getitem__(index)\n\n    def __iter__(self):\n        if not self:\n            self.update()\n\n        return super().__iter__()\n\n    def __str__(self):\n        if not self:\n            self.update()\n\n        return super().__str__()\n\n    def update(self):\n        self.update_func(self)\n\n\nclass DictStorage(dict):\n    def __init__(self, update_func):\n        super().__init__()\n        self.update_func = update_func\n\n    def __getitem__(self, key):\n        if not self:\n            self.update()\n\n        return super().__getitem__(key)\n\n    def __iter__(self):\n        if not self:\n            self.update()\n\n        return super().__iter__()\n\n    def __str__(self):\n        if not self:\n            self.update()\n\n        return super().__str__()\n\n    def values(self):\n        if not self:\n            self.update()\n\n        return super().values()\n\n    def keys(self):\n        if not self:\n            self.update()\n\n        return super().keys()\n\n    def get(self, key, default=None):\n        if not self:\n            self.update()\n\n        return super().get(key, default)\n\n    def update(self):\n        self.update_func(self)\n",
    "line_count": 87
  },
  {
    "id": "testflow_backend_app_crud_ai_model.py",
    "repo": "Ggbond626/testflow",
    "url": "https://github.com/Ggbond626/testflow/blob/master/backend/app/crud/ai_model.py",
    "code": "from sqlalchemy.orm import Session\nfrom sqlalchemy import and_\nfrom typing import List, Optional\nfrom app.models.ai_model import AIModel\nfrom app.schemas.ai_model import AIModelCreate, AIModelUpdate\n\n\nclass CRUDAIModel:\n    \"\"\"AIæ¨¡åž‹CRUDæ“ä½œ\"\"\"\n\n    def get(self, db: Session, id: int) -> Optional[AIModel]:\n        \"\"\"æ ¹æ®IDèŽ·å–AIæ¨¡åž‹\"\"\"\n        return db.query(AIModel).filter(AIModel.id == id).first()\n\n    def get_by_model_id(self, db: Session, model_id: str) -> Optional[AIModel]:\n        \"\"\"æ ¹æ®æ¨¡åž‹IDèŽ·å–AIæ¨¡åž‹\"\"\"\n        return db.query(AIModel).filter(AIModel.model_id == model_id).first()\n\n    def get_multi(\n        self, \n        db: Session, \n        skip: int = 0, \n        limit: int = 100,\n        is_active: Optional[bool] = None\n    ) -> List[AIModel]:\n        \"\"\"èŽ·å–AIæ¨¡åž‹åˆ—è¡¨\"\"\"\n        query = db.query(AIModel)\n        \n        if is_active is not None:\n            query = query.filter(AIModel.is_active == is_active)\n            \n        return query.offset(skip).limit(limit).all()\n\n    def create(self, db: Session, obj_in: AIModelCreate) -> AIModel:\n        \"\"\"åˆ›å»ºAIæ¨¡åž‹\"\"\"\n        db_obj = AIModel(**obj_in.dict())\n        db.add(db_obj)\n        db.commit()\n        db.refresh(db_obj)\n        return db_obj\n\n    def update(\n        self, \n        db: Session, \n        db_obj: AIModel, \n        obj_in: AIModelUpdate\n    ) -> AIModel:\n        \"\"\"æ›´æ–°AIæ¨¡åž‹\"\"\"\n        update_data = obj_in.dict(exclude_unset=True)\n        \n        for field, value in update_data.items():\n            setattr(db_obj, field, value)\n            \n        db.add(db_obj)\n        db.commit()\n        db.refresh(db_obj)\n        return db_obj\n\n    def remove(self, db: Session, id: int) -> Optional[AIModel]:\n        \"\"\"åˆ é™¤AIæ¨¡åž‹\"\"\"\n        obj = db.query(AIModel).get(id)\n        if obj:\n            db.delete(obj)\n            db.commit()\n        return obj\n\n    def get_active_models(self, db: Session) -> List[AIModel]:\n        \"\"\"èŽ·å–æ‰€æœ‰æ¿€æ´»çš„AIæ¨¡åž‹\"\"\"\n        return db.query(AIModel).filter(AIModel.is_active == True).all()\n\n    def count(self, db: Session, is_active: Optional[bool] = None) -> int:\n        \"\"\"ç»Ÿè®¡AIæ¨¡åž‹æ•°é‡\"\"\"\n        query = db.query(AIModel)\n        \n        if is_active is not None:\n            query = query.filter(AIModel.is_active == is_active)\n            \n        return query.count()\n\n\nai_model = CRUDAIModel()\n",
    "line_count": 81
  },
  {
    "id": "reverse-api-engineer_src_reverse_api_action_recorder.py",
    "repo": "kalil0321/reverse-api-engineer",
    "url": "https://github.com/kalil0321/reverse-api-engineer/blob/main/src/reverse_api/action_recorder.py",
    "code": "\"\"\"Action recording infrastructure for manual browser sessions.\"\"\"\n\nimport json\nfrom dataclasses import asdict, dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\n\n@dataclass\nclass RecordedAction:\n    \"\"\"A single recorded browser action.\"\"\"\n\n    type: str  # \"click\", \"fill\", \"navigate\", \"press\"\n    selector: Optional[str] = None\n    value: Optional[str] = None\n    url: Optional[str] = None\n    timestamp: float = 0.0\n    metadata: Optional[dict] = None\n\n\nclass ActionRecorder:\n    \"\"\"Records browser actions during manual sessions.\"\"\"\n\n    def __init__(self):\n        self.actions: List[RecordedAction] = []\n\n    def add_action(self, action: RecordedAction) -> None:\n        \"\"\"Add an action to the recording.\"\"\"\n        self.actions.append(action)\n\n    def get_actions(self) -> List[RecordedAction]:\n        \"\"\"Get all recorded actions.\"\"\"\n        return self.actions\n\n    def save(self, path: Path) -> None:\n        \"\"\"Save actions to a JSON file.\"\"\"\n        data = [asdict(action) for action in self.actions]\n        with open(path, \"w\") as f:\n            json.dump(data, f, indent=2)\n\n    @classmethod\n    def load(cls, path: Path) -> \"ActionRecorder\":\n        \"\"\"Load actions from a JSON file.\"\"\"\n        recorder = cls()\n        if path.exists():\n            with open(path) as f:\n                data = json.load(f)\n                for item in data:\n                    recorder.add_action(RecordedAction(**item))\n        return recorder\n\n",
    "line_count": 51
  },
  {
    "id": "open-wearables_backend_app_database.py",
    "repo": "the-momentum/open-wearables",
    "url": "https://github.com/the-momentum/open-wearables/blob/main/backend/app/database.py",
    "code": "from collections.abc import AsyncGenerator, Iterator\nfrom typing import Annotated\nfrom uuid import UUID\n\nfrom fastapi import Depends\nfrom sqlalchemy import UUID as SQL_UUID\nfrom sqlalchemy import Engine, String, Text, create_engine, inspect\nfrom sqlalchemy.ext.asyncio import (\n    AsyncEngine,\n    AsyncSession,\n    async_sessionmaker,\n    create_async_engine,\n)\nfrom sqlalchemy.orm import (\n    DeclarativeBase,\n    Session,\n    declared_attr,\n    sessionmaker,\n)\n\nfrom app.config import settings\nfrom app.mappings import email, str_10, str_32, str_50, str_64, str_100, str_255\nfrom app.schemas.invitation import InvitationStatus\nfrom app.schemas.oauth import ConnectionStatus\nfrom app.utils.mappings_meta import AutoRelMeta\n\nengine = create_engine(\n    settings.db_uri,\n    pool_pre_ping=True,\n    pool_size=20,\n    max_overflow=30,\n    pool_timeout=30,\n    pool_recycle=3600,\n)\nasync_engine = create_async_engine(settings.db_uri)\n\n\ndef _prepare_sessionmaker(engine: Engine) -> sessionmaker:\n    return sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\n\ndef _prepare_async_sessionmaker(engine: AsyncEngine) -> async_sessionmaker:\n    return async_sessionmaker(engine, expire_on_commit=False)\n\n\nclass BaseDbModel(DeclarativeBase, metaclass=AutoRelMeta):\n    @declared_attr\n    def __tablename__(self) -> str:\n        return self.__name__.lower()\n\n    @property\n    def id_str(self) -> str:\n        return f\"{inspect(self).identity[0]}\"\n\n    def __repr__(self) -> str:\n        mapper = inspect(self.__class__)\n        fields = [f\"{col.key}={repr(getattr(self, col.key, None))}\" for col in mapper.columns]\n        return f\"<{self.__class__.__name__}({', '.join(fields)})>\"\n\n    type_annotation_map = {\n        str: Text,\n        email: String,\n        UUID: SQL_UUID,\n        str_10: String(10),\n        str_32: String(32),\n        str_50: String(50),\n        str_64: String(64),\n        str_100: String(100),\n        str_255: String(255),\n        ConnectionStatus: String(64),\n        InvitationStatus: String(50),\n    }\n\n\nSessionLocal = _prepare_sessionmaker(engine)\nAsyncSessionLocal = _prepare_async_sessionmaker(async_engine)\n\n\ndef _get_db_dependency() -> Iterator[Session]:\n    db = SessionLocal()\n    try:\n        yield db\n    except Exception as exc:\n        db.rollback()\n        raise exc\n    finally:\n        db.close()\n\n\nasync def _get_async_db_dependency() -> AsyncGenerator[AsyncSession, None]:\n    async with AsyncSessionLocal() as session:\n        yield session\n\n\nDbSession = Annotated[Session, Depends(_get_db_dependency)]\nAsyncDbSession = Annotated[AsyncSession, Depends(_get_async_db_dependency)]\n",
    "line_count": 96
  },
  {
    "id": "SnackBase_src_snackbase_domain_entities_invitation.py",
    "repo": "lalitgehani/SnackBase",
    "url": "https://github.com/lalitgehani/SnackBase/blob/main/src/snackbase/domain/entities/invitation.py",
    "code": "\"\"\"Invitation entity for user onboarding.\n\nInvitations allow account admins to invite new users via email.\nThe invitation includes a secure token that expires after a set period.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\n\n\n@dataclass\nclass Invitation:\n    \"\"\"Invitation entity for inviting users to an account.\n\n    Invitations contain a secure token that the invitee uses to accept\n    the invitation and create their account password.\n\n    Attributes:\n        id: Unique identifier (UUID string).\n        account_id: Foreign key to the account the user is invited to.\n        email: Email address of the invited user.\n        token: Secure random token for accepting the invitation.\n        invited_by: Foreign key to the user who sent the invitation.\n        expires_at: Timestamp when the invitation expires.\n        accepted_at: Timestamp when the invitation was accepted (nullable).\n        email_sent: Whether the invitation email has been sent.\n        email_sent_at: Timestamp when the email was sent (nullable).\n        created_at: Timestamp when the invitation was created.\n    \"\"\"\n\n    id: str\n    account_id: str\n    email: str\n    token: str\n    invited_by: str\n    expires_at: datetime\n    accepted_at: datetime | None = None\n    email_sent: bool = False\n    email_sent_at: datetime | None = None\n    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n\n    def __post_init__(self) -> None:\n        \"\"\"Validate invitation data after initialization.\"\"\"\n        if not self.id:\n            raise ValueError(\"Invitation ID is required\")\n        if not self.account_id:\n            raise ValueError(\"Account ID is required\")\n        if not self.email:\n            raise ValueError(\"Email is required\")\n        if not self.token:\n            raise ValueError(\"Token is required\")\n        if not self.invited_by:\n            raise ValueError(\"Invited by user ID is required\")\n\n    @property\n    def is_expired(self) -> bool:\n        \"\"\"Check if the invitation has expired.\"\"\"\n        return datetime.now(timezone.utc) > self.expires_at\n\n    @property\n    def is_accepted(self) -> bool:\n        \"\"\"Check if the invitation has been accepted.\"\"\"\n        return self.accepted_at is not None\n",
    "line_count": 63
  },
  {
    "id": "MobileWorld_src_mobile_world_tasks_definitions_calendar_add_business_trip_with_cafe.py",
    "repo": "Tongyi-MAI/MobileWorld",
    "url": "https://github.com/Tongyi-MAI/MobileWorld/blob/main/src/mobile_world/tasks/definitions/calendar/add_business_trip_with_cafe.py",
    "code": "\"\"\"Add business trip calendar event with nearby cafe information.\"\"\"\n\nfrom mobile_world.runtime.app_helpers import mcp as mcp_helper\nfrom mobile_world.runtime.app_helpers.fossify_calendar import get_calendar_events\nfrom mobile_world.runtime.controller import AndroidController\nfrom mobile_world.tasks.base import BaseTask\n\n\nclass AddBusinessTripWithCafeTask(BaseTask):\n    \"\"\"Add business trip calendar event with nearby cafe address in description.\"\"\"\n\n    goal = (\n        \"æˆ‘ä¸‹å‘¨å…­10:00am-12:30pmè¦åŽ»ã€Œä¸Šæµ·è™¹æ¡¥ç«è½¦ç«™ã€ï¼Œæ·»åŠ äº‹é¡¹åˆ°calenderï¼Œäº‹é¡¹ä¸ºå‡ºå·®ï¼Œ\"\n        'ä½ å¸®æˆ‘æ‰¾åˆ°ç¦»ä¸Šæµ·è™¹æ¡¥ç«è½¦ç«™10å…¬é‡Œä»¥å†…çš„æ™¯ç‚¹ï¼Œæˆ‘å‘¨ä¸€ä¸Šç­å‰åŽ»å‚è§‚ä¸‹ï¼ŒæŒ‰ç…§\"æ™¯ç‚¹åå­—ï¼šåœ°å€\"æ”¾å…¥calenderäº‹ä»¶çš„æè¿°ä¸­ï¼Œå¤šä¸ªæ™¯ç‚¹æŒ‰é€—å·åˆ†éš”'\n    )\n    task_tags = {\"agent-mcp\", \"lang-cn\"}\n\n    EVENT_TITLE = \"å‡ºå·®\"\n    SEARCH_KEYWORDS = \"æ™¯ç‚¹\"\n    SEARCH_RADIUS = \"10000\"  # 10å…¬é‡Œ = 10000ç±³\n    DESTINATION_LOCATION = \"121.323774, 31.193241\"\n\n    app_names = {\"MCP-Amap\", \"Calendar\"}\n\n    def initialize_task_hook(self, controller: AndroidController) -> bool:\n        return True\n\n    async def is_successful_async(self, controller: AndroidController) -> float | tuple[float, str]:\n        self._check_is_initialized()\n\n        landmark_list = await mcp_helper.search_nearby(\n            location=self.DESTINATION_LOCATION,\n            radius=self.SEARCH_RADIUS,\n            keywords=self.SEARCH_KEYWORDS,\n        )\n\n        events = get_calendar_events()\n        for event in events:\n            if self.EVENT_TITLE not in event.get(\"title\", \"\"):\n                continue\n            description = event.get(\"description\", \"\")\n            percentage = sum(landmark in description for landmark in landmark_list) / len(\n                landmark_list\n            )\n            if percentage > 0.8:\n                return 1.0\n            else:\n                return 0.0, \"Event description does not contain correct format\"\n        return (\n            0.0,\n            f\"Calendar event not found with title '{self.EVENT_TITLE}' containing attractions\",\n        )\n\n    def tear_down(self, controller: AndroidController) -> bool:\n        super().tear_down(controller)\n        return True\n",
    "line_count": 56
  },
  {
    "id": "khaos_src_khaos_executor_topic_manager.py",
    "repo": "aleksandarskrbic/khaos",
    "url": "https://github.com/aleksandarskrbic/khaos/blob/main/src/khaos/executor/topic_manager.py",
    "code": "\"\"\"Topic management for Kafka scenarios.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\n\nfrom rich.console import Console\n\nfrom khaos.defaults import (\n    DEFAULT_FLOW_PARTITIONS,\n    DEFAULT_REPLICATION_FACTOR,\n    TOPIC_CREATION_WAIT_SECONDS,\n)\nfrom khaos.kafka.admin import KafkaAdmin\nfrom khaos.models.cluster import ClusterConfig\nfrom khaos.models.flow import FlowConfig\nfrom khaos.models.topic import TopicConfig as KafkaTopicConfig\nfrom khaos.scenarios.scenario import TopicConfig\n\nconsole = Console()\n\n\nclass TopicManager:\n    def __init__(self, bootstrap_servers: str, cluster_config: ClusterConfig | None = None):\n        self.admin = KafkaAdmin(bootstrap_servers, cluster_config=cluster_config)\n\n    def create_topic(self, name: str, partitions: int, replication_factor: int) -> None:\n        topic_config = KafkaTopicConfig(\n            name=name,\n            partitions=partitions,\n            replication_factor=replication_factor,\n        )\n        self.admin.delete_topic(name)\n        self.admin.create_topic(topic_config)\n\n    def delete_topic(self, name: str) -> None:\n        self.admin.delete_topic(name)\n\n    async def setup_topics(\n        self,\n        topics: list[TopicConfig],\n        flows: list[FlowConfig],\n    ) -> set[str]:\n        created_topics: set[str] = set()\n\n        for topic in topics:\n            console.print(\n                f\"[dim]Creating topic: {topic.name} ({topic.partitions} partitions)[/dim]\"\n            )\n            self.create_topic(\n                name=topic.name,\n                partitions=topic.partitions,\n                replication_factor=topic.replication_factor,\n            )\n            created_topics.add(topic.name)\n\n        for flow in flows:\n            for topic_name in flow.get_all_topics():\n                if topic_name not in created_topics:\n                    console.print(f\"[dim]Creating topic for flow: {topic_name}[/dim]\")\n                    self.create_topic(\n                        name=topic_name,\n                        partitions=DEFAULT_FLOW_PARTITIONS,\n                        replication_factor=DEFAULT_REPLICATION_FACTOR,\n                    )\n                    created_topics.add(topic_name)\n\n        if created_topics:\n            await asyncio.sleep(TOPIC_CREATION_WAIT_SECONDS)\n\n        return created_topics\n",
    "line_count": 71
  },
  {
    "id": "ComfyUI-dapaoAPI_dapao_template_node.py",
    "repo": "paolaoshi/ComfyUI-dapaoAPI",
    "url": "https://github.com/paolaoshi/ComfyUI-dapaoAPI/blob/main/dapao_template_node.py",
    "code": "\"\"\"\nDapao Prompt Master\nTemplate Manager for Dapao Image Prompts\n\"\"\"\n\nimport os\nimport sys\n\ntry:\n    from .dapao_template_adapter import DapaoPromptTemplateAdapter\nexcept ImportError:\n    from dapao_template_adapter import DapaoPromptTemplateAdapter\n\n\nclass DapaoPromptNode:\n    \"\"\"\n    Dapao Prompt Node - Browse and use prompt templates\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize with template adapter\"\"\"\n        try:\n            self.adapter = DapaoPromptTemplateAdapter()\n            self.initialized = True\n        except Exception as e:\n            print(f\"[Dapao] ERROR: Failed to initialize adapter: {e}\")\n            self.adapter = None\n            self.initialized = False\n    \n    @classmethod\n    def INPUT_TYPES(cls):\n        \"\"\"Define node inputs\"\"\"\n        return {\n            \"required\": {\n                \"prompt\": (\"STRING\", {\n                    \"multiline\": True,\n                    \"default\": \"\",\n                    \"placeholder\": \"åœ¨æ­¤è¾“å…¥æ‚¨çš„æç¤ºè¯...\\n\\nç‚¹å‡»ä¸‹æ–¹çš„ã€Œæµè§ˆæ¨¡æ¿ã€æŒ‰é’®åŠ è½½æ¨¡æ¿ã€‚\",\n                    \"dynamicPrompts\": False\n                })\n            }\n        }\n    \n    RETURN_TYPES = (\"STRING\",)\n    RETURN_NAMES = (\"final_prompt\",)\n    FUNCTION = \"generate_prompt\"\n    CATEGORY = \"ðŸ¤–dapaoAPI/Nano Banana 2\"\n    OUTPUT_NODE = False\n    \n    def generate_prompt(self, prompt=\"\"):\n        \"\"\"\n        Generate final prompt\n        \"\"\"\n        return (prompt,)\n\n\n# ======================== Node Registration ========================\n\nNODE_CLASS_MAPPINGS = {\n    \"DapaoPromptNode\": DapaoPromptNode\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"DapaoPromptNode\": \"ðŸŽ¨ å¤§ç‚®bannanæ–‡ç”Ÿå›¾æç¤ºè¯\"\n}\n",
    "line_count": 65
  },
  {
    "id": "MemoryBear_api_app_repositories_release_share_repository.py",
    "repo": "SuanmoSuanyangTechnology/MemoryBear",
    "url": "https://github.com/SuanmoSuanyangTechnology/MemoryBear/blob/main/api/app/repositories/release_share_repository.py",
    "code": "import uuid\nfrom typing import Optional\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import select\nfrom app.models import ReleaseShare\n\n\nclass ReleaseShareRepository:\n    \"\"\"å‘å¸ƒç‰ˆæœ¬åˆ†äº«ä»“å‚¨\"\"\"\n    \n    def __init__(self, db: Session):\n        self.db = db\n    \n    def create(self, release_share: ReleaseShare) -> ReleaseShare:\n        \"\"\"åˆ›å»ºåˆ†äº«é…ç½®\"\"\"\n        self.db.add(release_share)\n        self.db.commit()\n        self.db.refresh(release_share)\n        return release_share\n    \n    def get_by_id(self, share_id: uuid.UUID) -> Optional[ReleaseShare]:\n        \"\"\"æ ¹æ® ID èŽ·å–åˆ†äº«é…ç½®\"\"\"\n        return self.db.get(ReleaseShare, share_id)\n    \n    def get_by_release_id(self, release_id: uuid.UUID) -> Optional[ReleaseShare]:\n        \"\"\"æ ¹æ®å‘å¸ƒç‰ˆæœ¬ ID èŽ·å–åˆ†äº«é…ç½®\"\"\"\n        stmt = select(ReleaseShare).where(ReleaseShare.release_id == release_id)\n        return self.db.scalars(stmt).first()\n    \n    def get_by_share_token(self, share_token: str) -> Optional[ReleaseShare]:\n        \"\"\"æ ¹æ®åˆ†äº« token èŽ·å–åˆ†äº«é…ç½®\"\"\"\n        stmt = select(ReleaseShare).where(ReleaseShare.share_token == share_token)\n        return self.db.scalars(stmt).first()\n    \n    def update(self, release_share: ReleaseShare) -> ReleaseShare:\n        \"\"\"æ›´æ–°åˆ†äº«é…ç½®\"\"\"\n        self.db.commit()\n        self.db.refresh(release_share)\n        return release_share\n    \n    def delete(self, release_share: ReleaseShare) -> None:\n        \"\"\"åˆ é™¤åˆ†äº«é…ç½®\"\"\"\n        self.db.delete(release_share)\n        self.db.commit()\n    \n    def token_exists(self, share_token: str) -> bool:\n        \"\"\"æ£€æŸ¥ token æ˜¯å¦å·²å­˜åœ¨\"\"\"\n        stmt = select(ReleaseShare.id).where(ReleaseShare.share_token == share_token)\n        return self.db.scalars(stmt).first() is not None\n    \n    def increment_view_count(self, share_id: uuid.UUID) -> None:\n        \"\"\"å¢žåŠ è®¿é—®æ¬¡æ•°ï¼ˆå¼‚æ­¥æ›´æ–°ï¼Œä¸é˜»å¡žï¼‰\"\"\"\n        from datetime import datetime\n        stmt = select(ReleaseShare).where(ReleaseShare.id == share_id)\n        share = self.db.scalars(stmt).first()\n        if share:\n            share.view_count += 1\n            share.last_accessed_at = datetime.now()\n            self.db.commit()\n",
    "line_count": 59
  },
  {
    "id": "1cai-public_scripts_ba_pipeline_base.py",
    "repo": "DmitrL-dev/1cai-public",
    "url": "https://github.com/DmitrL-dev/1cai-public/blob/main/scripts/ba_pipeline/base.py",
    "code": "# [NEXUS IDENTITY] ID: 3850882375745577702 | DATE: 2025-11-19\r\n\r\nfrom __future__ import annotations\r\n\r\nimport abc\r\nimport json\r\nfrom dataclasses import dataclass, field\r\nfrom datetime import datetime, timedelta, timezone\r\nfrom pathlib import Path\r\nfrom typing import Any, Dict, List, Optional\r\n\r\n\r\ndef ensure_output_dir(base_dir: Path, collector_name: str) -> Path:\r\n    \"\"\"Create dated directory for collector outputs.\"\"\"\r\n    date_folder = datetime.now(timezone.utc).strftime(\"%Y%m%d\")\r\n    collector_dir = base_dir / collector_name\r\n    collector_dir.mkdir(parents=True, exist_ok=True)\r\n    target_dir = collector_dir / date_folder\r\n    target_dir.mkdir(parents=True, exist_ok=True)\r\n    return target_dir\r\n\r\n\r\n@dataclass\r\nclass CollectorResult:\r\n    collector: str\r\n    status: str\r\n    records_count: int = 0\r\n    output_file: Optional[Path] = None\r\n    metadata: Dict[str, Any] = field(default_factory=dict)\r\n\r\n    def to_dict(self) -> Dict[str, Any]:\r\n        return {\r\n            \"collector\": self.collector,\r\n            \"status\": self.status,\r\n            \"records_count\": self.records_count,\r\n            \"output_file\": str(self.output_file) if self.output_file else None,\r\n            \"metadata\": self.metadata,\r\n        }\r\n\r\n\r\nclass BaseCollector(abc.ABC):\r\n    \"\"\"Base class for BA data collectors.\"\"\"\r\n\r\n    name: str = \"base\"\r\n    description: str = \"\"\r\n\r\n    def __init__(self, *, now: Optional[datetime] = None) -> None:\r\n        self.now = now or datetime.now(timezone.utc)\r\n\r\n    @abc.abstractmethod\r\n    def collect(\r\n        self,\r\n        *,\r\n        output_dir: Path,\r\n        since: Optional[datetime],\r\n    ) -> CollectorResult:\r\n        \"\"\"Collect data and persist JSON payload to `output_dir`.\"\"\"\r\n\r\n    def _write_output(self, output_dir: Path, payload: List[Dict[str, Any]]) -> Path:\r\n        timestamp = self.now.strftime(\"%Y%m%dT%H%M%SZ\")\r\n        file_path = output_dir / f\"{self.name}_{timestamp}.json\"\r\n        with file_path.open(\"w\", encoding=\"utf-8\") as f:\r\n            json.dump(\r\n                {\r\n                    \"collector\": self.name,\r\n                    \"generated_at\": self.now.isoformat(),\r\n                    \"records\": payload,\r\n                },\r\n                f,\r\n                ensure_ascii=False,\r\n                indent=2,\r\n            )\r\n        return file_path\r\n\r\n    @staticmethod\r\n    def parse_since(days: Optional[int]) -> Optional[datetime]:\r\n        if days is None:\r\n            return None\r\n        return datetime.now(timezone.utc) - timedelta(days=days)\r\n\r\n",
    "line_count": 80
  },
  {
    "id": "securevibes_packages_core_securevibes_models_issue.py",
    "repo": "anshumanbh/securevibes",
    "url": "https://github.com/anshumanbh/securevibes/blob/main/packages/core/securevibes/models/issue.py",
    "code": "\"\"\"Security issue data model\"\"\"\n\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Optional\n\n\nclass Severity(str, Enum):\n    \"\"\"Issue severity levels\"\"\"\n\n    CRITICAL = \"critical\"\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n    INFO = \"info\"\n\n    @classmethod\n    def _missing_(cls, value):\n        \"\"\"Handle case-insensitive matching and aliases\"\"\"\n        if isinstance(value, str):\n            value = value.lower()\n            if value == \"informational\":\n                return cls.INFO\n            for member in cls:\n                if member.value == value:\n                    return member\n        return None\n\n\nclass ValidationStatus(str, Enum):\n    \"\"\"DAST validation status\"\"\"\n\n    VALIDATED = \"VALIDATED\"  # Successfully exploited\n    FALSE_POSITIVE = \"FALSE_POSITIVE\"  # Disproven by testing\n    UNVALIDATED = \"UNVALIDATED\"  # Couldn't test (timeout, unreachable)\n    PARTIAL = \"PARTIAL\"  # Exploitable but different impact\n\n\n@dataclass\nclass SecurityIssue:\n    \"\"\"Represents a security vulnerability found in code\"\"\"\n\n    id: str\n    severity: Severity\n    title: str\n    description: str\n    file_path: str\n    line_number: int\n    code_snippet: str\n    recommendation: Optional[str] = None\n    cwe_id: Optional[str] = None\n\n    # DAST validation fields\n    validation_status: Optional[ValidationStatus] = None\n    dast_evidence: Optional[dict] = None\n    exploitability_score: Optional[float] = None\n    validated_at: Optional[str] = None\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary\"\"\"\n        base_dict = {\n            \"id\": self.id,\n            \"severity\": self.severity.value,\n            \"title\": self.title,\n            \"description\": self.description,\n            \"file_path\": self.file_path,\n            \"line_number\": self.line_number,\n            \"code_snippet\": self.code_snippet,\n            \"recommendation\": self.recommendation,\n            \"cwe_id\": self.cwe_id,\n        }\n\n        # Include DAST fields if present\n        if self.validation_status:\n            base_dict.update(\n                {\n                    \"validation_status\": self.validation_status.value,\n                    \"dast_evidence\": self.dast_evidence,\n                    \"exploitability_score\": self.exploitability_score,\n                    \"validated_at\": self.validated_at,\n                }\n            )\n\n        return base_dict\n\n    @property\n    def is_validated(self) -> bool:\n        \"\"\"Check if issue was validated by DAST\"\"\"\n        return self.validation_status == ValidationStatus.VALIDATED\n\n    @property\n    def is_false_positive(self) -> bool:\n        \"\"\"Check if issue was disproven by DAST\"\"\"\n        return self.validation_status == ValidationStatus.FALSE_POSITIVE\n",
    "line_count": 94
  },
  {
    "id": "tgstate-python_app_events.py",
    "repo": "buyi06/tgstate-python",
    "url": "https://github.com/buyi06/tgstate-python/blob/main/app/events.py",
    "code": "import asyncio\n\nclass BroadcastEventBus:\n    def __init__(self, queue_maxsize: int = 200):\n        self._queue_maxsize = queue_maxsize\n        self._subscribers: set[asyncio.Queue[str]] = set()\n        self._lock = asyncio.Lock()\n\n    async def subscribe(self) -> asyncio.Queue[str]:\n        q: asyncio.Queue[str] = asyncio.Queue(maxsize=self._queue_maxsize)\n        async with self._lock:\n            self._subscribers.add(q)\n        return q\n\n    async def unsubscribe(self, q: asyncio.Queue[str]) -> None:\n        async with self._lock:\n            self._subscribers.discard(q)\n\n    async def publish(self, data: str) -> None:\n        async with self._lock:\n            subscribers = list(self._subscribers)\n\n        for q in subscribers:\n            try:\n                q.put_nowait(data)\n            except asyncio.QueueFull:\n                try:\n                    q.get_nowait()\n                except asyncio.QueueEmpty:\n                    pass\n                try:\n                    q.put_nowait(data)\n                except asyncio.QueueFull:\n                    pass\n\n    async def put(self, data: str) -> None:\n        await self.publish(data)\n\n\nfile_update_queue = BroadcastEventBus()\n\n\ndef build_file_event(\n    *,\n    action: str,\n    file_id: str,\n    filename: str | None = None,\n    filesize: int | None = None,\n    upload_date: str | None = None,\n    short_id: str | None = None,\n) -> dict:\n    return {\n        \"action\": action,\n        \"file_id\": file_id,\n        \"filename\": filename,\n        \"filesize\": filesize,\n        \"upload_date\": upload_date,\n        \"short_id\": short_id,\n    }\n",
    "line_count": 59
  },
  {
    "id": "ODesign_src_utils_train_metrics.py",
    "repo": "The-Institute-for-AI-Molecular-Design/ODesign",
    "url": "https://github.com/The-Institute-for-AI-Molecular-Design/ODesign/blob/main/src/utils/train/metrics.py",
    "code": "import numpy as np\nimport torch\n\nfrom src.utils.license_register import register_license\nfrom src.utils.train.distributed import gather_and_merge\n\ncommon_aggregator = {\n    \"avg\": lambda x: np.mean(x),\n    \"median\": lambda x: np.median(x),\n    \"pct90\": lambda x: np.percentile(x, 90),\n    \"pct99\": lambda x: np.percentile(x, 99),\n    \"max\": lambda x: np.max(x),\n    \"min\": lambda x: np.min(x),\n}\n\n\nclass SimpleMetricAggregator(object):\n    \"\"\"A quite simple metrics calculator that only do simple metrics aggregation.\"\"\"\n\n    def __init__(\n        self, aggregator_names=None, gather_before_calc=True, need_gather=True\n    ):\n        super(SimpleMetricAggregator, self).__init__()\n        self.gather_before_calc = gather_before_calc\n        self.need_gather = need_gather\n        self._metric_data = {}\n\n        self.aggregators = {name: common_aggregator[name] for name in aggregator_names}\n\n    @register_license('bytedance2024')\n    def add(self, key, value, namespace=\"default\"):\n        value_dict = self._metric_data.setdefault(namespace, {})\n        value_dict.setdefault(key, [])\n        if isinstance(value, (float, int)):\n            value = np.array([value])\n        elif isinstance(value, torch.Tensor):\n            if value.dim() == 0:\n                value = np.array([value.item()])\n            else:\n                value = value.detach().cpu().numpy()\n        elif isinstance(value, np.ndarray):\n            pass\n        else:\n            raise ValueError(f\"Unsupported type for metric data: {type(value)}\")\n        value_dict[key].append(value)\n\n    @register_license('odesign2025')\n    def calc(self):\n        metric_data, self._metric_data = self._metric_data, {}\n        if self.need_gather and self.gather_before_calc:\n            metric_data = gather_and_merge(\n                metric_data, aggregation_func=lambda l: sum(l, [])\n            )\n        results = {}\n        for agg_name, agg_func in self.aggregators.items():\n            for namespace, value_dict in metric_data.items():\n                for key, data in value_dict.items():\n                    plain_key = f\"{namespace}/{key}\" if namespace != \"default\" else key\n                    plain_key = f\"{plain_key}.{agg_name}\"\n                    results[plain_key] = agg_func(np.concatenate(data, axis=0))\n        if self.need_gather and not self.gather_before_calc:  # need gather after calc\n            results = gather_and_merge(results, aggregation_func=np.mean)\n        return results\n",
    "line_count": 63
  },
  {
    "id": "atomworks_src_atomworks_ml_executables_x3dna.py",
    "repo": "RosettaCommons/atomworks",
    "url": "https://github.com/RosettaCommons/atomworks/blob/production/src/atomworks/ml/executables/x3dna.py",
    "code": "import logging\nimport os\nfrom os import PathLike\n\nfrom atomworks.ml.executables import Executable, ExecutableError\n\nlogger = logging.getLogger(__name__)\n\n\nclass X3DNAFiber(Executable):\n    \"\"\"Executable wrapper for the x3dna-fiber program from the 3DNA package.\n\n    This class manages the x3dna-fiber executable, which is used to generate B-form conformation\n    DNA fibers (i.e. linear duplexes).\n\n\n    Example:\n        ```python\n        fiber = X3DNAFiber.get_or_initialize(\"/path/to/x3dna-v2.4/bin/fiber\")\n        version = fiber.get_version()\n        bin_path = fiber.get_bin_path()\n        ```\n    \"\"\"\n\n    name = \"x3dna-fiber\"\n    required_verification_text = (\"fiber\", \"3DNA\", \"SYNOPSIS\", \"DESCRIPTION\")\n\n    @classmethod\n    def initialize(cls, bin_path: PathLike | None = None, *args, **kwargs) -> \"X3DNAFiber\":\n        if bin_path is None:\n            bin_path = cls._infer_bin_path_from_env_var()\n        return super().initialize(bin_path, *args, **kwargs)\n\n    @staticmethod\n    def _infer_bin_path_from_env_var() -> PathLike:\n        x3dna_path = os.environ.get(\"X3DNA\")\n        if x3dna_path is not None:\n            bin_path = os.path.join(x3dna_path, \"bin\", \"fiber\")\n            return bin_path\n        raise ExecutableError(\n            \"No `bin_path` provided and `X3DNA` environment variable not set.\\n\"\n            \"Please set the `X3DNA` environment variable to the root directory of the X3DNA installation \"\n            \"or provide a `bin_path` to the `X3DNAFiber` constructor: \"\n            \"`X3DNAFiber(bin_path='/path/to/fiber')`.\"\n        )\n\n    @classmethod\n    def _setup(cls, bin_path: PathLike) -> None:\n        \"\"\"Sets up the X3DNA environment by setting the X3DNA environment variable.\n\n        The X3DNA environment variable must point to the root directory of the X3DNA installation,\n        which is typically two levels up from the executable location.\n\n        Args:\n            - bin_path (PathLike): Path to the x3dna-fiber executable.\n        \"\"\"\n        # ... set the X3DNA environment variable\n        x3dna_path = os.path.dirname(os.path.dirname(bin_path))\n        logger.info(f\"Setting environment variable X3DNA={x3dna_path}\")\n        os.environ[\"X3DNA\"] = x3dna_path\n",
    "line_count": 60
  },
  {
    "id": "scope_src_scope_server_download_progress_manager.py",
    "repo": "daydreamlive/scope",
    "url": "https://github.com/daydreamlive/scope/blob/main/src/scope/server/download_progress_manager.py",
    "code": "\"\"\"\nDownload progress tracking for pipeline model downloads.\n\"\"\"\n\nimport threading\n\n\nclass DownloadProgressManager:\n    \"\"\"Simple progress tracker for pipeline downloads.\"\"\"\n\n    def __init__(self):\n        self._progress = {}\n        self._lock = threading.Lock()\n\n    def update(\n        self, pipeline_id: str, artifact: str, downloaded_mb: float, total_mb: float\n    ):\n        \"\"\"Update download progress.\"\"\"\n        with self._lock:\n            if pipeline_id not in self._progress:\n                self._progress[pipeline_id] = {\"artifacts\": {}, \"is_downloading\": True}\n            self._progress[pipeline_id][\"artifacts\"][artifact] = {\n                \"downloaded_mb\": downloaded_mb,\n                \"total_mb\": total_mb,\n            }\n\n    def get_progress(self, pipeline_id: str):\n        \"\"\"Get current artifact progress.\"\"\"\n        with self._lock:\n            if pipeline_id not in self._progress:\n                return None\n            data = self._progress[pipeline_id]\n            if not data[\"artifacts\"]:\n                return None\n\n            # The current artifact is the last one in the dict\n            *_, (current_artifact, current_data) = data[\"artifacts\"].items()\n\n            # Calculate percentage for current artifact\n            current_percentage = 0\n            if current_data[\"total_mb\"] > 0:\n                current_percentage = (\n                    current_data[\"downloaded_mb\"] / current_data[\"total_mb\"] * 100\n                )\n\n            return {\n                \"is_downloading\": data[\"is_downloading\"],\n                \"percentage\": round(current_percentage, 1),\n                \"current_artifact\": current_artifact,\n            }\n\n    def mark_complete(self, pipeline_id: str):\n        \"\"\"Mark download as complete.\"\"\"\n        with self._lock:\n            if pipeline_id in self._progress:\n                self._progress[pipeline_id][\"is_downloading\"] = False\n\n    def clear_progress(self, pipeline_id: str):\n        \"\"\"Clear progress data.\"\"\"\n        with self._lock:\n            self._progress.pop(pipeline_id, None)\n\n\n# Global singleton instance\ndownload_progress_manager = DownloadProgressManager()\n",
    "line_count": 65
  },
  {
    "id": "Vulnhalla_src_ui_components_splitter_divider.py",
    "repo": "cyberark/Vulnhalla",
    "url": "https://github.com/cyberark/Vulnhalla/blob/main/src/ui/components/splitter_divider.py",
    "code": "#!/usr/bin/env python3\n\"\"\"\nSplitter divider component for Vulnhalla UI.\n\"\"\"\n\nfrom textual.widget import Widget\n\n\nclass SplitterDivider(Widget):\n    \"\"\"\n    Draggable divider widget between panels for resizing.\n    \"\"\"\n    \n    DEFAULT_CSS = \"\"\"\n    SplitterDivider {\n        width: 1;\n        background: transparent;\n        color: $surface-lighten-2;\n    }\n    SplitterDivider:hover {\n        background: $primary;\n        color: $primary;\n    }\n    \"\"\"\n    \n    def __init__(self, app_instance=None):\n        \"\"\"\n        Initialize the SplitterDivider.\n\n        Args:\n            app_instance: Reference to the VulnhallaUI app instance for updating split position.\n        \"\"\"\n        super().__init__()\n        self.app_instance = app_instance\n        self.dragging = False\n\n\n    def render(self):\n        \"\"\"\n        Render the divider as a thin vertical line.\n\n        Returns:\n            str: Single vertical line character \"â”‚\".\n        \"\"\"\n        return \"â”‚\"\n\n\n    def on_mouse_down(self, event) -> None:\n        \"\"\"\n        Start dragging when mouse is pressed.\n\n        Args:\n            event: Mouse down event.\n        \"\"\"\n        self.dragging = True\n        self.capture_mouse()\n\n\n    def on_mouse_move(self, event) -> None:\n        \"\"\"\n        Update split position while dragging.\n\n        Args:\n            event: Mouse move event containing position information.\n        \"\"\"\n        if self.dragging and self.app_instance:\n            parent = self.parent\n            if parent and parent.region:\n                try:\n                    # Get mouse position relative to parent container\n                    mouse_x = event.screen_x - parent.region.x\n                    parent_width = parent.size.width\n                    if parent_width > 0:\n                        new_position = max(0.2, min(0.8, mouse_x / parent_width))\n                        self.app_instance.split_position = new_position\n                        self.app_instance._update_split_position()\n                except (AttributeError, TypeError):\n                    # Fallback: use delta if available\n                    if hasattr(event, 'delta_x') and event.delta_x != 0:\n                        parent_width = parent.size.width\n                        if parent_width > 0:\n                            delta = event.delta_x / parent_width\n                            new_position = max(0.2, min(0.8, self.app_instance.split_position + delta))\n                            self.app_instance.split_position = new_position\n                            self.app_instance._update_split_position()\n\n\n    def on_mouse_up(self, event) -> None:\n        \"\"\"\n        Stop dragging when mouse is released.\n\n        Args:\n            event: Mouse up event.\n        \"\"\"\n        if self.dragging:\n            self.dragging = False\n            self.release_mouse()\n\n",
    "line_count": 98
  },
  {
    "id": "FinSight_src_tools_base.py",
    "repo": "RUC-NLPIR/FinSight",
    "url": "https://github.com/RUC-NLPIR/FinSight/blob/main/src/tools/base.py",
    "code": "import pandas as pd\nimport uuid\n\nclass Tool:\n    def __init__(\n        self,\n        name: str,\n        description: str,\n        parameters: list[dict]\n    ):\n        self.name = name\n        self.type = f'tool_{name}'\n        self.id = f\"tool_{name}_{uuid.uuid4().hex[:8]}\"\n        self.short_description = description\n        self.parameters = parameters\n\n    def prepare_params(self, task) -> dict:\n        \"\"\"\n        Optional hook to derive API parameters from a task payload.\n        \"\"\"\n        return {}\n    \n    @property\n    def description(self):\n        params_str = \", \".join([\n            f\"{p['name']}: {p['type']} ({p['description']})\"\n                for p in self.parameters\n        ])\n        return f\"Tool name: {self.name}\\nDescription: {self.short_description}\\nParameters: {params_str}\\n\"\n\n    async def api_function(self, **kwargs):\n        \"\"\"\n        Execute the underlying API and return structured data.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_data(self, task):\n        params = self.prepare_params(task)\n        try:\n            data = await self.api_function(**params)\n            task.all_results.extend(data)\n            return data\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return []\n\n\nclass ToolResult:\n    def __init__(self, name, description, data, source = \"\"):\n        self.name = name\n        self.description = description\n        if isinstance(data, list) and len(data) == 1:\n            data = data[0]\n        self.data = data\n        self.data_type = type(data)\n        self.source = source  # str, data source\n\n    def brief_str(self):\n        return self.__str__()\n\n    def get_full_string(self):\n        if isinstance(self.data, pd.DataFrame):\n            return self.data.to_string()\n        else:\n            return str(self.data)\n\n    def __str__(self):\n        base_string = f\"Data name: {self.name}\\nDescription: {self.description}\\nSource: {self.source}\\n\"\n        base_string += f\"Data type: {type(self.data)}\\n\"\n        if isinstance(self.data, pd.DataFrame):\n            format_string = \"\"\n            format_string += f\"First five rows:\\n{self.data.head().to_string()}\\n\"\n        elif isinstance(self.data, dict):\n            format_string = \"Partial data preview: \"\n            format_string += str(self.data)[:100]\n        elif isinstance(self.data, list):\n            format_string = \"Partial data preview: \"\n            format_string += str(self.data)[:100]\n        else:\n            format_string = \"Partial data preview: \"\n            format_string += str(self.data)[:100]\n\n        return base_string + format_string\n\n    def __repr__(self):\n        return self.__str__()\n    \n    def __hash__(self):\n        return hash(self.name+self.description)\n    \n    def __eq__(self, other):\n        return self.name == other.name and self.description == other.description",
    "line_count": 92
  },
  {
    "id": "webctl_src_webctl_protocol_client.py",
    "repo": "cosinusalpha/webctl",
    "url": "https://github.com/cosinusalpha/webctl/blob/main/src/webctl/protocol/client.py",
    "code": "\"\"\"\nIPC client for CLI-to-daemon communication.\n\"\"\"\n\nimport json\nfrom collections.abc import AsyncIterator\nfrom types import TracebackType\nfrom typing import Any\n\nfrom .messages import (\n    DoneResponse,\n    ErrorResponse,\n    EventResponse,\n    ItemResponse,\n    Request,\n    Response,\n)\nfrom .transport import TransportType, get_client_transport\n\n\nclass DaemonClient:\n    \"\"\"Client for communicating with the webctl daemon.\"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        transport_type: TransportType | None = None,\n        tcp_port: int | None = None,\n    ):\n        self.session_id = session_id\n        self.transport = get_client_transport(session_id, transport_type, tcp_port)\n\n    async def connect(self) -> None:\n        \"\"\"Connect to the daemon.\"\"\"\n        await self.transport.connect()\n\n    async def send_command(\n        self, command: str, args: dict[str, Any] | None = None\n    ) -> AsyncIterator[Response]:\n        \"\"\"Send command and stream responses.\"\"\"\n        request = Request(command=command, args=args or {})\n        await self.transport.send_line(request.model_dump_json())\n\n        while True:\n            line = await self.transport.recv_line()\n            if not line:\n                break\n\n            data = json.loads(line)\n\n            if data[\"type\"] == \"done\":\n                yield DoneResponse(**data)\n                break\n            elif data[\"type\"] == \"item\":\n                yield ItemResponse(**data)\n            elif data[\"type\"] == \"event\":\n                yield EventResponse(**data)\n            elif data[\"type\"] == \"error\":\n                yield ErrorResponse(**data)\n                break\n\n    async def close(self) -> None:\n        \"\"\"Close the connection.\"\"\"\n        await self.transport.close()\n\n    async def __aenter__(self) -> \"DaemonClient\":\n        await self.connect()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        await self.close()\n",
    "line_count": 76
  },
  {
    "id": "Ally_app_utils_logger.py",
    "repo": "YassWorks/Ally",
    "url": "https://github.com/YassWorks/Ally/blob/main/app/utils/logger.py",
    "code": "import logging\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom app.utils.constants import DEFAULT_PATHS\n\n\nclass AllyLogger:\n    \"\"\"Centralized logging system for Ally.\"\"\"\n\n    _instance = None\n    _logger = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(AllyLogger, cls).__new__(cls)\n            cls._instance._initialize_logger()\n        return cls._instance\n\n    def _initialize_logger(self):\n        \"\"\"Initialize the logger with file and console handlers.\"\"\"\n        # Expand environment variables and user home directory\n        log_dir = os.path.expandvars(DEFAULT_PATHS[\"logs\"])\n        log_dir = os.path.expanduser(log_dir)\n\n        # Create log directory if it doesn't exist\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\n\n        # Create log filename with timestamp\n        log_filename = f\"ally_{datetime.now().strftime('%Y%m%d')}.log\"\n        log_path = os.path.join(log_dir, log_filename)\n\n        # Create logger\n        self._logger = logging.getLogger(\"ally\")\n        self._logger.setLevel(logging.DEBUG)\n        # Prevent logging from propagating to root logger (which outputs to console)\n        self._logger.propagate = False\n\n        # Avoid duplicate handlers if logger already configured\n        if not self._logger.handlers:\n            # File handler - logs everything\n            file_handler = logging.FileHandler(log_path, encoding=\"utf-8\")\n            file_handler.setLevel(logging.DEBUG)\n            file_formatter = logging.Formatter(\n                \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n                datefmt=\"%Y-%m-%d %H:%M:%S\",\n            )\n            file_handler.setFormatter(file_formatter)\n\n            self._logger.addHandler(file_handler)\n\n    def debug(self, message: str, **kwargs):\n        \"\"\"Log debug message.\"\"\"\n        self._logger.debug(message, extra=kwargs)\n\n    def info(self, message: str, **kwargs):\n        \"\"\"Log info message.\"\"\"\n        self._logger.info(message, extra=kwargs)\n\n    def warning(self, message: str, **kwargs):\n        \"\"\"Log warning message.\"\"\"\n        self._logger.warning(message, extra=kwargs)\n\n    def error(self, message: str, exc_info=None, **kwargs):\n        \"\"\"Log error message with optional exception info.\"\"\"\n        self._logger.error(message, exc_info=exc_info, extra=kwargs)\n\n    def critical(self, message: str, exc_info=None, **kwargs):\n        \"\"\"Log critical message with optional exception info.\"\"\"\n        self._logger.critical(message, exc_info=exc_info, extra=kwargs)\n\n    def exception(self, message: str, **kwargs):\n        \"\"\"Log exception with traceback.\"\"\"\n        self._logger.exception(message, extra=kwargs)\n\n\n# Create singleton instance\nlogger = AllyLogger()\n",
    "line_count": 78
  },
  {
    "id": "Puffin_src_models_radiov3_cls_token.py",
    "repo": "KangLiao929/Puffin",
    "url": "https://github.com/KangLiao929/Puffin/blob/main/src/models/radiov3/cls_token.py",
    "code": "# Copyright (c) 2023-2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\n\nclass ClsToken(nn.Module):\n    def __init__(self, ndim: int,\n                 num_tokens: int = 1,\n                 enabled: bool = True,\n                 register_multiple: Optional[int] = None,\n                 num_registers: Optional[int] = None,\n    ):\n        super().__init__()\n\n        self.ndim = ndim\n        self.enabled = enabled\n        self.num_registers = 0\n        self.num_tokens = num_tokens\n        if enabled:\n            if num_registers:\n                self.num_registers = num_registers\n            elif register_multiple:\n                self.num_registers = register_multiple - (num_tokens % register_multiple)\n\n            scale = ndim ** -0.5\n            self.token = nn.Parameter(torch.randn(num_tokens + self.num_registers, ndim) * scale)\n        else:\n            self.token = None\n\n        self.num_patches = self.num_tokens + self.num_registers\n\n    def disable(self):\n        self.token = None\n        self.enabled = False\n\n    def forward(self, x: torch.Tensor):\n        if self.token is None:\n            return x\n\n        token = self.token.unsqueeze(0).expand(x.shape[0], -1, -1)\n        x = torch.cat([\n            token,\n            x,\n        ], dim=1)\n\n        return x\n\n    def no_weight_decay(self):\n        return [\n            'token',\n        ]\n",
    "line_count": 59
  },
  {
    "id": "MinivLLM_src_myvllm_layers_layernorm.py",
    "repo": "Wenyueh/MinivLLM",
    "url": "https://github.com/Wenyueh/MinivLLM/blob/main/src/myvllm/layers/layernorm.py",
    "code": "import torch\nimport time \n\nclass LayerNorm(torch.nn.Module):\n    def __init__(self, gamma: torch.Tensor, eps: float = 1e-5):\n        super().__init__()\n        self.register_buffer('gamma', gamma)\n        self.eps = eps\n\n    @torch.compile\n    def rms_forward(self, x: torch.Tensor) -> torch.Tensor:\n        # RMSNorm(x) = (x / sqrt(mean(xÂ²) + Îµ)) âŠ™ Î³\n\n        variance = x.pow(2).mean(dim=-1, keepdim=True) + self.eps\n        sqrt_variance = variance.sqrt()\n        x_norm = (x / sqrt_variance * self.gamma)\n\n        return x_norm\n\n    def residual_rms_forward(self, x: torch.Tensor, residual: torch.Tensor) -> torch.Tensor:\n        x = x + residual\n        return self.rms_forward(x), x\n\n    def forward(self, x: torch.Tensor, residual: torch.Tensor | None = None) -> torch.Tensor:\n        if residual is not None:\n            return self.residual_rms_forward(x, residual)\n        else:\n            return self.rms_forward(x)\n\nif __name__ == \"__main__\":\n    # Example usage\n    x = torch.randn(8,4000,8000).cuda()\n    gamma = torch.full((8000,), 0.5, device=\"cuda\", dtype=x.dtype)\n    layer = LayerNorm(gamma=gamma).cuda()\n    residual = torch.full_like(x,fill_value=1)\n\n    for _ in range(10): # Warm-up iterations\n        _ = layer(x)\n    \n    # Without residuals\n    times = [] \n    for _ in range(100): # Timing iterations\n        torch.cuda.synchronize()\n        start_time = time.time()\n        _ = layer(x)\n        torch.cuda.synchronize()\n        end_time = time.time()\n        times.append(end_time - start_time)\n    avg_time = sum(times) / len(times)\n    print(f\"[Without residuals] Average inference time over 100 runs: {avg_time * 1000:.4f} ms\")\n\n    # With residuals\n    times.clear()\n    for _ in range(100): # Timing iterations\n        torch.cuda.synchronize()\n        start_time = time.time()\n        _ = layer(x,residual)\n        torch.cuda.synchronize()\n        end_time = time.time()\n        times.append(end_time - start_time)\n    avg_time = sum(times) / len(times)\n    print(f\"[With residuals] Average inference time over 100 runs: {avg_time * 1000:.4f} ms\")\n    \n",
    "line_count": 63
  },
  {
    "id": "nano-trm_src_nn_data_dummy_datamodule.py",
    "repo": "olivkoch/nano-trm",
    "url": "https://github.com/olivkoch/nano-trm/blob/main/src/nn/data/dummy_datamodule.py",
    "code": "# src/nn/data/dummy_datamodule.py\nfrom lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass DummyDataset(Dataset):\n    \"\"\"Dummy dataset for self-play training.\"\"\"\n    def __init__(self, length=100):\n        self.length = length\n    \n    def __len__(self):\n        return self.length\n    \n    def __getitem__(self, idx):\n        return {}  # Return empty dict\n\n\nclass DummyDataModule(LightningDataModule):\n    \"\"\"Dummy datamodule for pure self-play training.\"\"\"\n    \n    def __init__(\n        self,\n        batch_size: int = 32,\n        num_workers: int = 0,\n    ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        \n        # Properties needed by model\n        self.num_puzzles = 1  # Single game type\n        self.pad_value = 0\n        self.max_grid_size = 7\n        self.vocab_size = 25  # Enough for board + move + outcome tokens\n        self.seq_len = 42\n    \n    def setup(self, stage=None):\n        self.train_dataset = DummyDataset()\n        self.val_dataset = DummyDataset(length=100)\n        self.test_dataset = DummyDataset(length=100)\n    \n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=1,  # Batch size handled in model\n            num_workers=0,\n            shuffle=False\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=1,\n            num_workers=0\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=1,\n            num_workers=0\n        )",
    "line_count": 62
  },
  {
    "id": "sonobarr_src_sonobarr_app_models.py",
    "repo": "Dodelidoo-Labs/sonobarr",
    "url": "https://github.com/Dodelidoo-Labs/sonobarr/blob/main/src/sonobarr_app/models.py",
    "code": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom flask_login import UserMixin\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom .extensions import db\n\n\nclass User(UserMixin, db.Model):\n    __tablename__ = \"users\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False, index=True)\n    password_hash = db.Column(db.String(255), nullable=False)\n    display_name = db.Column(db.String(120), nullable=True)\n    avatar_url = db.Column(db.String(512), nullable=True)\n    lastfm_username = db.Column(db.String(120), nullable=True)\n    listenbrainz_username = db.Column(db.String(120), nullable=True)\n    is_admin = db.Column(db.Boolean, default=False, nullable=False)\n    is_active = db.Column(db.Boolean, default=True, nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(\n        db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False\n    )\n\n    def set_password(self, raw_password: str) -> None:\n        self.password_hash = generate_password_hash(raw_password)\n\n    def check_password(self, raw_password: str) -> bool:\n        if not self.password_hash:\n            return False\n        return check_password_hash(self.password_hash, raw_password)\n\n    @property\n    def name(self) -> str:\n        return self.display_name or self.username\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<User id={self.id} username={self.username!r} admin={self.is_admin}>\"\n\n\nclass ArtistRequest(db.Model):\n    __tablename__ = \"artist_requests\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    artist_name = db.Column(db.String(255), nullable=False, index=True)\n    requested_by_id = db.Column(db.Integer, db.ForeignKey(\"users.id\", ondelete=\"CASCADE\"), nullable=False)\n    status = db.Column(db.String(20), default=\"pending\", nullable=False)  # pending, approved, rejected\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(\n        db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False\n    )\n    approved_by_id = db.Column(db.Integer, db.ForeignKey(\"users.id\", ondelete=\"SET NULL\"), nullable=True)\n    approved_at = db.Column(db.DateTime, nullable=True)\n\n    # Relationships\n    requested_by = db.relationship(\"User\", foreign_keys=[requested_by_id], backref=\"requested_artists\")\n    approved_by = db.relationship(\"User\", foreign_keys=[approved_by_id], backref=\"approved_requests\")\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<ArtistRequest id={self.id} artist='{self.artist_name}' status={self.status}>\"\n",
    "line_count": 63
  },
  {
    "id": "Sidon_src_sidon_model_losses.py",
    "repo": "sarulab-speech/Sidon",
    "url": "https://github.com/sarulab-speech/Sidon/blob/main/src/sidon/model/losses.py",
    "code": "\"\"\"Reusable loss utilities for Sidon models.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nfrom audiotools import AudioSignal\nfrom dac.model.discriminator import Discriminator\nfrom dac.nn.loss import L1Loss, MelSpectrogramLoss, MultiScaleSTFTLoss\nfrom torch import nn\n\ntry:  # pragma: no cover - optional dependency typing\n    from omegaconf import DictConfig  # type: ignore\nexcept ModuleNotFoundError:  # pragma: no cover - controlled by dependencies\n    DictConfig = Dict[str, object]  # type: ignore\n\n\nclass DACLoss(nn.Module):\n    \"\"\"Aggregate losses used for training Descript Audio Codec decoders.\"\"\"\n\n    def __init__(self, cfg: DictConfig) -> None:\n        super().__init__()\n        self.stft_loss = MultiScaleSTFTLoss(**cfg.stft_loss)\n        self.mel_loss = MelSpectrogramLoss(**cfg.mel_loss)\n        self.wav_loss = L1Loss()\n\n    def forward(self, target: AudioSignal, predicted: AudioSignal) -> Dict[str, torch.Tensor]:\n        \"\"\"Compute STFT, mel, and waveform reconstruction losses.\"\"\"\n        stft_loss = self.stft_loss(predicted, target)\n        mel_loss = self.mel_loss(predicted, target)\n        wav_loss = self.wav_loss(predicted, target)\n        return {\n            \"stft_loss\": stft_loss,\n            \"mel_loss\": mel_loss,\n            \"wav_loss\": wav_loss,\n        }\n\n\nclass GANLoss(nn.Module):\n    \"\"\"Least-squares GAN losses for DAC discriminators.\"\"\"\n\n    def __init__(self, discriminator: Discriminator) -> None:\n        super().__init__()\n        self.discriminator = discriminator\n\n    def forward(self, fake: torch.Tensor, real: torch.Tensor):\n        d_fake = self.discriminator(fake)\n        d_real = self.discriminator(real)\n        return d_fake, d_real\n\n    def discriminator_loss(self, fake: torch.Tensor, real: torch.Tensor) -> torch.Tensor:\n        d_fake, d_real = self.forward(fake.clone().detach(), real)\n        loss_d = 0.0\n        for out_fake, out_real in zip(d_fake, d_real):\n            loss_d += torch.mean(out_fake[-1] ** 2)\n            loss_d += torch.mean((1 - out_real[-1]) ** 2)\n        return loss_d\n\n    def generator_loss(self, fake: torch.Tensor, real: torch.Tensor):\n        d_fake, d_real = self.forward(fake, real)\n        loss_g = 0.0\n        for out_fake in d_fake:\n            loss_g += torch.mean((1 - out_fake[-1]) ** 2)\n\n        feature_loss = 0.0\n        for idx in range(len(d_fake)):\n            for feature_idx in range(len(d_fake[idx]) - 1):\n                feature_loss += torch.nn.functional.l1_loss(\n                    d_fake[idx][feature_idx],\n                    d_real[idx][feature_idx].detach(),\n                )\n        return loss_g, feature_loss\n",
    "line_count": 73
  },
  {
    "id": "Romifleur_src_core_config_manager.py",
    "repo": "4Sitam4/Romifleur",
    "url": "https://github.com/4Sitam4/Romifleur/blob/main/src/core/config_manager.py",
    "code": "\nimport os\nimport json\nimport sys\n\nclass ConfigManager:\n    def __init__(self, settings_file=\"settings.json\", consoles_file=\"consoles.json\"):\n        self.settings_file = settings_file\n        self.consoles_file = consoles_file\n        self.settings = self.load_settings()\n        self.consoles = self.load_consoles()\n\n    def get_resource_path(self, relative_path):\n        \"\"\" Get absolute path to resource, works for dev and for PyInstaller \"\"\"\n        try:\n            # PyInstaller creates a temp folder and stores path in _MEIPASS\n            base_path = sys._MEIPASS\n        except Exception:\n            base_path = os.path.abspath(\".\")\n        return os.path.join(base_path, relative_path)\n\n    def load_settings(self):\n        \"\"\"Load user settings.\"\"\"\n        # Check standard data path first? For now keep local compatibility\n        path = os.path.join(os.getcwd(), \"data\", self.settings_file)\n        if not os.path.exists(path):\n            path = self.settings_file # Fallback to root\n\n        try:\n            if os.path.exists(path):\n                with open(path, \"r\") as f:\n                    return json.load(f)\n        except Exception as e:\n            print(f\"Error loading settings: {e}\")\n        \n        return {\"roms_path\": \"ROMs\", \"ra_api_key\": \"\"}\n\n    def save_settings(self):\n        \"\"\"Save user settings.\"\"\"\n        # Save to data/ if it exists, else root\n        data_dir = os.path.join(os.getcwd(), \"data\")\n        if os.path.exists(data_dir):\n            path = os.path.join(data_dir, self.settings_file)\n        else:\n            path = self.settings_file\n\n        try:\n            with open(path, \"w\") as f:\n                json.dump(self.settings, f, indent=4)\n        except Exception as e:\n            print(f\"Error saving settings: {e}\")\n\n    def load_consoles(self):\n        \"\"\"Load static console catalog.\"\"\"\n        try:\n            # Consoles file is a resource, not user data\n            # Check config/ folder or root or resource path\n            paths_to_try = [\n                self.get_resource_path(os.path.join(\"config\", self.consoles_file)),\n                self.get_resource_path(self.consoles_file),\n                \"consoles.json\"\n            ]\n            \n            for path in paths_to_try:\n                if os.path.exists(path):\n                    with open(path, 'r') as f:\n                        return json.load(f)\n            return {}\n        except Exception as e:\n            print(f\"Error loading consoles: {e}\")\n            return {}\n\n    def get_download_path(self):\n        path = self.settings.get(\"roms_path\", \"ROMs\")\n        if not os.path.isabs(path):\n            path = os.path.abspath(path)\n            \n        if not os.path.exists(path):\n            try:\n                os.makedirs(path, exist_ok=True)\n            except:\n                default = os.path.abspath(\"ROMs\")\n                os.makedirs(default, exist_ok=True)\n                return default\n        return path\n",
    "line_count": 85
  },
  {
    "id": "LLM-TradeBot_src_llm_claude_client.py",
    "repo": "EthanAlgoX/LLM-TradeBot",
    "url": "https://github.com/EthanAlgoX/LLM-TradeBot/blob/main/src/llm/claude_client.py",
    "code": "\"\"\"\nClaude å®¢æˆ·ç«¯å®žçŽ°\n================\n\nAnthropic Claude ä½¿ç”¨ä¸åŒçš„ API æ ¼å¼ï¼Œéœ€è¦å•ç‹¬å®žçŽ°ã€‚\n\"\"\"\n\nfrom typing import Dict, Any, List\nfrom .base import BaseLLMClient, LLMConfig, ChatMessage, LLMResponse\n\n\nclass ClaudeClient(BaseLLMClient):\n    \"\"\"\n    Claude å®¢æˆ·ç«¯ (Anthropic API)\n    \n    Claude ä½¿ç”¨ä¸åŒçš„ API æ ¼å¼ï¼š\n    - è®¤è¯ä½¿ç”¨ x-api-key è€Œéž Bearer token\n    - ç«¯ç‚¹æ˜¯ /messages è€Œéž /chat/completions\n    - system prompt æ˜¯ç‹¬ç«‹å­—æ®µ\n    \"\"\"\n    \n    DEFAULT_BASE_URL = \"https://api.anthropic.com/v1\"\n    DEFAULT_MODEL = \"claude-3-5-sonnet-20241022\"\n    PROVIDER = \"claude\"\n    \n    ANTHROPIC_VERSION = \"2023-06-01\"\n    \n    def _build_headers(self) -> Dict[str, str]:\n        \"\"\"æž„å»º Anthropic è®¤è¯å¤´\"\"\"\n        return {\n            \"x-api-key\": self.config.api_key,\n            \"anthropic-version\": self.ANTHROPIC_VERSION,\n            \"Content-Type\": \"application/json\"\n        }\n    \n    def _build_url(self) -> str:\n        \"\"\"Claude ä½¿ç”¨ /messages ç«¯ç‚¹\"\"\"\n        return f\"{self.base_url}/messages\"\n    \n    def _build_request_body(\n        self, \n        messages: List[ChatMessage],\n        **kwargs\n    ) -> Dict[str, Any]:\n        \"\"\"\n        æž„å»º Claude è¯·æ±‚ä½“\n        \n        Claude çš„ system prompt æ˜¯ç‹¬ç«‹å­—æ®µï¼Œä¸åœ¨ messages ä¸­\n        \"\"\"\n        # æå– system message\n        system_content = \"\"\n        user_messages = []\n        \n        for msg in messages:\n            if msg.role == \"system\":\n                system_content = msg.content\n            else:\n                user_messages.append({\"role\": msg.role, \"content\": msg.content})\n        \n        body = {\n            \"model\": self.model,\n            \"messages\": user_messages,\n            \"max_tokens\": kwargs.get(\"max_tokens\", self.config.max_tokens)\n        }\n        \n        if system_content:\n            body[\"system\"] = system_content\n        \n        # Claude ä¸æ”¯æŒ temperature=0ï¼Œæœ€å°å€¼æ˜¯ 0.1\n        temperature = kwargs.get(\"temperature\", self.config.temperature)\n        if temperature > 0:\n            body[\"temperature\"] = max(0.1, temperature)\n        \n        return body\n    \n    def _parse_response(self, response: Dict[str, Any]) -> LLMResponse:\n        \"\"\"è§£æž Claude å“åº”\"\"\"\n        content = \"\"\n        for block in response.get(\"content\", []):\n            if block.get(\"type\") == \"text\":\n                content = block.get(\"text\", \"\")\n                break\n        \n        return LLMResponse(\n            content=content,\n            model=response.get(\"model\", self.model),\n            provider=self.PROVIDER,\n            usage=response.get(\"usage\", {}),\n            raw_response=response\n        )\n",
    "line_count": 90
  },
  {
    "id": "BustAPI_python_bustapi_middleware.py",
    "repo": "GrandpaEJ/BustAPI",
    "url": "https://github.com/GrandpaEJ/BustAPI/blob/main/python/bustapi/middleware.py",
    "code": "from typing import Callable, List, Optional\n\nfrom .http.request import Request\nfrom .http.response import Response\n\n\nclass Middleware:\n    \"\"\"\n    Base class for BustAPI middleware.\n    \"\"\"\n\n    def process_request(self, request: Request) -> Optional[Response]:\n        \"\"\"\n        Called before the view function.\n\n        Args:\n            request: The incoming request object\n\n        Returns:\n            None to continue processing, or a Response object to stop invalid processing\n            and return the response immediately.\n        \"\"\"\n        return None\n\n    def process_response(self, request: Request, response: Response) -> Response:\n        \"\"\"\n        Called after the view function.\n\n        Args:\n            request: The request object\n            response: The response object produced by the view or previous middleware\n\n        Returns:\n            A Response object (modified or original).\n        \"\"\"\n        return response\n\n\nclass MiddlewareManager:\n    \"\"\"\n    Manages the execution of middleware chains.\n    \"\"\"\n\n    def __init__(self):\n        self._middlewares: List[Middleware] = []\n\n    @property\n    def middlewares(self) -> List[Middleware]:\n        return self._middlewares\n\n    def add(self, middleware: Middleware):\n        \"\"\"Add a middleware instance to the chain.\"\"\"\n        self._middlewares.append(middleware)\n\n    def process_request(self, request: Request) -> Optional[Response]:\n        \"\"\"Run process_request on all middleware.\"\"\"\n        for middleware in self._middlewares:\n            response = middleware.process_request(request)\n            if response:\n                return response\n        return None\n\n    def process_response(self, request: Request, response: Response) -> Response:\n        \"\"\"Run process_response on all middleware (reversed order).\"\"\"\n        for middleware in reversed(self._middlewares):\n            response = middleware.process_response(request, response)\n        return response\n",
    "line_count": 67
  },
  {
    "id": "claude-stt_src_claude_stt_engines_whisper.py",
    "repo": "jarrodwatts/claude-stt",
    "url": "https://github.com/jarrodwatts/claude-stt/blob/main/src/claude_stt/engines/whisper.py",
    "code": "\"\"\"Whisper STT engine using faster-whisper.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport os\nfrom typing import Optional\n\nimport numpy as np\n\n_whisper_available = False\n_WhisperModel = None\n\ntry:\n    from faster_whisper import WhisperModel as _WhisperModel\n\n    _whisper_available = True\nexcept ImportError:\n    pass\n\n\nclass WhisperEngine:\n    \"\"\"Whisper speech-to-text engine backed by faster-whisper.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"medium\",\n        device: Optional[str] = None,\n        compute_type: Optional[str] = None,\n    ):\n        self.model_name = model_name\n        self.device = device or os.environ.get(\"CLAUDE_STT_WHISPER_DEVICE\", \"cpu\")\n        self.compute_type = compute_type or os.environ.get(\n            \"CLAUDE_STT_WHISPER_COMPUTE_TYPE\",\n            \"int8\",\n        )\n        self._model: Optional[object] = None\n        self._logger = logging.getLogger(__name__)\n\n    def is_available(self) -> bool:\n        return _whisper_available\n\n    def load_model(self) -> bool:\n        if not self.is_available():\n            return False\n        if self._model is not None:\n            return True\n        try:\n            self._model = _WhisperModel(\n                self.model_name,\n                device=self.device,\n                compute_type=self.compute_type,\n            )\n            return True\n        except Exception:\n            self._logger.exception(\"Failed to load Whisper model\")\n            return False\n\n    def transcribe(self, audio: np.ndarray, sample_rate: int = 16000) -> str:\n        if not self.load_model():\n            return \"\"\n        try:\n            if audio.dtype != np.float32:\n                audio = audio.astype(np.float32)\n            segments, _info = self._model.transcribe(audio, sample_rate=sample_rate)\n            text = \" \".join(segment.text.strip() for segment in segments)\n            return text.strip()\n        except Exception:\n            self._logger.exception(\"Whisper transcription failed\")\n            return \"\"\n",
    "line_count": 70
  },
  {
    "id": "vibe-remote_modules_agent_router.py",
    "repo": "cyhhao/vibe-remote",
    "url": "https://github.com/cyhhao/vibe-remote/blob/master/modules/agent_router.py",
    "code": "import json\nimport logging\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass PlatformRoute:\n    default: str = \"claude\"\n    overrides: Dict[str, str] = field(default_factory=dict)\n\n\nclass AgentRouter:\n    \"\"\"Resolve which agent should serve a given message context.\"\"\"\n\n    def __init__(\n        self,\n        platform_routes: Dict[str, PlatformRoute],\n        global_default: str = \"claude\",\n    ):\n        self.platform_routes = platform_routes\n        self.global_default = global_default\n\n    @classmethod\n    def from_file(\n        cls, file_path: Optional[str], *, platform: str\n    ) -> \"AgentRouter\":\n        routes: Dict[str, PlatformRoute] = {}\n        global_default = \"claude\"\n\n        if file_path and os.path.exists(file_path):\n            try:\n                data = cls._load_file(file_path)\n                global_default = data.get(\"default\", global_default)\n                for key, value in data.items():\n                    if key == \"default\":\n                        continue\n                    if not isinstance(value, dict):\n                        continue\n                    routes[key] = PlatformRoute(\n                        default=value.get(\"default\", global_default),\n                        overrides=value.get(\"overrides\", {}) or {},\n                    )\n                logger.info(\n                    f\"Loaded agent routing config from {file_path} \"\n                    f\"(platforms: {list(routes.keys())})\"\n                )\n            except Exception as e:\n                logger.error(f\"Failed to parse agent route config {file_path}: {e}\")\n        else:\n            if file_path:\n                logger.warning(\n                    f\"Agent route config file not found at {file_path}, using defaults\"\n                )\n\n        # Ensure platform entry exists\n        routes.setdefault(platform, PlatformRoute(default=global_default))\n        return cls(routes, global_default=global_default)\n\n    @staticmethod\n    def _load_file(path: str) -> Dict:\n        _, ext = os.path.splitext(path)\n        if ext.lower() in {\".yaml\", \".yml\"}:\n            try:\n                import yaml  # type: ignore\n            except ImportError as exc:\n                raise RuntimeError(\n                    \"PyYAML is required to parse YAML agent route files. \"\n                    \"Install with `pip install pyyaml` or use JSON.\"\n                ) from exc\n            with open(path, \"r\") as f:\n                return yaml.safe_load(f) or {}\n        with open(path, \"r\") as f:\n            return json.load(f)\n\n    def resolve(self, platform: str, channel_id: str) -> str:\n        platform_route = self.platform_routes.get(platform)\n        if not platform_route:\n            return self.global_default\n        return platform_route.overrides.get(channel_id, platform_route.default)\n",
    "line_count": 83
  },
  {
    "id": "docify_backend_app_core_cache.py",
    "repo": "keshavashiya/docify",
    "url": "https://github.com/keshavashiya/docify/blob/main/backend/app/core/cache.py",
    "code": "\"\"\"\nRedis cache client for real-time updates and message streaming\n\"\"\"\nimport redis\nimport logging\nfrom app.core.config import settings\n\nlogger = logging.getLogger(__name__)\n\n_redis_client = None\n\n\ndef get_redis_client() -> redis.Redis:\n    \"\"\"Get or create Redis client instance\"\"\"\n    global _redis_client\n    \n    if _redis_client is None:\n        try:\n            _redis_client = redis.from_url(\n                settings.REDIS_URL,\n                decode_responses=True,\n                socket_connect_timeout=5,\n                socket_keepalive=True,\n                health_check_interval=30\n            )\n            # Test connection\n            _redis_client.ping()\n            logger.info(\"Redis client initialized successfully\")\n        except Exception as e:\n            logger.error(f\"Failed to connect to Redis: {e}\")\n            raise\n    \n    return _redis_client\n\n\ndef close_redis_client():\n    \"\"\"Close Redis connection\"\"\"\n    global _redis_client\n    if _redis_client is not None:\n        try:\n            _redis_client.close()\n            _redis_client = None\n        except Exception as e:\n            logger.error(f\"Error closing Redis connection: {e}\")\n\n\nclass MessageStreamCache:\n    \"\"\"Cache manager for message streaming\"\"\"\n    \n    def __init__(self):\n        self.redis = get_redis_client()\n        self.ttl = 3600  # 1 hour\n    \n    def set_status(self, message_id: str, status: str) -> None:\n        \"\"\"Set message status in cache\"\"\"\n        key = f\"msg:{message_id}:status\"\n        self.redis.setex(key, self.ttl, status)\n    \n    def get_status(self, message_id: str) -> str:\n        \"\"\"Get message status from cache\"\"\"\n        key = f\"msg:{message_id}:status\"\n        return self.redis.get(key) or \"pending\"\n    \n    def push_token(self, message_id: str, token: str) -> int:\n        \"\"\"Push token to message stream (returns stream length)\"\"\"\n        key = f\"msg:{message_id}:tokens\"\n        length = self.redis.rpush(key, token)\n        self.redis.expire(key, self.ttl)\n        return length\n    \n    def get_tokens(self, message_id: str, start: int = 0, end: int = -1) -> list:\n        \"\"\"Get tokens from message stream\"\"\"\n        key = f\"msg:{message_id}:tokens\"\n        return self.redis.lrange(key, start, end)\n    \n    def clear_stream(self, message_id: str) -> None:\n        \"\"\"Clear message stream\"\"\"\n        key = f\"msg:{message_id}:tokens\"\n        self.redis.delete(key)\n    \n    def publish_event(self, message_id: str, event_type: str, data: dict) -> int:\n        \"\"\"Publish event to subscribers\"\"\"\n        channel = f\"msg:{message_id}:events\"\n        import json\n        return self.redis.publish(channel, json.dumps({\"type\": event_type, **data}))\n",
    "line_count": 85
  },
  {
    "id": "OGhidra_src_session_store.py",
    "repo": "llnl/OGhidra",
    "url": "https://github.com/llnl/OGhidra/blob/main/src/session_store.py",
    "code": "\"\"\"\nSession history storage and retrieval.\n\"\"\"\n\nimport json\nimport os\nimport datetime\nfrom typing import List, Dict, Any, Optional\n\nfrom .memory_models import SessionRecord, ToolCallRecord\n\nclass SessionHistoryStore:\n    def __init__(self, storage_path: str = \"session_history.jsonl\"):\n        self.storage_path = storage_path\n        # Ensure the directory for the storage path exists\n        storage_dir = os.path.dirname(self.storage_path)\n        if storage_dir and not os.path.exists(storage_dir):\n            os.makedirs(storage_dir, exist_ok=True)\n\n    def save_session(self, record: SessionRecord):\n        \"\"\"Appends a session record to the storage file.\"\"\"\n        if record.outcome == \"in_progress\" and record.end_time is None:\n            record.end_time = datetime.datetime.utcnow()\n\n        with open(self.storage_path, \"a\", encoding=\"utf-8\") as f:\n            f.write(record.model_dump_json())\n            f.write(\"\\n\")\n\n    def load_all_sessions(self) -> List[SessionRecord]:\n        \"\"\"Loads all session records from the storage file.\"\"\"\n        if not os.path.exists(self.storage_path):\n            return []\n        \n        records = []\n        with open(self.storage_path, \"r\", encoding=\"utf-8\") as f:\n            for line_number, line in enumerate(f, 1):\n                if line.strip():\n                    try:\n                        data = json.loads(line)\n                        records.append(SessionRecord.model_validate(data))\n                    except json.JSONDecodeError as e:\n                        print(f\"Warning: Skipping malformed JSON line {line_number} in '{self.storage_path}': {e}\")\n                    except Exception as e:\n                        print(f\"Warning: Skipping record on line {line_number} due to data conversion error: {e}\")\n        return records\n    \n    def get_session_by_id(self, session_id: str) -> Optional[SessionRecord]:\n        \"\"\"Retrieves a specific session by its ID.\"\"\"\n        for session in self.load_all_sessions():\n            if session.session_id == session_id:\n                return session\n        return None ",
    "line_count": 52
  },
  {
    "id": "crypto-tax-calculator_src_logger.py",
    "repo": "ura-vf4/crypto-tax-calculator",
    "url": "https://github.com/ura-vf4/crypto-tax-calculator/blob/main/src/logger.py",
    "code": "import logging.handlers\n\nfrom .notifications import NotificationHandler\n\n\nclass Logger:\n    Logger = None\n    NotificationHandler = None \n\n    def __init__(self, logging_service=\"crypto_trading\", enable_notifications=True):\n        # Logger setup\n        self.Logger = logging.getLogger(f\"{logging_service}_logger\")\n        self.Logger.setLevel(logging.DEBUG)\n        self.Logger.propagate = False\n        formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n        # default is \"logs/crypto_trading.log\"\n        fh = logging.FileHandler(f\"logs/{logging_service}.log\")\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n        self.Logger.addHandler(fh)\n\n        # logging to console\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.INFO)\n        ch.setFormatter(formatter)\n        self.Logger.addHandler(ch)\n\n        # notification handler\n        self.NotificationHandler = NotificationHandler(enable_notifications)\n\n    def log(self, message, level=\"info\", notification=True):\n        if level == \"info\":\n            self.Logger.info(message)\n        elif level == \"warning\":\n            self.Logger.warning(message)\n        elif level == \"error\":\n            self.Logger.error(message)\n        elif level == \"debug\":\n            self.Logger.debug(message)\n\n        if notification and self.NotificationHandler.enabled:\n            self.NotificationHandler.send_notification(str(message))\n\n    def info(self, message, notification=True):\n        self.log(message, \"info\", notification)\n\n    def warning(self, message, notification=True):\n        self.log(message, \"warning\", notification)\n\n    def error(self, message, notification=True):\n        self.log(message, \"error\", notification)\n\n    def debug(self, message, notification=False):\n        self.log(message, \"debug\", notification)\n",
    "line_count": 54
  },
  {
    "id": "jdsh_src_jdsh_client.py",
    "repo": "Al00X/jdsh",
    "url": "https://github.com/Al00X/jdsh/blob/main/src/jdsh/client.py",
    "code": "import sys\nfrom myjdapi import Myjdapi\nfrom . import config\n\nclass JDClient:\n    def __init__(self):\n        self.api = Myjdapi()\n        self.api.set_app_key(config.APP_KEY)\n        self.device = None\n\n    def connect(self):\n        try:\n            if not self.api.direct_connect(config.HOST, config.PORT):\n                raise ConnectionError(f\"Failed to connect to {config.HOST}:{config.PORT}\")\n            self.device = self.api.get_device()\n            return self.device\n        except Exception as e:\n            print(f\"Connection Error: {e}\", file=sys.stderr)\n            sys.exit(1)\n\n    def fetch_stats(self):\n        try:\n            state = self.device.downloadcontroller.get_current_state()\n            \n            links = self.device.downloads.query_links([{\n                \"name\": True, \"bytesLoaded\": True, \"bytesTotal\": True, \n                \"speed\": True, \"running\": True, \"eta\": True, \"status\": True,\n                \"finished\": True, \"enabled\": True, \"uuid\": True\n            }])\n            \n            active = []\n            pending = []\n            \n            for l in links:\n                if l.get('finished'):\n                    continue\n                \n                # Active\n                if l.get('running'):\n                    active.append(l)\n                # Pending\n                elif l.get('enabled'):\n                    pending.append(l)\n                    \n            return state, active, pending\n        except Exception:\n            return \"ERROR\", [], []\n\n    def toggle_state(self, current_state):\n        if current_state in [\"RUNNING\", \"DOWNLOADING\"]:\n            self.device.downloadcontroller.stop_downloads()\n        else:\n            self.device.downloadcontroller.start_downloads()\n",
    "line_count": 53
  },
  {
    "id": "gfnx_src_gfnx_environment_amp.py",
    "repo": "d-tiapkin/gfnx",
    "url": "https://github.com/d-tiapkin/gfnx/blob/main/src/gfnx/environment/amp.py",
    "code": "from ..base import TRewardModule\nfrom ..utils import AMINO_ACIDS, PROTEINS_FULL_ALPHABET\nfrom .sequence import (\n    EnvParams,  # noqa: F401\n    EnvState,  # noqa: F401\n    AutoregressiveSequenceEnvironment,\n)\nimport jax.numpy as jnp\nimport chex\n\n\nclass AMPEnvironment(AutoregressiveSequenceEnvironment):\n    def __init__(self, reward_module: TRewardModule) -> None:\n        self.char_to_id = {char: i for i, char in enumerate(PROTEINS_FULL_ALPHABET)}\n\n        super().__init__(\n            reward_module,\n            max_length=60,\n            nchar=len(AMINO_ACIDS),\n            ntoken=len(PROTEINS_FULL_ALPHABET),\n            bos_token=self.char_to_id[\"[BOS]\"],\n            eos_token=self.char_to_id[\"[EOS]\"],\n            pad_token=self.char_to_id[\"[PAD]\"],\n        )\n\n    @property\n    def name(self) -> str:\n        \"\"\"Environment name.\"\"\"\n        return \"AMP-v0\"\n\n    def get_obs(self, state: EnvState, env_params: EnvParams) -> chex.Array:\n        \"\"\"Applies observation function to state.\"\"\"\n\n        # Use PAD if the last token is already PAD or EOS, otherwise use EOS\n        last_token = state.tokens[:, -1]\n        to_append = jnp.where(\n            jnp.logical_or(last_token == self.pad_token, last_token == self.eos_token),\n            self.pad_token,\n            self.eos_token\n        )\n        to_append = to_append[:, None]  # Add dimension to match concatenation\n\n        obs = jnp.concat(\n            [\n                state.tokens,\n                to_append,\n            ],\n            axis=-1,\n        )\n        return obs\n",
    "line_count": 50
  },
  {
    "id": "repo-swarm_src_investigator_activity_wrapper.py",
    "repo": "royosherove/repo-swarm",
    "url": "https://github.com/royosherove/repo-swarm/blob/main/src/investigator/activity_wrapper.py",
    "code": "\"\"\"\nActivityWrapper for executing Temporal activities without direct Temporal dependency.\n\"\"\"\n\nimport asyncio\nfrom typing import Optional, Any, Callable\nfrom datetime import timedelta\n\n\nclass ActivityWrapper:\n    \"\"\"\n    Wrapper class to execute Temporal activities without requiring direct Temporal imports.\n    This allows the investigator module to remain decoupled from Temporal while still\n    being able to execute activities when running within a Temporal workflow context.\n    \"\"\"\n    \n    def __init__(self, workflow_context: Optional[Any] = None):\n        \"\"\"\n        Initialize the ActivityWrapper.\n        \n        Args:\n            workflow_context: The Temporal workflow context (workflow module) if available\n        \"\"\"\n        self.workflow_context = workflow_context\n        self._is_temporal_context = workflow_context is not None\n    \n    async def execute_activity(self, activity_func: Callable, *args, \n                              start_to_close_timeout: Optional[timedelta] = None,\n                              retry_policy: Optional[Any] = None,\n                              **kwargs) -> Any:\n        \"\"\"\n        Execute an activity function.\n        \n        If running in a Temporal workflow context, this will execute the activity\n        via Temporal's workflow.execute_activity. Otherwise, it will execute the\n        function directly (for testing or non-Temporal environments).\n        \n        Args:\n            activity_func: The activity function to execute\n            *args: Positional arguments for the activity function\n            start_to_close_timeout: Timeout for the activity execution\n            retry_policy: Retry policy for the activity\n            **kwargs: Keyword arguments for the activity function\n            \n        Returns:\n            Result from the activity execution\n        \"\"\"\n        if self._is_temporal_context and hasattr(self.workflow_context, 'execute_activity'):\n            # Running in Temporal workflow context\n            return await self.workflow_context.execute_activity(\n                activity_func,\n                *args,\n                start_to_close_timeout=start_to_close_timeout or timedelta(minutes=10),\n                retry_policy=retry_policy,\n                **kwargs\n            )\n        else:\n            # Running outside Temporal context (testing or direct execution)\n            # Execute the activity function directly\n            if asyncio.iscoroutinefunction(activity_func):\n                return await activity_func(*args, **kwargs)\n            else:\n                return activity_func(*args, **kwargs)\n    \n    def is_temporal_context(self) -> bool:\n        \"\"\"\n        Check if running in a Temporal workflow context.\n        \n        Returns:\n            True if running in Temporal workflow context, False otherwise\n        \"\"\"\n        return self._is_temporal_context\n",
    "line_count": 72
  },
  {
    "id": "codex-autorunner_src_codex_autorunner_agents_base.py",
    "repo": "Git-on-my-level/codex-autorunner",
    "url": "https://github.com/Git-on-my-level/codex-autorunner/blob/main/src/codex_autorunner/agents/base.py",
    "code": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Any, AsyncIterator, Optional, Protocol\n\nfrom .types import ConversationRef, ModelCatalog, TurnRef\n\n\nclass AgentHarness(Protocol):\n    agent_id: str\n    display_name: str\n\n    async def ensure_ready(self, workspace_root: Path) -> None: ...\n\n    async def model_catalog(self, workspace_root: Path) -> ModelCatalog: ...\n\n    async def new_conversation(\n        self, workspace_root: Path, title: Optional[str] = None\n    ) -> ConversationRef: ...\n\n    async def list_conversations(\n        self, workspace_root: Path\n    ) -> list[ConversationRef]: ...\n\n    async def resume_conversation(\n        self, workspace_root: Path, conversation_id: str\n    ) -> ConversationRef: ...\n\n    async def start_turn(\n        self,\n        workspace_root: Path,\n        conversation_id: str,\n        prompt: str,\n        model: Optional[str],\n        reasoning: Optional[str],\n        *,\n        approval_mode: Optional[str],\n        sandbox_policy: Optional[Any],\n    ) -> TurnRef: ...\n\n    async def start_review(\n        self,\n        workspace_root: Path,\n        conversation_id: str,\n        prompt: str,\n        model: Optional[str],\n        reasoning: Optional[str],\n        *,\n        approval_mode: Optional[str],\n        sandbox_policy: Optional[Any],\n    ) -> TurnRef: ...\n\n    async def interrupt(\n        self, workspace_root: Path, conversation_id: str, turn_id: Optional[str]\n    ) -> None: ...\n\n    def stream_events(\n        self, workspace_root: Path, conversation_id: str, turn_id: str\n    ) -> AsyncIterator[str]: ...\n\n\n__all__ = [\"AgentHarness\"]\n",
    "line_count": 62
  },
  {
    "id": "NOVIX_backend_app_llm_gateway_providers_anthropic_provider.py",
    "repo": "unitagain/NOVIX",
    "url": "https://github.com/unitagain/NOVIX/blob/main/backend/app/llm_gateway/providers/anthropic_provider.py",
    "code": "\"\"\"\nAnthropic (Claude) Provider / Anthropic (Claude) é€‚é…å™¨\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom anthropic import AsyncAnthropic\nfrom app.llm_gateway.providers.base import BaseLLMProvider\n\n\nclass AnthropicProvider(BaseLLMProvider):\n    \"\"\"Anthropic API provider / Anthropic API æä¾›å•†\"\"\"\n    \n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"claude-3-5-sonnet-20241022\",\n        max_tokens: int = 8000,\n        temperature: float = 0.7\n    ):\n        super().__init__(api_key, model, max_tokens, temperature)\n        self.client = AsyncAnthropic(api_key=api_key)\n    \n    async def chat(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Send chat request to Anthropic\n        å‘é€èŠå¤©è¯·æ±‚åˆ° Anthropic\n        \n        Args:\n            messages: List of messages / æ¶ˆæ¯åˆ—è¡¨\n            temperature: Override temperature / è¦†ç›–æ¸©åº¦\n            max_tokens: Override max tokens / è¦†ç›–æœ€å¤§tokenæ•°\n            \n        Returns:\n            Response dict / å“åº”å­—å…¸\n        \"\"\"\n        # Extract system message if present / æå–ç³»ç»Ÿæ¶ˆæ¯\n        system_message = None\n        filtered_messages = []\n        \n        for msg in messages:\n            if msg[\"role\"] == \"system\":\n                system_message = msg[\"content\"]\n            else:\n                filtered_messages.append(msg)\n        \n        # Anthropic API call / Anthropic API è°ƒç”¨\n        kwargs = {\n            \"model\": self.model,\n            \"messages\": filtered_messages,\n            \"temperature\": temperature or self.temperature,\n            \"max_tokens\": max_tokens or self.max_tokens\n        }\n        \n        if system_message:\n            kwargs[\"system\"] = system_message\n        \n        response = await self.client.messages.create(**kwargs)\n        \n        return {\n            \"content\": response.content[0].text,\n            \"usage\": {\n                \"prompt_tokens\": response.usage.input_tokens,\n                \"completion_tokens\": response.usage.output_tokens,\n                \"total_tokens\": response.usage.input_tokens + response.usage.output_tokens\n            },\n            \"model\": response.model,\n            \"finish_reason\": response.stop_reason\n        }\n    \n    def get_provider_name(self) -> str:\n        \"\"\"Get provider name / èŽ·å–æä¾›å•†åç§°\"\"\"\n        return \"anthropic\"\n",
    "line_count": 77
  },
  {
    "id": "remnawave-bedolaga-telegram-bot_app_webapi_server.py",
    "repo": "BEDOLAGA-DEV/remnawave-bedolaga-telegram-bot",
    "url": "https://github.com/BEDOLAGA-DEV/remnawave-bedolaga-telegram-bot/blob/main/app/webapi/server.py",
    "code": "from __future__ import annotations\n\nimport asyncio\nimport logging\nfrom typing import Optional\n\nimport uvicorn\n\nfrom app.config import settings\n\nfrom .app import create_web_api_app\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass WebAPIServer:\n    \"\"\"ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹ uvicorn-ÑÐµÑ€Ð²ÐµÑ€ Ð´Ð»Ñ Ð°Ð´Ð¼Ð¸Ð½Ð¸ÑÑ‚Ñ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ API.\"\"\"\n\n    def __init__(self, app: Optional[object] = None) -> None:\n        self._app = app or create_web_api_app()\n\n        workers = max(1, int(settings.WEB_API_WORKERS or 1))\n        if workers > 1:\n            logger.warning(\"WEB_API_WORKERS > 1 Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð² embed-Ñ€ÐµÐ¶Ð¸Ð¼Ðµ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ 1\")\n            workers = 1\n\n        self._config = uvicorn.Config(\n            app=self._app,\n            host=settings.WEB_API_HOST,\n            port=int(settings.WEB_API_PORT or 8080),\n            log_level=settings.LOG_LEVEL.lower(),\n            workers=workers,\n            lifespan=\"on\",\n            access_log=False,\n        )\n        self._server = uvicorn.Server(self._config)\n        self._task: Optional[asyncio.Task[None]] = None\n\n    async def start(self) -> None:\n        if self._task and not self._task.done():\n            logger.info(\"ðŸŒ ÐÐ´Ð¼Ð¸Ð½Ð¸ÑÑ‚Ñ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ðµ Ð²ÐµÐ±-API ÑƒÐ¶Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¾\")\n            return\n\n        async def _serve() -> None:\n            try:\n                await self._server.serve()\n            except Exception as error:  # pragma: no cover - Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ ÑÐµÑ€Ð²ÐµÑ€Ð°\n                logger.exception(\"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð²ÐµÐ±-API: %s\", error)\n                raise\n\n        logger.info(\n            \"ðŸŒ Ð—Ð°Ð¿ÑƒÑÐº Ð°Ð´Ð¼Ð¸Ð½Ð¸ÑÑ‚Ñ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ API Ð½Ð° %s:%s\",\n            settings.WEB_API_HOST,\n            settings.WEB_API_PORT,\n        )\n        self._task = asyncio.create_task(_serve(), name=\"web-api-server\")\n\n        started_attr = getattr(self._server, \"started\", None)\n        started_event = getattr(self._server, \"started_event\", None)\n\n        if isinstance(started_attr, asyncio.Event):\n            await started_attr.wait()\n        elif isinstance(started_event, asyncio.Event):\n            await started_event.wait()\n        else:\n            while not getattr(self._server, \"started\", False):\n                if self._task.done():\n                    break\n                await asyncio.sleep(0.1)\n\n        if self._task.done() and self._task.exception():\n            raise self._task.exception()\n\n    async def stop(self) -> None:\n        if not self._task:\n            return\n\n        logger.info(\"ðŸ›‘ ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð°Ð´Ð¼Ð¸Ð½Ð¸ÑÑ‚Ñ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ API\")\n        self._server.should_exit = True\n        await self._task\n        self._task = None\n",
    "line_count": 82
  },
  {
    "id": "Wegent_backend_app_models_namespace.py",
    "repo": "wecode-ai/Wegent",
    "url": "https://github.com/wecode-ai/Wegent/blob/main/backend/app/models/namespace.py",
    "code": "# SPDX-FileCopyrightText: 2025 Weibo, Inc.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nfrom datetime import datetime\n\nfrom sqlalchemy import Boolean, Column, DateTime, Integer, String, Text\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.sql import func\n\nfrom app.db.base import Base\n\n\nclass Namespace(Base):\n    \"\"\"\n    Group (Namespace) model for resource organization.\n\n    Supports hierarchical structure with parent/child groups using name prefixes.\n    Example: 'aaa/bbb' represents group 'bbb' under parent group 'aaa'.\n    \"\"\"\n\n    __tablename__ = \"namespace\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    # Unique identifier, immutable after creation\n    # Sub-groups use prefix format (e.g., 'aaa/bbb')\n    name = Column(String(100), nullable=False, unique=True, index=True)\n    # Display name, can be modified\n    display_name = Column(String(100), nullable=True)\n    # Group owner user ID\n    owner_user_id = Column(Integer, nullable=False, index=True)\n    # Visibility: private, internal, public\n    visibility = Column(String(20), nullable=False, default=\"private\")\n    # Group description\n    description = Column(Text, nullable=False, default=\"\")\n    # Is group active\n    is_active = Column(Boolean, default=True)\n    created_at = Column(DateTime, default=func.now())\n    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())\n\n    # Relationships\n    members = relationship(\n        \"NamespaceMember\",\n        back_populates=\"namespace\",\n        cascade=\"all, delete-orphan\",\n        primaryjoin=\"Namespace.name == foreign(NamespaceMember.group_name)\",\n    )\n\n    __table_args__ = (\n        {\n            \"sqlite_autoincrement\": True,\n            \"mysql_engine\": \"InnoDB\",\n            \"mysql_charset\": \"utf8mb4\",\n            \"mysql_collate\": \"utf8mb4_unicode_ci\",\n            \"comment\": \"Group (Namespace) table for resource organization\",\n        },\n    )\n\n    def get_parent_name(self) -> str | None:\n        \"\"\"Get parent group name from hierarchical name.\"\"\"\n        if \"/\" not in self.name:\n            return None\n        return self.name.rsplit(\"/\", 1)[0]\n\n    def get_depth(self) -> int:\n        \"\"\"Get nesting depth (0 for root groups).\"\"\"\n        return self.name.count(\"/\")\n\n    def is_subgroup_of(self, parent_name: str) -> bool:\n        \"\"\"Check if this group is a subgroup of the given parent.\"\"\"\n        return self.name.startswith(f\"{parent_name}/\")\n",
    "line_count": 71
  },
  {
    "id": "Cybersecurity-Projects_PROJECTS_api-security-scanner_backend_models_Base.py",
    "repo": "CarterPerez-dev/Cybersecurity-Projects",
    "url": "https://github.com/CarterPerez-dev/Cybersecurity-Projects/blob/main/PROJECTS/api-security-scanner/backend/models/Base.py",
    "code": "\"\"\"\nâ’¸AngelaMos | 2025\nBase model class\nCommon fields and methods for all models\n\"\"\"\n\nfrom typing import Any\nfrom sqlalchemy import (\n    Column,\n    DateTime,\n    Integer,\n)\nfrom datetime import datetime, UTC\nfrom sqlalchemy.ext.declarative import declared_attr\n\nfrom core.database import Base\n\n\nclass BaseModel(Base):\n    \"\"\"\n    Abstract base model with common fields and methods\n    All models inherit from this class\n    \"\"\"\n\n    __abstract__ = True\n\n    id = Column(\n        Integer,\n        primary_key = True,\n        index = True,\n        autoincrement = True\n    )\n    created_at = Column(\n        DateTime(timezone = True),\n        default = lambda: datetime.now(UTC)\n    )\n    updated_at = Column(\n        DateTime(timezone = True),\n        default = lambda: datetime.now(UTC),\n        onupdate = lambda: datetime.now(UTC),\n    )\n\n    @declared_attr\n    def __tablename__(cls) -> str:\n        \"\"\"\n        Auto-generate table name from class name\n        \"\"\"\n        return cls.__name__.lower()\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Convert model instance to dictionary\n\n        Returns:\n            dict: Dictionary representation of the model\n        \"\"\"\n        return {\n            column.name: getattr(self,\n                                 column.name)\n            for column in self.__table__.columns\n        }\n\n    def update(self, **kwargs: Any) -> None:\n        \"\"\"\n        Update model fields from keyword arguments\n\n        Args:\n            **kwargs: Field names and values to update\n        \"\"\"\n        for key, value in kwargs.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n        self.updated_at = datetime.now(UTC)\n\n    def __repr__(self) -> str:\n        \"\"\"\n        String representation of model\n        \"\"\"\n        return f\"<{self.__class__.__name__}(id={self.id})>\"\n",
    "line_count": 79
  },
  {
    "id": "spider_server_lib_tinker-cookbook_tinker_cookbook_recipes_verifiers_rl_verifiers_env.py",
    "repo": "collinear-ai/spider",
    "url": "https://github.com/collinear-ai/spider/blob/main/server/lib/tinker-cookbook/tinker_cookbook/recipes/verifiers_rl/verifiers_env.py",
    "code": "from __future__ import annotations\n\nfrom typing import Sequence\n\nimport chz\nimport verifiers as vf\nfrom tinker_cookbook.rl.types import EnvGroupBuilder, RLDataset, RLDatasetBuilder\n\n\nclass VerifiersRLDataset(RLDataset):\n    def __init__(\n        self,\n        rows: list[dict],\n        vf_env: vf.Environment,\n        groups_per_batch: int,\n    ):\n        self.rows = rows\n        self.vf_env = vf_env\n        self.groups_per_batch = groups_per_batch\n\n    def __len__(self) -> int:\n        return (len(self.rows) + self.groups_per_batch - 1) // self.groups_per_batch\n\n    def get_batch(self, index: int) -> Sequence[EnvGroupBuilder]:\n        start = index * self.groups_per_batch\n        end = min(len(self.rows), start + self.groups_per_batch)\n        builders: list[EnvGroupBuilder] = []\n        for j in range(start, end):\n            row = self.rows[j]\n            builders.append(\n                VerifiersEnvGroupBuilder(\n                    vf_env=self.vf_env,\n                    prompt=row[\"prompt\"],\n                    answer=row.get(\"answer\", \"\"),\n                    task=row.get(\"task\", \"default\"),\n                    info=row.get(\"info\", {}),\n                )\n            )\n        return builders\n\n\n@chz.chz\nclass VerifiersRLDatasetBuilder(RLDatasetBuilder):\n    vf_env: vf.Environment\n    groups_per_batch: int\n    dataset_n: int\n    dataset_seed: int | None\n\n    async def __call__(self) -> tuple[RLDataset, RLDataset | None]:\n        ds = self.vf_env.get_dataset(n=self.dataset_n, seed=self.dataset_seed)\n        rows = [\n            {\n                \"prompt\": ds[\"prompt\"][i],\n                **({\"answer\": ds[\"answer\"][i]} if \"answer\" in ds.column_names else {}),\n                **({\"task\": ds[\"task\"][i]} if \"task\" in ds.column_names else {}),\n                **({\"info\": ds[\"info\"][i]} if \"info\" in ds.column_names else {}),\n            }\n            for i in range(len(ds))\n        ]\n        return VerifiersRLDataset(rows, self.vf_env, self.groups_per_batch), None\n\n\nclass VerifiersEnvGroupBuilder(EnvGroupBuilder):\n    def __init__(\n        self,\n        vf_env: vf.Environment,\n        prompt: vf.Messages,\n        answer: str,\n        task: str,\n        info: dict,\n    ):\n        self.vf_env = vf_env\n        self.prompt = prompt\n        self.answer = answer\n        self.task = task\n        self.info = info\n\n    async def make_envs(self):\n        return []  # unused when using custom_do_group_rollout\n\n    def logging_tags(self):\n        return [self.task] if self.task else []\n",
    "line_count": 82
  },
  {
    "id": "NegPy_src_infrastructure_storage_repository.py",
    "repo": "marcinz606/NegPy",
    "url": "https://github.com/marcinz606/NegPy/blob/main/src/infrastructure/storage/repository.py",
    "code": "import sqlite3\nimport json\nimport os\nfrom typing import Any, Optional\nfrom src.domain.models import WorkspaceConfig\nfrom src.domain.interfaces import IRepository\n\n\nclass StorageRepository(IRepository):\n    \"\"\"\n    SQLite backend for settings.\n    \"\"\"\n\n    def __init__(self, edits_db_path: str, settings_db_path: str) -> None:\n        self.edits_db_path = edits_db_path\n        self.settings_db_path = settings_db_path\n\n    def initialize(self) -> None:\n        \"\"\"\n        Ensures DB tables exist.\n        \"\"\"\n        os.makedirs(os.path.dirname(self.edits_db_path), exist_ok=True)\n\n        with sqlite3.connect(self.edits_db_path) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS file_settings (\n                    file_hash TEXT PRIMARY KEY,\n                    settings_json TEXT\n                )\n            \"\"\")\n\n        with sqlite3.connect(self.settings_db_path) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS global_settings (\n                    key TEXT PRIMARY KEY,\n                    value_json TEXT\n                )\n            \"\"\")\n\n    def save_file_settings(self, file_hash: str, settings: WorkspaceConfig) -> None:\n        with sqlite3.connect(self.edits_db_path) as conn:\n            settings_json = json.dumps(settings.to_dict(), default=str)\n            conn.execute(\n                \"INSERT OR REPLACE INTO file_settings (file_hash, settings_json) VALUES (?, ?)\",\n                (file_hash, settings_json),\n            )\n\n    def load_file_settings(self, file_hash: str) -> Optional[WorkspaceConfig]:\n        with sqlite3.connect(self.edits_db_path) as conn:\n            cursor = conn.execute(\n                \"SELECT settings_json FROM file_settings WHERE file_hash = ?\",\n                (file_hash,),\n            )\n            row = cursor.fetchone()\n            if row:\n                data = json.loads(row[0])\n                return WorkspaceConfig.from_flat_dict(data)\n        return None\n\n    def save_global_setting(self, key: str, value: Any) -> None:\n        with sqlite3.connect(self.settings_db_path) as conn:\n            conn.execute(\n                \"INSERT OR REPLACE INTO global_settings (key, value_json) VALUES (?, ?)\",\n                (key, json.dumps(value, default=str)),\n            )\n\n    def get_global_setting(self, key: str, default: Any = None) -> Any:\n        with sqlite3.connect(self.settings_db_path) as conn:\n            cursor = conn.execute(\n                \"SELECT value_json FROM global_settings WHERE key = ?\", (key,)\n            )\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n        return default\n",
    "line_count": 75
  },
  {
    "id": "N3D-VLM_src_llamafactory_webui_manager.py",
    "repo": "W-Ted/N3D-VLM",
    "url": "https://github.com/W-Ted/N3D-VLM/blob/main/src/llamafactory/webui/manager.py",
    "code": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections.abc import Generator\nfrom typing import TYPE_CHECKING\n\n\nif TYPE_CHECKING:\n    from gradio.components import Component\n\n\nclass Manager:\n    r\"\"\"A class to manage all the gradio components in Web UI.\"\"\"\n\n    def __init__(self) -> None:\n        self._id_to_elem: dict[str, Component] = {}\n        self._elem_to_id: dict[Component, str] = {}\n\n    def add_elems(self, tab_name: str, elem_dict: dict[str, \"Component\"]) -> None:\n        r\"\"\"Add elements to manager.\"\"\"\n        for elem_name, elem in elem_dict.items():\n            elem_id = f\"{tab_name}.{elem_name}\"\n            self._id_to_elem[elem_id] = elem\n            self._elem_to_id[elem] = elem_id\n\n    def get_elem_list(self) -> list[\"Component\"]:\n        r\"\"\"Return the list of all elements.\"\"\"\n        return list(self._id_to_elem.values())\n\n    def get_elem_iter(self) -> Generator[tuple[str, \"Component\"], None, None]:\n        r\"\"\"Return an iterator over all elements with their names.\"\"\"\n        for elem_id, elem in self._id_to_elem.items():\n            yield elem_id.split(\".\")[-1], elem\n\n    def get_elem_by_id(self, elem_id: str) -> \"Component\":\n        r\"\"\"Get element by id.\n\n        Example: top.lang, train.dataset\n        \"\"\"\n        return self._id_to_elem[elem_id]\n\n    def get_id_by_elem(self, elem: \"Component\") -> str:\n        r\"\"\"Get id by element.\"\"\"\n        return self._elem_to_id[elem]\n\n    def get_base_elems(self) -> set[\"Component\"]:\n        r\"\"\"Get the base elements that are commonly used.\"\"\"\n        return {\n            self._id_to_elem[\"top.lang\"],\n            self._id_to_elem[\"top.model_name\"],\n            self._id_to_elem[\"top.model_path\"],\n            self._id_to_elem[\"top.finetuning_type\"],\n            self._id_to_elem[\"top.checkpoint_path\"],\n            self._id_to_elem[\"top.quantization_bit\"],\n            self._id_to_elem[\"top.quantization_method\"],\n            self._id_to_elem[\"top.template\"],\n            self._id_to_elem[\"top.rope_scaling\"],\n            self._id_to_elem[\"top.booster\"],\n        }\n",
    "line_count": 70
  },
  {
    "id": "Raw-Alchemy_setup.py",
    "repo": "shenmintao/Raw-Alchemy",
    "url": "https://github.com/shenmintao/Raw-Alchemy/blob/main/setup.py",
    "code": "import json\nimport os\nimport platform\nimport shutil\nimport sys\nimport urllib.request\nfrom pathlib import Path\n\nfrom setuptools import setup\nfrom setuptools.command.build_py import build_py\n\n\nclass CustomBuildPy(build_py):\n    \"\"\"Custom build command to download and set up Lensfun.\"\"\"\n\n    def run(self):\n        self.download_and_extract_lensfun()\n        super().run()\n\n    def get_download_url(self, asset_name):\n        \"\"\"Gets the download URL for a given asset from the latest GitHub release.\"\"\"\n        api_url = \"https://api.github.com/repos/shenmintao/lensfun/releases/latest\"\n        # Try both GITHUB_TOKEN and GH_TOKEN for compatibility\n        token = os.environ.get(\"GITHUB_TOKEN\") or os.environ.get(\"GH_TOKEN\")\n        headers = {}\n        if token:\n            headers[\"Authorization\"] = f\"token {token}\"\n\n        req = urllib.request.Request(api_url, headers=headers)\n        try:\n            with urllib.request.urlopen(req) as response:\n                data = json.loads(response.read().decode())\n                for asset in data[\"assets\"]:\n                    if asset[\"name\"] == asset_name:\n                        return asset[\"browser_download_url\"]\n        except Exception as e:\n            print(f\"Error fetching release info from GitHub: {e}\", file=sys.stderr)\n            return None\n        return None\n\n    def download_and_extract_lensfun(self):\n        \"\"\"Downloads and extracts the appropriate Lensfun library.\"\"\"\n        vendor_dir = Path(\"src/raw_alchemy/vendor/lensfun\")\n        vendor_dir.mkdir(parents=True, exist_ok=True)\n\n        system = platform.system().lower()\n        if system == \"windows\":\n            asset_name = \"lensfun-windows.zip\"\n        elif system == \"linux\":\n            asset_name = \"lensfun-linux.tar.gz\"\n        elif system == \"darwin\":\n            asset_name = \"lensfun-macos.tar.gz\"\n        else:\n            print(f\"Unsupported system: {system}\", file=sys.stderr)\n            sys.exit(1)\n\n        download_url = self.get_download_url(asset_name)\n        if not download_url:\n            print(f\"Could not find download URL for {asset_name}\", file=sys.stderr)\n            sys.exit(1)\n\n        archive_path = asset_name\n        try:\n            print(f\"Downloading Lensfun for {system} from {download_url}...\")\n            with urllib.request.urlopen(download_url) as response, open(\n                archive_path, \"wb\"\n            ) as out_file:\n                shutil.copyfileobj(response, out_file)\n\n            print(\"Extracting Lensfun...\")\n            shutil.unpack_archive(archive_path, vendor_dir)\n            os.remove(archive_path)\n            print(\"Lensfun setup complete.\")\n\n        except Exception as e:\n            print(f\"Error during Lensfun setup: {e}\", file=sys.stderr)\n            sys.exit(1)\n\n\nsetup(\n    cmdclass={\n        \"build_py\": CustomBuildPy,\n    }\n)",
    "line_count": 84
  },
  {
    "id": "ai-llm-red-team-handbook_scripts_compliance_ai_recon_scanner_source.py",
    "repo": "Shiva108/ai-llm-red-team-handbook",
    "url": "https://github.com/Shiva108/ai-llm-red-team-handbook/blob/main/scripts/compliance/ai_recon_scanner_source.py",
    "code": "#!/usr/bin/env python3\n\"\"\"\nThe `AI_Recon_Scanner`\n\nSource: Chapter_39_AI_Bug_Bounty_Programs\nCategory: compliance\n\"\"\"\n\nimport aiohttp\nimport asyncio\nfrom typing import Dict, List\n\nimport argparse\nimport sys\n\nclass AIReconScanner:\n    \"\"\"\n    Fingerprints AI backends by analyzing HTTP headers and\n    404/405 error responses for specific signatures.\n    \"\"\"\n\n    def __init__(self, targets: List[str]):\n        self.targets = targets\n        self.signatures = {\n            \"OpenAI\": [\"x-request-id\", \"openai-organization\", \"openai-processing-ms\"],\n            \"Anthropic\": [\"x-api-key\", \"anthropic-version\"],\n            \"HuggingFace\": [\"x-linked-model\", \"x-huggingface-reason\"],\n            \"LangChain\": [\"x-langchain-trace\"],\n            \"Azure OAI\": [\"apim-request-id\", \"x-ms-region\"]\n        }\n\n    async def scan_target(self, url: str) -> Dict:\n        \"\"\"Probes a URL for AI-specific artifacts.\"\"\"\n        results = {\"url\": url, \"backend\": \"Unknown\", \"confidence\": 0}\n\n        try:\n            async with aiohttp.ClientSession() as session:\n                # Probe 1: Check Headers\n                async with session.get(url, verify_ssl=False) as resp:\n                    headers = resp.headers\n                    for tech, sigs in self.signatures.items():\n                        matches = [s for s in sigs if s in headers or s.lower() in headers]\n                        if matches:\n                            results[\"backend\"] = tech\n                            results[\"confidence\"] += 30\n                            results[\"signatures\"] = matches\n\n                # Probe 2: Check Standard API Paths\n                api_paths = [\"/v1/chat/completions\", \"/api/generate\", \"/v1/models\"]\n                for path in api_paths:\n                    full_url = f\"{url.rstrip('/')}{path}\"\n                    async with session.post(full_url, json={}) as resp:\n                        # 400 or 422 usually means \"I understood the path but you sent bad JSON\"\n                        # This confirms the endpoint exists.\n                        if resp.status in [400, 422]:\n                            results[\"endpoint_found\"] = path\n                            results[\"confidence\"] += 50\n\n            return results\n\n        except Exception as e:\n            return {\"url\": url, \"error\": str(e)}\n\n    async def run(self):\n        tasks = [self.scan_target(t) for t in self.targets]\n        return await asyncio.gather(*tasks)\n\n# Usage\n# targets = [\"https://chat.target-corp.com\", \"https://api.startup.io\"]\n# scanner = AIReconScanner(targets)\n# asyncio.run(scanner.run())\n\n\ndef main():\n    \"\"\"Command-line interface.\"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Verbose output\")\n    args = parser.parse_args()\n    \n    # TODO: Add main execution logic\n    pass\n\nif __name__ == \"__main__\":\n    main()",
    "line_count": 84
  },
  {
    "id": "polaris_src_polaris_policy_abstract_client.py",
    "repo": "arhanjain/polaris",
    "url": "https://github.com/arhanjain/polaris/blob/main/src/polaris/policy/abstract_client.py",
    "code": "from abc import ABC, abstractmethod\nfrom typing import Callable\nimport numpy as np\n\nfrom polaris.config import PolicyArgs\n\n\nclass InferenceClient(ABC):\n    REGISTERED_CLIENTS = {}\n\n    # def __init_subclass__(cls, client_name: str, *args, **kwargs) -> None:\n    #     super().__init_subclass__(*args, **kwargs)\n    #     InferenceClient.REGISTERED_CLIENTS[client_name] = cls\n\n    @staticmethod\n    def register(client_name: str) -> Callable[[type], type]:\n        def decorator(cls: type):\n            InferenceClient.REGISTERED_CLIENTS[client_name] = cls\n            return cls\n\n        return decorator\n\n    @staticmethod\n    def get_client(policy_args: PolicyArgs) -> \"InferenceClient\":\n        if policy_args.client not in InferenceClient.REGISTERED_CLIENTS:\n            raise ValueError(\n                f\"Client {policy_args.client} not found. Available clients: {list(InferenceClient.REGISTERED_CLIENTS.keys())}\"\n            )\n        return InferenceClient.REGISTERED_CLIENTS[policy_args.client](policy_args)\n\n    @abstractmethod\n    def __init__(self, args) -> None:\n        \"\"\"\n        Initializes the client.\n        \"\"\"\n        pass\n\n    @property\n    def rerender(self) -> bool:\n        \"\"\"\n        Policy requests a rerender of the visualization. Optimization for less splat rendering\n        for chunked policies. Can default to always True if optimization is not desired.\n        \"\"\"\n        return True\n\n    @abstractmethod\n    def infer(\n        self, obs, instruction, return_viz: bool = False\n    ) -> tuple[np.ndarray, np.ndarray | None]:\n        \"\"\"\n        Does inference on observation and returns action and visualization. If visualization is not needed, return None.\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"\n        Resets the client to start a new episode. Useful if policy is stateful.\n        \"\"\"\n        pass\n\n\nclass FakeClient(InferenceClient):\n    \"\"\"\n    Fake client that returns a dummy action and visualization.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs) -> None:\n        return\n\n    def infer(\n        self, obs, instruction, return_viz: bool = False\n    ) -> tuple[np.ndarray, np.ndarray | None]:\n        import cv2\n\n        external = obs[\"splat\"][\"external_cam\"]\n        wrist = obs[\"splat\"][\"wrist_cam\"]\n        external = cv2.resize(external, (224, 224))\n        wrist = cv2.resize(wrist, (224, 224))\n        both = np.concatenate([external, wrist], axis=1)\n        return np.zeros((8,)), both\n\n    def reset(self, *args, **kwargs):\n        return\n",
    "line_count": 85
  },
  {
    "id": "RoboCOIN_src_lerobot_datasets_sampler.py",
    "repo": "FlagOpen/RoboCOIN",
    "url": "https://github.com/FlagOpen/RoboCOIN/blob/main/src/lerobot/datasets/sampler.py",
    "code": "#!/usr/bin/env python\n\n# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom collections.abc import Iterator\n\nimport torch\n\n\nclass EpisodeAwareSampler:\n    def __init__(\n        self,\n        episode_data_index: dict,\n        episode_indices_to_use: list | None = None,\n        drop_n_first_frames: int = 0,\n        drop_n_last_frames: int = 0,\n        shuffle: bool = False,\n    ):\n        \"\"\"Sampler that optionally incorporates episode boundary information.\n\n        Args:\n            episode_data_index: Dictionary with keys 'from' and 'to' containing the start and end indices of each episode.\n            episode_indices_to_use: List of episode indices to use. If None, all episodes are used.\n                                    Assumes that episodes are indexed from 0 to N-1.\n            drop_n_first_frames: Number of frames to drop from the start of each episode.\n            drop_n_last_frames: Number of frames to drop from the end of each episode.\n            shuffle: Whether to shuffle the indices.\n        \"\"\"\n        indices = []\n        for episode_idx, (start_index, end_index) in enumerate(\n            zip(episode_data_index[\"from\"], episode_data_index[\"to\"], strict=True)\n        ):\n            if episode_indices_to_use is None or episode_idx in episode_indices_to_use:\n                indices.extend(\n                    range(start_index.item() + drop_n_first_frames, end_index.item() - drop_n_last_frames)\n                )\n\n        self.indices = indices\n        self.shuffle = shuffle\n\n    def __iter__(self) -> Iterator[int]:\n        if self.shuffle:\n            for i in torch.randperm(len(self.indices)):\n                yield self.indices[i]\n        else:\n            for i in self.indices:\n                yield i\n\n    def __len__(self) -> int:\n        return len(self.indices)\n",
    "line_count": 61
  },
  {
    "id": "apm_src_apm_cli_runtime_base.py",
    "repo": "danielmeppiel/apm",
    "url": "https://github.com/danielmeppiel/apm/blob/main/src/apm_cli/runtime/base.py",
    "code": "\"\"\"Base runtime adapter interface for APM.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\n\n\nclass RuntimeAdapter(ABC):\n    \"\"\"Base adapter interface for LLM runtimes.\"\"\"\n    \n    @abstractmethod\n    def execute_prompt(self, prompt_content: str, **kwargs) -> str:\n        \"\"\"Execute a single prompt and return the response.\n        \n        Args:\n            prompt_content: The prompt text to execute\n            **kwargs: Additional arguments passed to the runtime\n            \n        Returns:\n            str: The response text from the runtime\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def list_available_models(self) -> Dict[str, Any]:\n        \"\"\"List all available models in the runtime.\n        \n        Returns:\n            Dict[str, Any]: Dictionary of available models and their info\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def get_runtime_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about this runtime.\n        \n        Returns:\n            Dict[str, Any]: Runtime information including name, version, capabilities\n        \"\"\"\n        pass\n    \n    @staticmethod\n    @abstractmethod\n    def is_available() -> bool:\n        \"\"\"Check if this runtime is available on the system.\n        \n        Returns:\n            bool: True if runtime is available, False otherwise\n        \"\"\"\n        pass\n    \n    @staticmethod\n    @abstractmethod\n    def get_runtime_name() -> str:\n        \"\"\"Get the name of this runtime.\n        \n        Returns:\n            str: Runtime name (e.g., 'llm', 'codex')\n        \"\"\"\n        pass\n    \n    def __str__(self) -> str:\n        \"\"\"String representation of the runtime.\"\"\"\n        return f\"{self.get_runtime_name()}RuntimeAdapter\"",
    "line_count": 63
  },
  {
    "id": "python-sdk_src_acp_task_queue.py",
    "repo": "agentclientprotocol/python-sdk",
    "url": "https://github.com/agentclientprotocol/python-sdk/blob/main/src/acp/task/queue.py",
    "code": "from __future__ import annotations\n\nimport asyncio\nfrom collections.abc import AsyncIterator\nfrom contextlib import suppress\nfrom typing import Protocol\n\nfrom . import RpcTask\n\n__all__ = [\"InMemoryMessageQueue\", \"MessageQueue\"]\n\n\nclass MessageQueue(Protocol):\n    async def publish(self, task: RpcTask) -> None: ...\n\n    async def close(self) -> None: ...\n\n    def task_done(self) -> None: ...\n\n    async def join(self) -> None: ...\n\n    def __aiter__(self) -> AsyncIterator[RpcTask]: ...\n\n\nclass InMemoryMessageQueue:\n    \"\"\"Simple in-memory broker for RPC task dispatch.\"\"\"\n\n    def __init__(self, *, maxsize: int = 0) -> None:\n        self._queue: asyncio.Queue[RpcTask | None] = asyncio.Queue(maxsize=maxsize)\n        self._closed = False\n\n    async def publish(self, task: RpcTask) -> None:\n        if self._closed:\n            msg = \"mssage queue already closed\"\n            raise RuntimeError(msg)\n        await self._queue.put(task)\n\n    async def close(self) -> None:\n        if self._closed:\n            return\n        self._closed = True\n        await self._queue.put(None)\n\n    async def join(self) -> None:\n        await self._queue.join()\n\n    def task_done(self) -> None:\n        with suppress(ValueError):\n            self._queue.task_done()\n\n    def __aiter__(self) -> AsyncIterator[RpcTask]:\n        return _QueueIterator(self)\n\n\nclass _QueueIterator:\n    def __init__(self, queue: InMemoryMessageQueue) -> None:\n        self._queue = queue\n\n    def __aiter__(self) -> _QueueIterator:\n        return self\n\n    async def __anext__(self) -> RpcTask:\n        item = await self._queue._queue.get()\n        if item is None:\n            self._queue.task_done()\n            raise StopAsyncIteration\n        return item\n",
    "line_count": 67
  },
  {
    "id": "C3G_src_dataset_view_sampler_view_sampler.py",
    "repo": "cvlab-kaist/C3G",
    "url": "https://github.com/cvlab-kaist/C3G/blob/main/src/dataset/view_sampler/view_sampler.py",
    "code": "from abc import ABC, abstractmethod\nfrom typing import Generic, TypeVar\n\nimport torch\nfrom jaxtyping import Float, Int64\nfrom torch import Tensor\n\nfrom ...misc.step_tracker import StepTracker\nfrom ..types import Stage\n\nT = TypeVar(\"T\")\n\n\nclass ViewSampler(ABC, Generic[T]):\n    cfg: T\n    stage: Stage\n    is_overfitting: bool\n    cameras_are_circular: bool\n    step_tracker: StepTracker | None\n\n    def __init__(\n        self,\n        cfg: T,\n        stage: Stage,\n        is_overfitting: bool,\n        cameras_are_circular: bool,\n        step_tracker: StepTracker | None,\n    ) -> None:\n        self.cfg = cfg\n        self.stage = stage\n        self.is_overfitting = is_overfitting\n        self.cameras_are_circular = cameras_are_circular\n        self.step_tracker = step_tracker\n\n    @abstractmethod\n    def sample(\n        self,\n        scene: str,\n        extrinsics: Float[Tensor, \"view 4 4\"],\n        intrinsics: Float[Tensor, \"view 3 3\"],\n        device: torch.device = torch.device(\"cpu\"),\n    ) -> tuple[\n        Int64[Tensor, \" context_view\"],  # indices for context views\n        Int64[Tensor, \" target_view\"],  # indices for target views\n        Float[Tensor, \" overlap\"],  # overlap\n    ]:\n        pass\n\n    @property\n    @abstractmethod\n    def num_target_views(self) -> int:\n        pass\n\n    @property\n    @abstractmethod\n    def num_context_views(self) -> int:\n        pass\n\n    @property\n    def global_step(self) -> int:\n        return 0 if self.step_tracker is None else self.step_tracker.get_step()\n",
    "line_count": 61
  },
  {
    "id": "raxe-ce_src_raxe_domain_models.py",
    "repo": "raxe-ai/raxe-ce",
    "url": "https://github.com/raxe-ai/raxe-ce/blob/main/src/raxe/domain/models.py",
    "code": "\"\"\"Core domain models for RAXE CE.\n\nAll models are immutable value objects (frozen=True).\nPure domain layer - no I/O operations.\n\"\"\"\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n# Re-export Detection and ScanResult from executor for convenience\nfrom raxe.domain.engine.executor import Detection, ScanResult\n\n\nclass ThreatType(Enum):\n    \"\"\"Categories of threats.\n\n    Maps to rule families but provides more semantic categorization.\n    \"\"\"\n    PROMPT_INJECTION = \"PROMPT_INJECTION\"\n    JAILBREAK = \"JAILBREAK\"\n    PII_LEAK = \"PII_LEAK\"\n    DATA_EXFILTRATION = \"DATA_EXFIL\"\n    SECURITY = \"SECURITY\"\n    QUALITY = \"QUALITY\"\n    CUSTOM = \"CUSTOM\"\n\n\n@dataclass(frozen=True)\nclass ScanRequest:\n    \"\"\"Request to scan text for threats.\n\n    Immutable value object for scan input.\n\n    Attributes:\n        text: The text to scan for threats\n        context: Optional context metadata (user_id, session_id, etc.)\n        rule_filters: Optional list of rule IDs to apply (None = all rules)\n        max_text_length: Maximum allowed text length (default 1MB)\n    \"\"\"\n    text: str\n    context: dict[str, str] | None = None\n    rule_filters: list[str] | None = None\n    max_text_length: int = 1_000_000  # 1MB default limit\n\n    def __post_init__(self) -> None:\n        \"\"\"Validate request after construction.\n\n        Raises:\n            ValueError: If validation fails\n        \"\"\"\n        if not self.text:\n            raise ValueError(\"Text cannot be empty\")\n\n        if len(self.text) > self.max_text_length:\n            raise ValueError(\n                f\"Text exceeds maximum length of {self.max_text_length} \"\n                f\"(got {len(self.text)} chars)\"\n            )\n\n        if self.rule_filters is not None and not self.rule_filters:\n            raise ValueError(\"rule_filters cannot be empty list (use None for all rules)\")\n\n    @property\n    def text_length(self) -> int:\n        \"\"\"Length of text to scan.\"\"\"\n        return len(self.text)\n\n    @property\n    def has_filters(self) -> bool:\n        \"\"\"True if rule filters are specified.\"\"\"\n        return self.rule_filters is not None\n\n\n# Note: BlockAction and ScanPolicy have been removed.\n# The advanced policy system in domain/policies/ replaces this functionality.\n\n\n__all__ = [\n    \"Detection\",\n    \"ScanRequest\",\n    \"ScanResult\",\n    \"ThreatType\",\n]\n",
    "line_count": 82
  },
  {
    "id": "langrepl_src_langrepl_checkpointer_base.py",
    "repo": "midodimori/langrepl",
    "url": "https://github.com/midodimori/langrepl/blob/main/src/langrepl/checkpointer/base.py",
    "code": "\"\"\"Base checkpointer with extended methods for langrepl.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any\n\nfrom langgraph.checkpoint.base import BaseCheckpointSaver as _BaseCheckpointSaver\n\nif TYPE_CHECKING:\n    from langchain_core.messages import BaseMessage\n    from langgraph.checkpoint.base import CheckpointTuple\n\n\n@dataclass\nclass HumanMessageEntry:\n    \"\"\"Human message with replay metadata.\"\"\"\n\n    text: str\n    reference_mapping: dict[str, Any]\n    messages_before_count: int\n    checkpoint_id: str | None\n    input_tokens: int | None = None\n    output_tokens: int | None = None\n    total_cost: float | None = None\n\n\nclass BaseCheckpointer(_BaseCheckpointSaver):\n    \"\"\"Base checkpointer with additional query methods.\"\"\"\n\n    async def get_threads(self) -> set[str]:\n        \"\"\"Get all thread IDs.\"\"\"\n        raise NotImplementedError\n\n    async def get_history(self, latest: CheckpointTuple) -> list[CheckpointTuple]:\n        \"\"\"Get checkpoint history in chronological order (oldest first).\"\"\"\n        raise NotImplementedError\n\n    async def delete_after(self, thread_id: str, checkpoint_id: str | None) -> int:\n        \"\"\"Delete checkpoints after checkpoint_id. Returns count deleted.\"\"\"\n        raise NotImplementedError\n\n    async def get_human_messages(\n        self,\n        thread_id: str,\n        latest: CheckpointTuple,\n        on_indexing: Callable[[], None] | None = None,\n    ) -> tuple[list[HumanMessageEntry], list[BaseMessage]]:\n        \"\"\"Get human messages with replay metadata.\n\n        Returns:\n            Tuple of (human_messages, all_messages)\n        \"\"\"\n        raise NotImplementedError\n",
    "line_count": 55
  },
  {
    "id": "fs-explorer_packages_eval-framework_src_eval_framework__templating.py",
    "repo": "run-llama/fs-explorer",
    "url": "https://github.com/run-llama/fs-explorer/blob/main/packages/eval-framework/src/eval_framework/_templating.py",
    "code": "import re\n\nPATTERN = re.compile(r\"\\{\\{([^\\}]+)\\}\\}\")\n\n\nclass TemplateValidationError(Exception):\n    \"\"\"Raised when the arguments to render a template fail to validate\"\"\"\n\n\nclass Template:\n    \"\"\"\n    Jinja2-like class for string templating\n\n    Attributes:\n        content (str): original template string\n        _to_render (list[str]): fields of the string that have to be rendered with the template\n    \"\"\"\n\n    def __init__(self, content: str):\n        \"\"\"\n        Create a template from a string.\n\n        Args:\n            content (str): the template string\n        \"\"\"\n        self.content = content\n        self._to_render = PATTERN.findall(content)\n\n    def _validate(self, args: dict[str, str]) -> bool:\n        return all(el in args for el in self._to_render) and all(\n            isinstance(args[k], str) for k in args\n        )\n\n    def render(self, args: dict[str, str]) -> str:\n        \"\"\"\n        Render the template.\n\n        Args:\n            args (dict[str, str]): a dictionary of arguments for the template to be rendered. The keys represent the fields in the template, and the values represent the strings with which to fill the template.\n\n        Returns:\n            str: The rendered template string.\n        \"\"\"\n        if self._validate(args):\n            content = self.content\n            for word in self._to_render:\n                content = content.replace(\"{{\" + word + \"}}\", args[word])\n            return content\n        else:\n            if (ls := list(set(self._to_render) - set(list(args.keys())))) != []:\n                raise TemplateValidationError(\n                    f\"Missing the following arguments for the template: {', '.join(ls)}\"\n                )\n            else:\n                raise TemplateValidationError(\n                    \"You should provide a dictionary with only string values.\"\n                )\n",
    "line_count": 57
  },
  {
    "id": "InfiniteVGGT_src_croco_models_head_downstream.py",
    "repo": "AutoLab-SAI-SJTU/InfiniteVGGT",
    "url": "https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT/blob/main/src/croco/models/head_downstream.py",
    "code": "# Copyright (C) 2022-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n\n# --------------------------------------------------------\n# Heads for downstream tasks\n# --------------------------------------------------------\n\n\"\"\"\nA head is a module where the __init__ defines only the head hyperparameters.\nA method setup(croconet) takes a CroCoNet and set all layers according to the head and croconet attributes.\nThe forward takes the features as well as a dictionary img_info containing the keys 'width' and 'height'\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom .dpt_block import DPTOutputAdapter\n\n\nclass PixelwiseTaskWithDPT(nn.Module):\n    \"\"\"DPT module for CroCo.\n    by default, hooks_idx will be equal to:\n    * for encoder-only: 4 equally spread layers\n    * for encoder+decoder: last encoder + 3 equally spread layers of the decoder\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        hooks_idx=None,\n        layer_dims=[96, 192, 384, 768],\n        output_width_ratio=1,\n        num_channels=1,\n        postprocess=None,\n        **kwargs,\n    ):\n        super(PixelwiseTaskWithDPT, self).__init__()\n        self.return_all_blocks = True  # backbone needs to return all layers\n        self.postprocess = postprocess\n        self.output_width_ratio = output_width_ratio\n        self.num_channels = num_channels\n        self.hooks_idx = hooks_idx\n        self.layer_dims = layer_dims\n\n    def setup(self, croconet):\n        dpt_args = {\n            \"output_width_ratio\": self.output_width_ratio,\n            \"num_channels\": self.num_channels,\n        }\n        if self.hooks_idx is None:\n            if hasattr(croconet, \"dec_blocks\"):  # encoder + decoder\n                step = {8: 3, 12: 4, 24: 8}[croconet.dec_depth]\n                hooks_idx = [\n                    croconet.dec_depth + croconet.enc_depth - 1 - i * step\n                    for i in range(3, -1, -1)\n                ]\n            else:  # encoder only\n                step = croconet.enc_depth // 4\n                hooks_idx = [\n                    croconet.enc_depth - 1 - i * step for i in range(3, -1, -1)\n                ]\n            self.hooks_idx = hooks_idx\n            print(\n                f\"  PixelwiseTaskWithDPT: automatically setting hook_idxs={self.hooks_idx}\"\n            )\n        dpt_args[\"hooks\"] = self.hooks_idx\n        dpt_args[\"layer_dims\"] = self.layer_dims\n        self.dpt = DPTOutputAdapter(**dpt_args)\n        dim_tokens = [\n            (\n                croconet.enc_embed_dim\n                if hook < croconet.enc_depth\n                else croconet.dec_embed_dim\n            )\n            for hook in self.hooks_idx\n        ]\n        dpt_init_args = {\"dim_tokens_enc\": dim_tokens}\n        self.dpt.init(**dpt_init_args)\n\n    def forward(self, x, img_info):\n        out = self.dpt(x, image_size=(img_info[\"height\"], img_info[\"width\"]))\n        if self.postprocess:\n            out = self.postprocess(out)\n        return out\n",
    "line_count": 83
  },
  {
    "id": "Aether_src_middleware_rate_limit_config.py",
    "repo": "fawney19/Aether",
    "url": "https://github.com/fawney19/Aether/blob/master/src/middleware/rate_limit_config.py",
    "code": "\"\"\"\né€ŸçŽ‡é™åˆ¶é…ç½®\n\næä¾›çµæ´»çš„ç«¯ç‚¹é€ŸçŽ‡é™åˆ¶ç­–ç•¥é…ç½®\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Literal, Optional\n\nRateLimitScope = Literal[\"server_ip\", \"user\", \"api_key\", \"skip\"]\n\n\n@dataclass\nclass RateLimitPolicy:\n    \"\"\"é€ŸçŽ‡é™åˆ¶ç­–ç•¥\"\"\"\n\n    scope: RateLimitScope  # é™åˆ¶èŒƒå›´\n    limit: int  # é™åˆ¶å€¼ï¼ˆè¯·æ±‚æ•°/åˆ†é’Ÿï¼‰\n    description: str = \"\"\n\n\nclass RateLimitConfig:\n    \"\"\"\n    é€ŸçŽ‡é™åˆ¶é…ç½®ç®¡ç†\n\n    å®šä¹‰ä¸åŒè·¯å¾„å‰ç¼€çš„é€ŸçŽ‡é™åˆ¶ç­–ç•¥\n    \"\"\"\n\n    # é»˜è®¤ç­–ç•¥é…ç½®\n    POLICIES: Dict[str, RateLimitPolicy] = {\n        # å®¢æˆ·ç«¯ API ç«¯ç‚¹ - æœåŠ¡å™¨çº§åˆ« IP é™åˆ¶\n        \"/v1/\": RateLimitPolicy(\n            scope=\"server_ip\", limit=60, description=\"Claude/OpenAI API ç«¯ç‚¹ï¼ŒæœåŠ¡å™¨çº§åˆ«é™åˆ¶\"\n        ),\n        # å…¬å…± API ç«¯ç‚¹ - æœåŠ¡å™¨çº§åˆ« IP é™åˆ¶\n        \"/api/public/\": RateLimitPolicy(\n            scope=\"server_ip\", limit=60, description=\"å…¬å…±åªè¯» APIï¼ŒæœåŠ¡å™¨çº§åˆ«é™åˆ¶\"\n        ),\n        # ç®¡ç†åŽå°ç«¯ç‚¹ - ç”¨æˆ·çº§åˆ«é™åˆ¶\n        \"/api/admin/\": RateLimitPolicy(\n            scope=\"user\", limit=1000, description=\"ç®¡ç†åŽå°ï¼Œç”¨æˆ·çº§åˆ«é™åˆ¶\"\n        ),\n        # è®¤è¯ç«¯ç‚¹ - è·³è¿‡ä¸­é—´ä»¶ï¼ˆåœ¨è·¯ç”±å±‚å¤„ç†ï¼‰\n        \"/api/auth/\": RateLimitPolicy(scope=\"skip\", limit=0, description=\"è®¤è¯ç«¯ç‚¹ï¼Œè·¯ç”±å±‚å¤„ç†\"),\n        # ç”¨æˆ·ç«¯ç‚¹ - ç”¨æˆ·çº§åˆ«é™åˆ¶\n        \"/api/users/\": RateLimitPolicy(scope=\"skip\", limit=0, description=\"ç”¨æˆ·ç«¯ç‚¹ï¼Œè·¯ç”±å±‚å¤„ç†\"),\n        # ç›‘æŽ§ç«¯ç‚¹ - è·³è¿‡é™åˆ¶\n        \"/api/monitoring/\": RateLimitPolicy(scope=\"skip\", limit=0, description=\"ç›‘æŽ§ç«¯ç‚¹\"),\n    }\n\n    @classmethod\n    def get_policy_for_path(cls, path: str) -> Optional[RateLimitPolicy]:\n        \"\"\"\n        æ ¹æ®è·¯å¾„èŽ·å–é€ŸçŽ‡é™åˆ¶ç­–ç•¥\n\n        æŒ‰ç…§æœ€é•¿åŒ¹é…åŽŸåˆ™ï¼Œä¼˜å…ˆåŒ¹é…æ›´å…·ä½“çš„è·¯å¾„\n\n        Args:\n            path: è¯·æ±‚è·¯å¾„\n\n        Returns:\n            åŒ¹é…çš„é€ŸçŽ‡é™åˆ¶ç­–ç•¥ï¼Œå¦‚æžœæ²¡æœ‰åŒ¹é…åˆ™è¿”å›ž None\n        \"\"\"\n        # æŒ‰è·¯å¾„é•¿åº¦é™åºæŽ’åºï¼Œç¡®ä¿æœ€é•¿åŒ¹é…ä¼˜å…ˆ\n        sorted_prefixes = sorted(cls.POLICIES.keys(), key=len, reverse=True)\n\n        for prefix in sorted_prefixes:\n            if path.startswith(prefix):\n                return cls.POLICIES[prefix]\n\n        return None\n\n    @classmethod\n    def register_policy(cls, prefix: str, policy: RateLimitPolicy) -> None:\n        \"\"\"\n        æ³¨å†Œæ–°çš„é€ŸçŽ‡é™åˆ¶ç­–ç•¥\n\n        Args:\n            prefix: è·¯å¾„å‰ç¼€\n            policy: é€ŸçŽ‡é™åˆ¶ç­–ç•¥\n        \"\"\"\n        cls.POLICIES[prefix] = policy\n\n    @classmethod\n    def get_all_policies(cls) -> Dict[str, RateLimitPolicy]:\n        \"\"\"èŽ·å–æ‰€æœ‰ç­–ç•¥é…ç½®\"\"\"\n        return cls.POLICIES.copy()\n",
    "line_count": 87
  }
]