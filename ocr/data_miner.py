import json
import time
import random
from datetime import datetime
from github import Github
from tqdm import tqdm

# ================= é…ç½®åŒº (æ ¹æ®æ‚¨çš„è¦æ±‚ä¿®æ”¹) =================
# âš ï¸ è¯·åŠ¡å¿…åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„ GitHub Token
GITHUB_TOKEN = "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" 

TARGET_DATE = "2025-08-01"  # æˆªæ­¢æ—¥æœŸ
TARGET_LANG = "python"      # ç›®æ ‡è¯­è¨€
MIN_STARS = 50              # æœ€å° Star æ•°
MAX_STARS = 200             # æœ€å¤§ Star æ•°
MIN_LINES = 50              # æœ€å°è¡Œæ•°
MAX_LINES = 120             # æœ€å¤§è¡Œæ•°
LIMIT = 10                  # æŠ“å–æ•°é‡
OUTPUT_FILE = "dataset_fresh_2025.json"

# éšæœºåŒ–è®¾ç½®
ENABLE_RANDOM = True        # æ˜¯å¦å¯ç”¨éšæœºåŒ–
RANDOM_POOL_SIZE = 50       # ä»Žå‰ N ä¸ªç»“æžœä¸­éšæœºæŠ½å–
# =========================================================

def fetch_fresh_code():
    # ç®€å•çš„ Token æ£€æŸ¥
    if "ghp_" not in GITHUB_TOKEN and "github_" not in GITHUB_TOKEN:
        print("âš ï¸ è­¦å‘Š: GitHub Token å¯èƒ½æœªé…ç½®ï¼Œè¯·æ£€æŸ¥ data_miner.py")

    print(f"ðŸš€ [Module 1] Data Miner Started...")
    print(f"ðŸ“… Filter: Created > {TARGET_DATE} | Lines: {MIN_LINES}-{MAX_LINES} | Limit: {LIMIT}")
    
    g = Github(GITHUB_TOKEN)
    
    # éšæœºåŒ–æŸ¥è¯¢å‚æ•°
    if ENABLE_RANDOM:
        # éšæœºé€‰æ‹©æŽ’åºæ–¹å¼å’Œé¡ºåº
        sort_options = ["stars", "forks", "updated"]
        order_options = ["desc", "asc"]
        sort_by = random.choice(sort_options)
        order_by = random.choice(order_options)
        
        # éšæœºåç§»æ˜Ÿæ˜ŸèŒƒå›´ (åœ¨ MIN_STARS~MAX_STARS åŸºç¡€ä¸Šéšæœºåç§»)
        star_offset = random.randint(0, 50)
        actual_min_stars = MIN_STARS + star_offset
        actual_max_stars = MAX_STARS + star_offset
        
        print(f"ðŸŽ² Random mode: sort={sort_by}, order={order_by}, stars={actual_min_stars}..{actual_max_stars}")
    else:
        sort_by = "stars"
        order_by = "desc"
        actual_min_stars = MIN_STARS
        actual_max_stars = MAX_STARS
    
    query = f"language:{TARGET_LANG} created:>{TARGET_DATE} stars:{actual_min_stars}..{actual_max_stars}"
    
    try:
        repos = g.search_repositories(query, sort=sort_by, order=order_by)
    except Exception as e:
        print(f"âŒ GitHub API Error: {e}")
        return []

    # æ”¶é›†å€™é€‰ä»“åº“ï¼ˆå…ˆæ”¶é›†ä¸€ä¸ªæ± å­ï¼Œå†éšæœºæŠ½å–ï¼‰
    candidate_repos = []
    repo_count = 0
    
    print(f"ðŸ“¦ Building candidate pool (max {RANDOM_POOL_SIZE} repos)...")
    for repo in repos:
        if repo_count >= RANDOM_POOL_SIZE:
            break
        candidate_repos.append(repo)
        repo_count += 1
        time.sleep(0.05)  # é¿å… API é™åˆ¶
    
    # éšæœºæ‰“ä¹±å€™é€‰ä»“åº“é¡ºåº
    if ENABLE_RANDOM:
        random.shuffle(candidate_repos)
        print(f"ðŸ”€ Shuffled {len(candidate_repos)} candidate repos")

    dataset = []
    pbar = tqdm(total=LIMIT, desc="Mining Code")

    for repo in candidate_repos:
        if len(dataset) >= LIMIT:
            break
        try:
            contents = repo.get_contents("")
            files_to_check = []
            while contents:
                file_content = contents.pop(0)
                if file_content.type == "dir":
                    if file_content.path in ['src', 'lib', 'core', 'app']:
                        try:
                            contents.extend(repo.get_contents(file_content.path))
                        except: pass
                elif file_content.path.endswith(".py"):
                    if "test" not in file_content.path and "__init__" not in file_content.path:
                        files_to_check.append(file_content)
            
            for file_node in files_to_check:
                if 1000 < file_node.size < 20000:
                    code_text = file_node.decoded_content.decode('utf-8')
                    lines = code_text.splitlines()
                    if MIN_LINES <= len(lines) <= MAX_LINES:
                        dataset.append({
                            "id": f"{repo.name}_{file_node.path}".replace("/", "_"), # æ‰å¹³åŒ–IDæ–¹ä¾¿åšæ–‡ä»¶å
                            "repo": repo.full_name,
                            "url": file_node.html_url,
                            "code": code_text,
                            "line_count": len(lines)
                        })
                        pbar.update(1)
                        break 
        except:
            continue
        time.sleep(0.1)

    pbar.close()
    
    print(f"âœ… [Module 1] Completed. Saved {len(dataset)} items to {OUTPUT_FILE}")
    return dataset

if __name__ == "__main__":
    fetch_fresh_code()